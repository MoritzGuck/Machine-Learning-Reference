{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Exploring German LLM Expert Bots\n",
    "author: Moritz Gueck\n",
    "date: 2023-12-11\n",
    "format: \n",
    "  html:\n",
    "    toc-depth: 3\n",
    "wrap: auto\n",
    "execute:\n",
    "  enabled: false\n",
    "number-sections: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we are exploring how we can provide a large language model with context information (\"grounding\") and have it answer questions in German about it. We will then explore how different models perform at answering the questions.\n",
    "\n",
    "### Intro\n",
    "\n",
    "##### What we need:\n",
    "\n",
    "1. A large language model that can answer questions. -> Question Answering LLM (QA-LLM)\n",
    "2. A large language model that can map text to vectors. -> Vector-LLM\n",
    "3. A database to store the vectors. -> Vector-DB\n",
    "\n",
    "We will be using the framework Langchain for splitting texts, grounding and connecting to models and sources. Langchain provides us with a couple of benefits over specific APIs from different providers:\n",
    "\n",
    "- Standardised interface for different document-loaders, LLMs and vector-DBs\n",
    "- Easy integration of different LLMs into different vector-DBs\n",
    "- The high level API avoids boilerplate code. \n",
    "\n",
    " Here is a good intro to the topic: [langchain: Retrieval](https://python.langchain.com/docs/modules/data_connection/).\n",
    "\n",
    "##### How to do it:\n",
    "\n",
    "1. **Create a grounding database for your QA-LLM**.\n",
    "    1. Gather the information you want your QA-llm to answer questions about.\n",
    "    2. Cut the data into snippets that are small enough to be processed by your QA-llm.\n",
    "    3. Map each snippet to a vector using your Vector-llm. The vector represents the meaning/topic of the snippet.\n",
    "    4. Store the vectors in a database.\n",
    "2. **Answer questions using the grounding database**.\n",
    "    1. Given a question, map it to a vector using your Vector-llm. Then search the database for the snippets with the most similar vectors.\n",
    "    2. Prepend the snippet to your question.\n",
    "    3. Feed the augmented question to your QA-llm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "\n",
    "graph TB\n",
    "  subgraph SB[\"build grounding database\"]\n",
    "  A(Data Sources) -->|Load| B(Text-Files)\n",
    "  B -->|Chunk| C(Snippets)\n",
    "  C -->|\"vector-map (Vector-LLM)\"| D[(Vector DB)]\n",
    "\n",
    "  end\n",
    "\n",
    "  subgraph SU[\"use grounding database\"]\n",
    "  D -->|retrieve| E(Relevant snippets)\n",
    "  E -->|insert| F(Augmented Prompt)\n",
    "  F -->|\"query (QA-LLM)\"| G(Result)\n",
    "  end\n",
    "\n",
    "  style SB fill:#F2F2F2\n",
    "  style SU fill:#F2F2F2\n",
    "  linkStyle 0,1,2,3,4,5 stroke:#BFBFBF \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "# | code-summary: \"Show code: Base libraries to import\"\n",
    "\n",
    "# Importing libraries\n",
    "import os\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import random\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "# | code-summary: \"Show code: Print results more nicely\"\n",
    "\n",
    "def nice_print(text):\n",
    "    print(textwrap.fill(text, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating a grounding database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Gather the information\n",
    "\n",
    "In this step we will load the data that we want our model to answer questions about. \n",
    "We will use the data from the public, German Helsana website (largest health insurance in Switzerland)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions for loading the data:\n",
    "def download_webpage(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode(\"UTF-8\")\n",
    "\n",
    "    file_path = \"data/html/\" + get_page_name(url)\n",
    "    f = open(file_path, \"w\")\n",
    "    f.write(webContent)\n",
    "    f.close\n",
    "\n",
    "def get_page_name(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    page_name = parsed_url.path.split(\"/\")[-1]\n",
    "    return page_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "# | code-summary: \"Show code: Web-pages to crawl\"\n",
    "\n",
    "# List of web-pages where we will find the information for our knowledge base\n",
    "urls = [\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/basis.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/benefit-plus-hausarzt.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/benefit-plus-telmed.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/benefit-plus-flexmed.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/premed-24.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/uebersicht-grundversicherungen.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/spezialversicherungen.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/leistungsuebersicht.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/top.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/sana.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/completa.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/world.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/primeo.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-eco.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-halbprivat.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-privat.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-flex.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/zahnversicherung.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/weitere/advocare-plus.html\",\n",
    "    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/weitere/advocare-extra.html\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading \n",
    "for url in urls:\n",
    "    download_webpage(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Cut the data into snippets\n",
    "\n",
    "Now we need to cut the contents from the page into snippets that are small enough to be processed by our QA-llm but large enough to contain the relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1. Minimalist approach (not used)\n",
    "\n",
    "This is probably the simplest approach to cut the webpages into snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_texts = []\n",
    "for html_document_path in os.listdir(\"data/html\"):\n",
    "    soup = BeautifulSoup(\n",
    "        open(\"./data/html/\" + html_document_path), features=\"html.parser\"\n",
    "    )\n",
    "\n",
    "    website_text = soup.get_text()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    page_texts = text_splitter.create_documents([website_text])\n",
    "    website_texts = website_texts + page_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regrettably, the vector-LLM had a hard time to reasonably map the snippets to meaningful vectors and the QA-LLM was not able to answer questions based on the snippets. The reasons were probably, that the snippets started in the middle of paragraphs and titles from the paragraphs were missing. The QA-LLM then used snippets to answer questions from wrong topics.\n",
    "\n",
    "Therefore, I did not use this approach, but **used approach 1.2.2. instead**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Subsection based splitting (used)\n",
    "\n",
    "To embed the meaning and context clearer, we prepend the title and subtile of the relevant snippet to each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This algorithm might be stupid but it works. :-D\n",
    "def parse_website_texts(soup):\n",
    "    \"\"\"Cut website texts into snippets. \n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup.soup): Soup object from the Beautiful soup webscraper. \n",
    "         It contains the different elements of the html-code of the website.\n",
    "\n",
    "    Returns:\n",
    "        : list(str): List of strings. \n",
    "         Each strings contains the title, subtitle and text of a snippet.\n",
    "    \"\"\"\n",
    "    level_dict = {\n",
    "        \"h1\": 0,\n",
    "        \"h2\": 1,\n",
    "        \"h3\": 2,\n",
    "        \"p\": 3,\n",
    "        \"li\": 3,\n",
    "    }  # hierarchical level from html tags\n",
    "    element_list = soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"])\n",
    "    prev_level = 9999\n",
    "    webpage_snippet_list = []\n",
    "    snippet_texts_list = []\n",
    "    # Pattern of each snippet: h1-title, h2-subtitle, h3-subsubtitle, paragraph-text:\n",
    "    base_element_list = [\"\",\"\", \"\", \"\"]  \n",
    "    for element in element_list:\n",
    "        current_level = level_dict[element.name]\n",
    "\n",
    "        if current_level < prev_level:  # i.e. we are at a new topic\n",
    "            # save the previous snippet as one string\n",
    "            webpage_string_snip = \" \".join(snippet_texts_list)\n",
    "            webpage_string_snip = webpage_string_snip.replace(\"\\n\", \" \")\n",
    "            if webpage_string_snip != \"\":\n",
    "                webpage_snippet_list.append(webpage_string_snip)\n",
    "            # update base_element_list\n",
    "            base_element_list[prev_level:current_level] = \"\"\n",
    "            base_element_list[current_level] = element.text\n",
    "            snippet_texts_list = base_element_list\n",
    "\n",
    "        # update snippet_texts_list\n",
    "        else:\n",
    "            snippet_texts_list = snippet_texts_list + [element.text]\n",
    "        prev_level = current_level\n",
    "    return webpage_snippet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "website_texts = []\n",
    "for html_document_path in os.listdir(\"data/html\"):\n",
    "    soup = BeautifulSoup(\n",
    "        open(\"./data/html/\" + html_document_path), features=\"html.parser\"\n",
    "    )\n",
    "    website_texts_page = parse_website_texts(soup)\n",
    "\n",
    "    website_texts = website_texts + website_texts_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the snippets (heading + subheadings + paragraphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helsana Advocare PLUS Häufig gestellte Fragen    Wer kann diese Versicherung abschliessen?    Sie können die Versicherung abschliessen, wenn Sie folgende Voraussetzungen erfüllen: Sie leben in der Schweiz (offizieller Wohnsitz). Sie haben bereits eine der Zusatzversicherungen TOP, OMNIA oder COMPLETA oder beantragen diese zeitgleich mit Helsana Advocare PLUS.\n",
      "SANA Weitere Zusatzversicherungen TOP  Ihr Zusatz zur Grundversicherung: Wichtige ambulante Leistungen sind gedeckt.\n",
      "BeneFit PLUS Telmed    Bei gesundheitlichen Problemen rufen Sie immer zuerst das unabhängige Zentrum für Telemedizin an: 0800 800 090. Sie erhalten rund um die Uhr medizinische Unterstützung und einen attraktiven Prämienrabatt.    24/7 kostenlose, verbindliche medizinische Telefonberatung     Digitale Services, wie z. B. Symptom-Checker und Videokonsultation     Attraktiver Prämienrabatt  \n",
      "BeneFit PLUS Telmed  Prämie berechnen  Ihre Prämie CHF 0 CHF 500 CHF 300 CHF 500 CHF 1000 CHF 1500 CHF 2000 CHF 2500 eingeschlossen ausgeschlossen\n",
      "BeneFit PLUS Flexmed Weitere Modelle der Grundversicherung BeneFit PLUS Hausarzt  Der Hausarzt oder die HMO-Gruppenpraxis ist Ihre erste Anlaufstelle.\n"
     ]
    }
   ],
   "source": [
    "for text in random.sample(website_texts, 5):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Map each snippet to a vector\n",
    "\n",
    "Here we use another large language model to map each snippet to a vector. This vector represents the meaning/topic of the snippet. I used the FastEmbed model for practical reasons. However, I would recommend to use a more powerful model like this one: [https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.fastembed import FastEmbedEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013905464671552181, 0.038332026451826096, 0.01669456996023655, 0.010435071773827076, -0.01078716292977333]\n"
     ]
    }
   ],
   "source": [
    "# Choosing a suitable embedding model can make a big difference in retrieval performance.\n",
    "# For simplicity, we use the FastEmbedEmbeddings model off-the-shelf.\n",
    "embedder = FastEmbedEmbeddings() \n",
    "\n",
    "# Just to show a vector, we embed the first snippet:\n",
    "embeddings = embedder.embed_documents(website_texts[0])\n",
    "print(embeddings[0][0:5])  # The first 5 dimensions of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Store embeddings in a vector database\n",
    "\n",
    "We could just keep the embeddings in memory or store them in a file. It is however more efficient to store them in a database. I used the open-source chroma database for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma.from_texts(website_texts, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Answering questions using the grounding database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. The manual approach (not recommended, just for understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1. Given a question, map it to a vector and search the database\n",
    "\n",
    "Now we will use the Vector-llm to map the question to a vector. Then we will search the database for the snippets with the most similar vectors. These contain the information that we need to answer the question. \n",
    "\n",
    "First we need a question. Here we ask whether the additional insurance model \"Completa\" of the Helsana health insurance cover the costs of glasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Sind Kosten für meine Brille von Completa gedeckt?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we task the chroma-db to find snippets, which are similar to the topic of the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETA Weitere Zusatzversicherungen COMPLETA PLUS  COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA.\n",
      "COMPLETA Häufig gestellte Fragen    Wann lohnt sich ein Upgrade zu COMPLETA PLUS?     COMPLETA PLUS erweitert den\n",
      "Deckungsumfang von COMPLETA. Die Zusatzversicherung lohnt sich, wenn Ihnen Leistungen rund um Gesundheitsförderung\n",
      "wichtig sind. Sie erhalten unter anderem zusätzliche Kostenbeiträge für Gesundheitsförderung, Komplementärmedizin,\n",
      "Prävention sowie für Brillen, Kontaktlinsen und Augenlaserkorrekturen.\n",
      "COMPLETA Häufig gestellte Fragen    Lohnt sich ein Upgrade zu COMPLETA, wenn ich bereits TOP oder SANA habe?    Wenn Sie\n",
      "gerne möglichst breit abgesichert sind, dann lohnt sich COMPLETA für Sie. Damit schliessen Sie die meisten\n",
      "Deckungslücken der Grundversicherung. COMPLETA vereint die Vorteile von TOP und SANA. Besser noch: Viele Vergütungen\n",
      "sind noch grosszügiger, beispielsweise für medizinische Hilfsmittel oder Präventionsmassnahmen wie Check-ups. Und für\n",
      "Brillen und Kontaktlinsen erhalten Sie gar doppelt so viel Geld erstattet wie mit TOP. Zudem werden Behandlungen im\n",
      "Ausland sowie durch Nichtvertragsärzte unterstützt.\n",
      "COMPLETA Häufig gestellte Fragen    Wer kann COMPLETA abschliessen?    Sie können die Versicherung abschliessen, wenn\n",
      "Sie in der Schweiz wohnhaft sind (offizieller Wohnsitz) und über eine Gesundheitsdeklaration mit positivem\n",
      "Aufnahmebescheid verfügen.\n",
      "COMPLETA Versicherte Leistungen Prämie berechnen  Nachfolgende Leistungen erstatten wir Ihnen ergänzend zu den\n",
      "gesetzlichen Leistungen der Grundversicherung aus der Zusatzversicherung COMPLETA zurück:\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 5\n",
    "similar_docs = chroma_db.similarity_search(question, k=n_neighbors)\n",
    "\n",
    "for doc in similar_docs:\n",
    "    nice_print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2. Prepend the relevant snippets to your question\n",
    "\n",
    "To give our QA-model the information it needs to answer the question, we prepend the relevant snippets to the question. Additionally we add some instructions to the model to make sure it understands what we want from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Du bist ein hilfreicher Assistent. Benutze die Informationen der Helsana Gesundheitsversicherung um die darauf folgende Frage zu beantworten. \n",
      "Die Informationen: 1: COMPLETA Weitere Zusatzversicherungen COMPLETA PLUS  COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. \n",
      "2: COMPLETA Weitere Zusatzversicherungen COMPLETA PLUS  COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. \n",
      "3: COMPLETA Häufig gestellte Fragen    Wann lohnt sich ein Upgrade zu COMPLETA PLUS?     COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. Die Zusatzversicherung lohnt sich, wenn Ihnen Leistungen rund um Gesundheitsförderung wichtig sind. Sie erhalten unter anderem zusätzliche Kostenbeiträge für Gesundheitsförderung, Komplementärmedizin, Prävention sowie für Brillen, Kontaktlinsen und Augenlaserkorrekturen.\n",
      "4: COMPLETA Häufig gestellte Fragen    Wann lohnt sich ein Upgrade zu COMPLETA PLUS?     COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. Die Zusatzversicherung lohnt sich, wenn Ihnen Leistungen rund um Gesundheitsförderung wichtig sind. Sie erhalten unter anderem zusätzliche Kostenbeiträge für Gesundheitsförderung, Komplementärmedizin, Prävention sowie für Brillen, Kontaktlinsen und Augenlaserkorrekturen.\n",
      "5: COMPLETA Häufig gestellte Fragen    Lohnt sich ein Upgrade zu COMPLETA, wenn ich bereits TOP oder SANA habe?    Wenn Sie gerne möglichst breit abgesichert sind, dann lohnt sich COMPLETA für Sie. Damit schliessen Sie die meisten Deckungslücken der Grundversicherung. COMPLETA vereint die Vorteile von TOP und SANA. Besser noch: Viele Vergütungen sind noch grosszügiger, beispielsweise für medizinische Hilfsmittel oder Präventionsmassnahmen wie Check-ups. Und für Brillen und Kontaktlinsen erhalten Sie gar doppelt so viel Geld erstattet wie mit TOP. Zudem werden Behandlungen im Ausland sowie durch Nichtvertragsärzte unterstützt.\n",
      "Frage: Sind Kosten für meine Brille von Completa gedeckt? Assistant:\n"
     ]
    }
   ],
   "source": [
    "docs_in_prompt = \"\"\n",
    "for id, doc in enumerate(similar_docs):\n",
    "    docs_in_prompt += str(id + 1) + \": \" + doc.page_content + \"\\n\"\n",
    "context_instruction = \"\"\"Du bist ein hilfreicher Assistent. Benutze die Informationen der Helsana Gesundheitsversicherung um die darauf folgende Frage zu beantworten.\"\"\"\n",
    "augmented_prompt = f\"{context_instruction} \\nDie Informationen: {docs_in_prompt}Frage: {question} Assistant:\"\n",
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could use this prompt and send it to out QA-llm. But there is a better automated way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. The automated approach (recommended)\n",
    "\n",
    "Now we will try out different QA-llms to answer the question and see which model performs the best.\n",
    "\n",
    "We will try:\n",
    "\n",
    "* OpenAI ChatGPT-3.5\n",
    "* Llama2 70B\n",
    "* Mistral 7B and Mistral 7B tuned on German texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI GPT-3.5\n",
    "\n",
    "Currently one of the strongest but also biggest models. We can only run it via an API-request to the OpenAI-Server. \n",
    "You can find advice for prompting the model here: [Promptingguide.ai: chatgpt](https://www.promptingguide.ai/models/chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the OpenAI API, you need to register ([OpenAI API](https://openai.com/blog/openai-api)) and save the API-key as an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opneai_llm = OpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since langchain lets us swap out models easily, we write one query-function for all our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(model, prompt_template):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        model,\n",
    "        retriever=chroma_db.as_retriever(\n",
    "            search_type=\"mmr\", search_kwargs={\"k\": 6, \"lambda_mult\": 0.25}\n",
    "        ),\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    )\n",
    "    response = qa_chain({\"query\": question})\n",
    "    nice_print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a prompt-template where we can plug-in our question and context. As with the manual approach, the QA-model gets our context and instructions to answer our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ja, Kosten für eine Brille von Completa sind gedeckt, sofern die Angaben über die Brillenstärke auf der Rechnung\n",
      "ausgewiesen sind.\n"
     ]
    }
   ],
   "source": [
    "openai_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Du bist ein hilfreicher Assistent der Fragen beantwortet. Benutze die folgenden Stücke von Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du es nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten.  \n",
    "    ### Frage: {question} \n",
    "    ### Context: {context} \n",
    "    ### Antwort: \"\"\"\n",
    ")\n",
    "\n",
    "query_llm(opneai_llm, openai_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer looks pretty correct to me. Only missing is the information that the insurance covers the costs up to 300 CHF per annum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLama-2 70B\n",
    "\n",
    "This is a large model but still smaller than GPT-3.5. It is still too big to run it on our machine. Therefore we will run it in the cloud (in our case on an Nvidia A100 GPU on [replicate.com](https://www.replicate.com)). You can find more info on LLama here: [Hugging Face: Llama2](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_llm = Replicate(\n",
    "    model=\"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    model_kwargs={\"temperature\": 0.75},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models profit from different prompt formats. Thus, we create a different template here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ja, die Kosten für Ihre Brille sind von Completa gedeckt. According to the information provided, you will receive 90%\n",
      "of the costs up to a maximum of 150 francs per calendar year for your glasses or contact lenses. However, it is\n",
      "important to note that there may be a deductible and coinsurance applicable to the coverage. Additionally, if you have\n",
      "already exhausted your annual limit for glasses or contact lenses under your TOP policy, you may not be able to claim\n",
      "the full amount under Completa. It's always best to check\n"
     ]
    }
   ],
   "source": [
    "llama_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"[INST] Du bist ein hilfreicher Assistent. Benutze den folgenden Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du die Antwort nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten. \n",
    "    Frage: {question} \n",
    "    Context: {context} \n",
    "    Antwort:[/INST]\"\"\"\n",
    ")\n",
    "\n",
    "query_llm(llama_llm, llama_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct. However it suddenly starts writing in English. Let's try to fix this by using few shot-learning: We will provide it a few examples of questions and answers in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ja, die Kosten für Ihre Brille sind von Completa gedeckt. Sie erhalten 90% der Kosten bis max. 150 Franken pro\n",
      "Kalenderjahr für Ihre Brillengläser und Kontaktlinsen. Wenn Sie jedoch eine Upgrade zu COMPLETA PLUS erwogen, lohnt sich\n",
      "dies, da Viele Vergütungen noch grosszügiger sind, wie zum Beispiel für medizinische Hilfsmittel oder\n",
      "Präventionsmassnahmen wie Check-ups. Und für Brillen und Kontaktlinsen erhalten Sie\n"
     ]
    }
   ],
   "source": [
    "llama_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"[INST] Du bist ein hilfreicher Assistent. Benutze den folgenden Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du die Antwort nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten. Antworte ausschliesslich auf Deutsch. Beispiel \n",
    "    Frage: Sind Notfälle im Ausland vom Modell BeneFit PLUS abgedeckt? \n",
    "    Context: Bei einem Notfall erhalten Sie in den EU/EFTA-Staaten den jeweiligen Sozialtarif. In allen übrigen Ländern erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. \n",
    "    Antwort:[/INST] Ja, Notfälle im Ausland sind abgedeckt. In nicht-EU/EFTA-Staaten erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. [INST]\n",
    "    Frage: Ich bin Hospital Privat versichert. Ich hatte einen Unfall und war auf eine Haushaltshilfe angewiesen. Erstattet mir die Helsana die Kosten zurück? \n",
    "    Context: Kann ein akuter stationärer Spital- oder Kuraufenthalt durch eine ärztlich verordnete Haushaltshilfe verhindert oder zumindest verkürzt werden, erhalten Sie bis max. 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. \n",
    "    Antwort: [/INST] Ja, die Helsana erstattet Ihnen bis zu 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr.[INST] \n",
    "    Frage: {question} \n",
    "    Context: {context} \n",
    "    Antwort: [/INST]\"\"\"\n",
    ")\n",
    "\n",
    "query_llm(llama_llm, llama_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already much better. For this specific question, the model gives a correct answer with all the relevant context. There is just one wrong word in the answer (erwogen). Let's go one step smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mistral 7B\n",
    "\n",
    "This is a much smaller model than GPT-3.5 but still pretty strong. We could run this model on a standard notebook (16GB RAM required), but it is faster to run it on GPUs in the cloud (in our case on a Nvidia A40 GPU on replicate.com). Using GPUs generated a speedup of up to 50x in our case. More info here: [Mistral: Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Replicate\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_llm = Replicate(\n",
    "    model=\"mistralai/mistral-7b-instruct-v0.1:83b6a56e7c828e667f21fd596c338fd4f0039b46bcfa18d973e8e70e455fda70\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 500},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the few-shot learning approach directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja, Brillen und Kontaktlinsen von Completa sind von COMPLETA gedeckt. Für Brillengläser und Kontaktlinsen erhalten Sie\n",
      "bis zu 90% der Kosten bis zu 150 Franken pro Jahr. Für Kinder und Jugendliche bis 18 Jahre erhalten Sie 180 Franken pro\n",
      "Jahr.\n"
     ]
    }
   ],
   "source": [
    "mistral_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"<s>[INST] Du bist ein hilfreicher Assistent. Benutze den folgenden Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du die Antwort nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten. Beispiel \n",
    "    Frage: Sind Notfälle im Ausland vom Modell BeneFit PLUS abgedeckt? \n",
    "    Context: Bei einem Notfall erhalten Sie in den EU/EFTA-Staaten den jeweiligen Sozialtarif. In allen übrigen Ländern erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. \n",
    "    Antwort:[/INST] Ja, Notfälle im Ausland sind abgedeckt. In nicht-EU/EFTA-Staaten erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. [INST]\n",
    "    Frage: Ich bin Hospital Privat versichert. Ich hatte einen Unfall und war auf eine Haushaltshilfe angewiesen. Erstattet mir die Helsana die Kosten zurück? \n",
    "    Context: Kann ein akuter stationärer Spital- oder Kuraufenthalt durch eine ärztlich verordnete Haushaltshilfe verhindert oder zumindest verkürzt werden, erhalten Sie bis max. 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. \n",
    "    Antwort: [/INST] Ja, die Helsana erstattet Ihnen bis zu 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. </s> [INST] \n",
    "    Frage: {question} \n",
    "    Context: {context} \n",
    "    Antwort: [/INST]\"\"\"\n",
    ")\n",
    "\n",
    "query_llm(mistral_llm, mistral_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer is already better but still a bit misleading. (And I have to admit that I generated a few more incomplete answers.) Let us try to use a model that is tuned on German texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### German Mistral 7B\n",
    "\n",
    "Let us use a version of Mistral that has been tuned for German texts. Maybe it will perform better.\n",
    "More info on the model here: [Hugging Face: em_german_leo_mistral](https://huggingface.co/jphme/em_german_leo_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we will run it on our own device:\n",
    "german_mistral_llm = Ollama(\n",
    "    model=\"germanleo\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Ja, die Kosten für Ihre Brille von Completa sind gedeckt. Kompleta PLUS erweitert den Deckungsumfang und lohnt sich insbesondere dann, wenn Sie gerne möglichst breit abgesichert sind.\" \"Ja, die Kosten für Ihre Brille von Completa sind gedeckt. Kompleta PLUS erweitert den Deckungsumfang und lohnt sich\n",
      "insbesondere dann, wenn Sie gerne möglichst breit abgesichert sind.\"\n"
     ]
    }
   ],
   "source": [
    "german_mistral_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Du bist ein hilfreicher Assistent. Für die folgende Aufgabe stehen dir zwischen den tags BEGININPUT und ENDINPUT mehrere Quellen zur Verfügung. Die eigentliche Aufgabe oder Frage ist zwischen BEGININSTRUCTION und ENDINCSTRUCTION zu finden. Beantworte diese wortwörtlich mit einem Zitat aus den Quellen. Sollten diese keine Antwort enthalten, antworte, dass auf Basis der gegebenen Informationen keine Antwort möglich ist!\n",
    "    BEGININSTRUCTION Sind Notfälle im Ausland vom Modell BeneFit PLUS abgedeckt? ENDINCSTRUCTION BEGININPUT Bei einem Notfall erhalten Sie in den EU/EFTA-Staaten den jeweiligen Sozialtarif. In allen übrigen Ländern erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. ENDINPUT \n",
    "    ASSISTANT: \"Ja, Notfälle im Ausland sind abgedeckt. In nicht-EU/EFTA-Staaten erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages.\"\n",
    "    BEGININSTRUCTION Ich bin Hospital Privat versichert. Ich hatte einen Unfall und war auf eine Haushaltshilfe angewiesen. Erstattet mir die Helsana die Kosten zurück? ENDINCSTRUCTION BEGININPUT Kann ein akuter stationärer Spital- oder Kuraufenthalt durch eine ärztlich verordnete Haushaltshilfe verhindert oder zumindest verkürzt werden, erhalten Sie bis max. 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. ENDINPUT \n",
    "    ASSISTANT: \"Ja, die Helsana erstattet Ihnen bis zu 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. Voraussetzung ist eine ärztliche Verordnung.\" \n",
    "    BEGININSTRUCTION {question} ENDINCSTRUCTION\n",
    "    BEGININPUT {context} ENDINPUT \n",
    "    ASSISTANT: \"\"\"\n",
    ")\n",
    "\n",
    "query_llm(german_mistral_llm, german_mistral_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already a bit better. The answer contains irrelevant information and some important information is missing. \n",
    "\n",
    "**A side-mark:** \n",
    "\n",
    "On my device (without using possible optimizations), the model took 3:34 minutes to answer the questions. So even for a small model like this, it is not really feasible to use it in a chatbot on commodity hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- Grounding: \n",
    "    - Grounding your model helps it giving you correct answers to very specific questions.\n",
    "    - Formatting the text in a way that the model can understand it is crucial.\n",
    "- Larger Models perform better:\n",
    "    - GPT3.5 is better at answering questions off-the-shelf.\n",
    "    - Llama-2 70B is able to answer the questions correctly. However, it struggles to provide correct German texts.\n",
    "    - Mistral 7B struggles to answer questions correctly. More effort than I took is required.\n",
    "- Different models need different prompt templates.\n",
    "- With few-shot learning you can improve the quality of the response dramatically.\n",
    "- A high-level framework like langchain makes switching models and sources easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook\n",
    "\n",
    "- **New Models:** \n",
    "    - Just today Mistral released a new multi-lingual model with very impressive reported performance. It's size is between Mistral 7B and LLama2 70B. This might solve our problems with Mistral: [Mistral: Mixtral of experts](https://mistral.ai/news/mixtral-of-experts/)\n",
    "    - There are LLama-Models that have been pretrained on German texts and might provide a better performance with our German prompts and responses: [Huggingface: LLama2 German Assistant](https://huggingface.co/TheBloke/llama-2-13B-German-Assistant-v2-GGML)\n",
    "- **Fine Tuning models:** Since the models from Mistral are substantially smaller, they can more easily be retrained (fine-tuned) to achieve higher accuracy. Furthermore, parameter tuning could help lessening hallucinations of the model (i.e. making things up). \n",
    "- **Benchmarking:** To properly evaluate the performance of the different approaches (Vector-LLM and QA-LLM), a larger test-set and requirements for correct answers would be needed. \n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "A special thank you to Moritz Settele and Koen Tersago from *morrow ventures* for their helpful feedback on this blog-post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
