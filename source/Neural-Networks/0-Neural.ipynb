{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Neural Networks\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "---"
      ],
      "id": "459bf80d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks \n",
        "\n",
        "## Fundamentals\n",
        "\n",
        "On the most basic level, neural networks consist of many simple models\n",
        "(e.g. linear and logistic models) that are chained together in a\n",
        "directed network. The models sit on the neurons (nodes) of the network.\n",
        "The most important components of neurons are:\n",
        "\n",
        "1.  **Activation**: $a = Wx+b$ ($W$ = weights and $b$ = bias)\n",
        "\n",
        "2.  **Non-linearity**: $f(x, \\theta)=\\sigma(a)$ (e.g. a sigmoid function\n",
        "    for logistic regression, giving you a probability output. $\\theta$\n",
        "    is a threshold)\n",
        "\n",
        "The neurons (nodes) in the first layer uses as its input the sample\n",
        "values and feeds its output into the activation function of the next\n",
        "nodes in the next layer, a.s.o. The later layers should thereby learn\n",
        "more and more complicated concepts or structures.\n",
        "\n",
        "![Model of an artificial neural network with one hidden layer. *Figure\n",
        "from [user LearnDataSci on\n",
        "wikimedia.org](https://commons.wikimedia.org/wiki/File:Artificial_Neural_Network.jpg).*](/figures/Artificial_Neural_Network.jpg){#CDF\n",
        "width=\"70%\"}\n",
        "\n",
        "Explanation on the idea and mechanisms of neural networks: [Stanford Computer Vision Class](https://cs231n.github.io/neural-networks-1/)\n",
        "\n",
        "### Non-Linearities\n",
        "\n",
        "Different non-linear functions can be used to generate the output of the\n",
        "neurons.\n",
        "\n",
        "#### Sigmoid/Logistic Functions\n",
        "\n",
        "This activation function is often used in the last layer of NNs for classification (since it scales the output between 0 and 1).\n",
        "\n",
        "$$f(x) = \\frac{1}{1+e^{-x}}$$\n"
      ],
      "id": "f94a554e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "x = np.arange(-6, 6, 0.1)\n",
        "f = 1 / (1+math.e**(-x))\n",
        "\n",
        "sns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\n",
        "sns.lineplot(x=x, y=f, )\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')"
      ],
      "id": "94bdc2b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros:**\n",
        "\n",
        "- Scales output between 0 and 1 (good for output layer in classification tasks)\n",
        "\n",
        "- Outputs are bound between 0 and 1 $\\rightarrow$ No explosion of activations \n",
        "\n",
        "**Cons:**\n",
        "\n",
        "- No saturation / dying neuron / vanishing gradient: When f(x) = 0 or 1, the gradient of f(x) is 0. This blocks back-propagation (see [here](https://medium.com/analytics-vidhya/comprehensive-synthesis-of-the-main-activation-functions-pros-and-cons-dab105fe4b3b))\n",
        "\n",
        "- Output not centered around 0: All weight-updates during backpropagation are either positive or negative, leading to zig-zag SGD instead of direct descent to optimum (see [here](https://rohanvarma.me/inputnormalization/) or [here](https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation))\n",
        "\n",
        "- computationally more expensive than ReLu\n",
        "\n",
        "\n",
        "#### Tanh Functions\n",
        "\n",
        "$$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n"
      ],
      "id": "a13ad34f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "x = np.arange(-6, 6, 0.1)\n",
        "f = (math.e**x - math.e**(-x) ) / (math.e**(x)+math.e**(-x))\n",
        "\n",
        "sns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\n",
        "sns.lineplot(x=x, y=f, )\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')"
      ],
      "id": "d585a347",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros:**\n",
        "\n",
        "- Centered around zero\n",
        "\n",
        "**Cons:** \n",
        "\n",
        "- saturation / dying neuron / vanishing gradient problem\n",
        "\n",
        "- computationally more expensive than ReLu\n",
        "\n",
        "#### Rectifiers/ReLU\n",
        "\n",
        "$$f(x) = \\max(0,x)$$\n"
      ],
      "id": "722a3c52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "x = np.arange(-6, 6, 0.1)\n",
        "f = [max(0,x_i) for x_i in x] \n",
        "\n",
        "sns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\n",
        "sns.lineplot(x=x, y=f, )\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')"
      ],
      "id": "6b093452",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros:**\n",
        "\n",
        "- Computationally cheap\n",
        "\n",
        "- No saturation for positive values\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "- Not zero centered\n",
        "\n",
        "- Saturation for negative values\n",
        "\n",
        "#### Leaky ReLU\n",
        "\n",
        "$$f(x) = \\begin{cases}\n",
        "        \\alpha x \\text{ if } x < 0 \\\\\n",
        "        x \\text{ if } x \\ge 0\n",
        "        \\end{cases}\n",
        "$$\n"
      ],
      "id": "f6127db1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "a = 0.1\n",
        "x = np.arange(-6, 6, 0.1)\n",
        "f = [(a*x_i if x_i < 0 else x_i) for x_i in x] \n",
        "\n",
        "sns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\n",
        "sns.lineplot(x=x, y=f, )\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')"
      ],
      "id": "69b46a9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros:**\n",
        "\n",
        "- No saturation problem\n",
        "\n",
        "- fast to compute\n",
        "\n",
        "- more zero-centered than e.g. sigmoid-activation\n",
        "\n",
        "### Terminology\n",
        "\n",
        "-   **Input layer/visible layer:** Input variables\n",
        "\n",
        "-   **Hidden layer:** Layers of nodes between input and output layer\n",
        "\n",
        "-   **Output layer:** Layer of nodes that produce output variables\n",
        "\n",
        "-   **Size:** Number of nodes in the network\n",
        "\n",
        "-   **Width:** Number of nodes in a layer\n",
        "\n",
        "-   **Depth:** Number of layers\n",
        "\n",
        "-   **Capacity:** The type of functions that can be learned by the\n",
        "    network\n",
        "\n",
        "-   **Architecture:** The arrangement of layers and nodes in the network\n",
        "\n",
        "### Feedforward Neural Network / Multi-Layer Perceptron\n",
        "\n",
        "This is the simplest type of proper neural networks. Each neuron of a\n",
        "layer is connected to each neuron of the next layer and there are no\n",
        "cycles. The outputs of the previous layer corresponds to the $x$ in the\n",
        "activation function. Each output ($x_i$) of the previous layer gets it's\n",
        "own weight ($w_i$) in each node and a bias ($b$) is added to each node.\n",
        "Neurons with a very high output are \"active\" neurons, those with\n",
        "negative outputs are \"inactive\". The result is mapped to the probability\n",
        "range by (commonly) a sigmoid function. The output is then again given\n",
        "to the next layer.\\\n",
        "If your input layer has 6400 features (80\\*80 image), a network with 2\n",
        "hidden layers of 16 nodes will have\n",
        "$6400*16+16*16+16*10+16+16+10 = 102'858$ parameters. This is a very high\n",
        "number of degrees of freedom and requires a lot of training samples.\n",
        "\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "### PyTorch\n",
        "```python\n",
        "from torch import nn\n",
        "\n",
        "    class CustomNet(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(CustomNet, self).__init__()\n",
        "            self.lin_layer_1 = nn.Linear(in_features=10, out_features=10)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.lin_layer_2 = nn.Linear(in_features=10, out_features=10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.lin_layer_1(x)\n",
        "            x = self.relu\n",
        "            x = self.lin_layer_2(x)\n",
        "            return x\n",
        "\n",
        "        def num_flat_features(self, x):\n",
        "            size = x.size()[1:] # Use all but the batch dimension\n",
        "            num = 1\n",
        "            for i in size:\n",
        "                num *= i\n",
        "            return num\n",
        "\n",
        "    new_net = CustomNet()\n",
        "```\n",
        "\n",
        "### Keras\n",
        "\n",
        "Example of a small Keras model for text-classification.\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 20 \n",
        "sequence_length = 50\n",
        "vocab_size = 5000 # length of word index / corpus\n",
        "\n",
        "# Specify model:\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=sequence_length))\n",
        "model.add(layers.SpatialDropout1D(0.1)) # Against overfitting\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train model:\n",
        "history = model.fit(train_texts_padded, \n",
        "                    y_train,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=(test_texts_padded, y_test),\n",
        "                    batch_size=100)\n",
        "loss, accuracy = model.evaluate(train_texts_padded, y_train)\n",
        "print(\"Accuracy training: {:.3f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(test_texts_padded, y_test)\n",
        "print(\"Accuracy test:  {:.3f}\".format(accuracy))\n",
        "```\n",
        "\n",
        "::: \n",
        "\n",
        "### Backpropagation \n",
        "\n",
        "This is the method by which neural networks learn the optimal weights\n",
        "and biases of the nodes. The components are a cost function and a\n",
        "gradient descent method.\\\n",
        "The cost function analyses the difference between the designated\n",
        "activation in the output layer (according to the label of the data) and\n",
        "the actual activation of that layer. Commonly a residual sum of squares\n",
        "is used.\\\n",
        "You get the direction of the next best parameter-combination by using a\n",
        "*stochastic gradient descent* algorithm using the gradient for your cost\n",
        "function:\n",
        "\n",
        "1.  We use a \"mini-batch\" of samples for each round/step of the gradient\n",
        "    descent.\n",
        "\n",
        "2.  We calculate squared residual of each feature of the output layer\n",
        "    for each sample.\n",
        "\n",
        "3.  From that we calculate what the bias or weights from the output\n",
        "    layer and the activation from the last hidden layer must have been\n",
        "    to get this result. We average that out for all images in our\n",
        "    mini-batch.\n",
        "\n",
        "4.  From that we calculate the weights, biases and activations of the\n",
        "    upstream layers $\\rightarrow$ we *backpropagate*.\n",
        "\n",
        "### Initialization\n",
        "\n",
        "The weights of the nodes are commonly initialized randomly with a certain distribution. The biases are commonly initialized as zero, thus 0-centering of the input data is recommended. \n",
        "\n",
        "## Types of NNs\n",
        "\n",
        "### Convolutional Neural Networks\n",
        "\n",
        "### Autoencoders\n",
        "\n",
        "Contrary to the other architectures, autoencoders are used for\n",
        "unsupervised learning. Their goal is to compress and decompress data to\n",
        "learn the most important structures of the data. The layers therefore\n",
        "become smaller for the encoding step and the later layers get bigger\n",
        "again, up to the original representation of the data. The optimization\n",
        "problem is now:\n",
        "$$\\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2$$ with $x_i$\n",
        "being the original datapoint and $\\hat{x}_i$ the reconstructed\n",
        "datapoint.\n",
        "\n",
        "![Model of an autoencoder. The encoder layers compress the data towards\n",
        "the code layer, the decoder layers decompress the data again. *Figure\n",
        "from [Michela Massi on\n",
        "wikimedia.org](https://commons.wikimedia.org/wiki/File:Autoencoder_schema.png).*](/figures/Autoencoder_schema.png){width=\"50%\"}\n",
        "\n",
        "#### Autoencoders for clustering\n",
        "\n",
        "You can look at layers of a NN as ways to represent data in different\n",
        "form of complexity and compactness. The code layers of autoencoders are\n",
        "a very compact way to represent the data. You can then use the\n",
        "compressed representation of the code layer and do clustering on that\n",
        "data. Because the code layer is however not optimized for that task. [Song et al.](https://www.semanticscholar.org/paper/Auto-encoder-Based-Data-Clustering-Song-Liu/27a2c94a310d20ccae9c98e0f38d7684a16f9e61)\n",
        "combined the cost function of the **autoencoder and k-means\n",
        "clustering**:\n",
        "$$\\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda \\sum_{i=1}^N ||f(x_i) - c_i||^2$$\n",
        "with $f(x_i)$ being the non-linearity of the code layer and $\\lambda$ is\n",
        "a weight constant.\\\n",
        "XXXX adapted spectral clustering (section\n",
        "[3.3](#Spectral%20Clustering)) using autoencoders by replacing the\n",
        "(linear) eigen-decomposition with the (non-linear) decomposition by the\n",
        "encoder. As in spectral clustering the Laplacian matrix is used as the\n",
        "the input to the decomposition step (encoder) and the compressed\n",
        "representation (code-layer) is fed into k-means clustering.\\\n",
        "**Deep subspace clustering** by [Pan et al.](https://arxiv.org/abs/1709.02508) employs autoencoders combined with\n",
        "sparse subspace clustering. They used autoencoders and optimized for a compact\n",
        "representation of the code layer: $$\\begin{split}\n",
        "                \\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda ||V||_1 \\\\\n",
        "                \\text{s.t.} F(X) = F(X)*V \\text{ and diag}(V)=0\n",
        "            \\end{split}$$ with V being the sparse representation of the\n",
        "code layer ($F(X)$) .\n",
        "\n",
        "<font color=\"grey\">\n",
        "\n",
        "### Generative adversarial networks\n",
        "\n",
        "### Recurrent neural networks\n",
        "\n",
        "### Long short-term memory networks\n",
        "\n",
        "## Learnig methods\n",
        "\n",
        "There are specific methods for learning in neural networks. \n",
        "\n",
        "### Transfer learning {#transfer-learning}\n",
        "\n",
        "\n",
        "\n",
        "### Domain adaptation\n",
        "\n",
        "\n",
        "\n",
        "</font>"
      ],
      "id": "93f78377"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}