{
  "hash": "a5c0df88dc522a888d9a932b3dfd269d",
  "result": {
    "markdown": "---\ntitle: Neural Networks\nformat:\n  html:\n    code-fold: true\n---\n\n# Neural Networks\n\n## Fundamentals\n\nOn the most basic level, neural networks consist of many simple models (e.g. linear and logistic models) that are chained together in a directed network. The models sit on the neurons (nodes) of the network. The most important components of neurons are:\n\n1.  **Activation**: $a = Wx+b$ ($W$ = weights and $b$ = bias)\n\n2.  **Non-linearity**: $f(x, \\theta)=\\sigma(a)$ (e.g. a sigmoid function for logistic regression, giving you a probability output. $\\theta$ is a threshold)\n\nThe neurons (nodes) in the first layer uses as its input the sample values and feeds its output into the activation function of the next nodes in the next layer, a.s.o. The later layers should thereby learn more and more complicated concepts or structures.\n\n![Model of an artificial neural network with one hidden layer. *Figure from [user LearnDataSci on wikimedia.org](https://commons.wikimedia.org/wiki/File:Artificial_Neural_Network.jpg).*](/figures/Artificial_Neural_Network.jpg){#CDF width=\"70%\"}\n\nExplanation on the idea and mechanisms of neural networks: [Stanford Computer Vision Class](https://cs231n.github.io/neural-networks-1/)\n\n### Non-Linearities\n\nDifferent non-linear functions can be used to generate the output of the neurons.\n\n#### Sigmoid/Logistic Functions\n\nThis activation function is often used in the last layer of NNs for classification (since it scales the output between 0 and 1).\n\n$$f(x) = \\frac{1}{1+e^{-x}}$$\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nx = np.arange(-6, 6, 0.1)\nf = 1 / (1+math.e**(-x))\n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\nText(0, 0.5, 'f(x)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](0-Neural_files/figure-html/cell-2-output-2.png){width=303 height=214}\n:::\n:::\n\n\n**Pros:**\n\n-   Scales output between 0 and 1 (good for output layer in classification tasks)\n\n-   Outputs are bound between 0 and 1 $\\rightarrow$ No explosion of activations\n\n**Cons:**\n\n-   No saturation / dying neuron / vanishing gradient: When f(x) = 0 or 1, the gradient of f(x) is 0. This blocks back-propagation (see [here](https://medium.com/analytics-vidhya/comprehensive-synthesis-of-the-main-activation-functions-pros-and-cons-dab105fe4b3b))\n\n-   Output not centered around 0: All weight-updates during backpropagation are either positive or negative, leading to zig-zag SGD instead of direct descent to optimum (see [here](https://rohanvarma.me/inputnormalization/) or [here](https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation))\n\n-   computationally more expensive than ReLu\n\n#### Tanh Functions\n\n$$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nx = np.arange(-6, 6, 0.1)\nf = (math.e**x - math.e**(-x) ) / (math.e**(x)+math.e**(-x))\n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n```\n\n::: {.cell-output .cell-output-display execution_count=62}\n```\nText(0, 0.5, 'f(x)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](0-Neural_files/figure-html/cell-3-output-2.png){width=304 height=214}\n:::\n:::\n\n\n**Pros:**\n\n-   Centered around zero\n\n**Cons:**\n\n-   saturation / dying neuron / vanishing gradient problem\n\n-   computationally more expensive than ReLu\n\n#### Rectifiers/ReLU\n\n$$f(x) = \\max(0,x)$$\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nx = np.arange(-6, 6, 0.1)\nf = [max(0,x_i) for x_i in x] \n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\nText(0, 0.5, 'f(x)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](0-Neural_files/figure-html/cell-4-output-2.png){width=283 height=215}\n:::\n:::\n\n\n**Pros:**\n\n-   Computationally cheap\n\n-   No saturation for positive values\n\n**Cons:**\n\n-   Not zero centered\n\n-   Saturation for negative values\n\n#### Leaky ReLU\n\n$$f(x) = \\begin{cases}\n        \\alpha x \\text{ if } x < 0 \\\\\n        x \\text{ if } x \\ge 0\n        \\end{cases}\n$$\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\na = 0.1\nx = np.arange(-6, 6, 0.1)\nf = [(a*x_i if x_i < 0 else x_i) for x_i in x] \n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```\nText(0, 0.5, 'f(x)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](0-Neural_files/figure-html/cell-5-output-2.png){width=283 height=214}\n:::\n:::\n\n\n**Pros:**\n\n-   No saturation problem\n\n-   fast to compute\n\n-   more zero-centered than e.g. sigmoid-activation\n\n### Terminology\n\n-   **Input layer/visible layer:** Input variables\n\n-   **Hidden layer:** Layers of nodes between input and output layer\n\n-   **Output layer:** Layer of nodes that produce output variables\n\n-   **Size:** Number of nodes in the network\n\n-   **Width:** Number of nodes in a layer\n\n-   **Depth:** Number of layers\n\n-   **Capacity:** The type of functions that can be learned by the network\n\n-   **Architecture:** The arrangement of layers and nodes in the network\n\n### Feedforward Neural Network / Multi-Layer Perceptron\n\nThis is the simplest type of proper neural networks. Each neuron of a layer is connected to each neuron of the next layer and there are no cycles. The outputs of the previous layer corresponds to the $x$ in the activation function. Each output ($x_i$) of the previous layer gets it's own weight ($w_i$) in each node and a bias ($b$) is added to each node. Neurons with a very high output are \"active\" neurons, those with negative outputs are \"inactive\". The result is mapped to the probability range by (commonly) a sigmoid function. The output is then again given to the next layer.\\\nIf your input layer has 6400 features (80\\*80 image), a network with 2 hidden layers of 16 nodes will have $6400*16+16*16+16*10+16+16+10 = 102'858$ parameters. This is a very high number of degrees of freedom and requires a lot of training samples.\n\n::: panel-tabset\n### PyTorch\n\n``` python\nfrom torch import nn\n\n    class CustomNet(nn.Module):\n        def __init__(self):\n            super(CustomNet, self).__init__()\n            self.lin_layer_1 = nn.Linear(in_features=10, out_features=10)\n            self.relu = nn.ReLU()\n            self.lin_layer_2 = nn.Linear(in_features=10, out_features=10)\n\n        def forward(self, x):\n            x = self.lin_layer_1(x)\n            x = self.relu\n            x = self.lin_layer_2(x)\n            return x\n\n        def num_flat_features(self, x):\n            size = x.size()[1:] # Use all but the batch dimension\n            num = 1\n            for i in size:\n                num *= i\n            return num\n\n    new_net = CustomNet()\n```\n\n### Keras\n\nExample of a small Keras model for text-classification.\n\n``` python\nfrom keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 20 \nsequence_length = 50\nvocab_size = 5000 # length of word index / corpus\n\n# Specify model:\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=sequence_length))\nmodel.add(layers.SpatialDropout1D(0.1)) # Against overfitting\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n\n# Train model:\nhistory = model.fit(train_texts_padded, \n                    y_train,\n                    epochs=5,\n                    verbose=True,\n                    validation_data=(test_texts_padded, y_test),\n                    batch_size=100)\nloss, accuracy = model.evaluate(train_texts_padded, y_train)\nprint(\"Accuracy training: {:.3f}\".format(accuracy))\nloss, accuracy = model.evaluate(test_texts_padded, y_test)\nprint(\"Accuracy test:  {:.3f}\".format(accuracy))\n```\n:::\n\n### Backpropagation\n\nThis is the method by which neural networks learn the optimal weights and biases of the nodes. The components are a cost function and a gradient descent method.\\\nThe cost function analyses the difference between the designated activation in the output layer (according to the label of the data) and the actual activation of that layer. Commonly a residual sum of squares is used.\\\nYou get the direction of the next best parameter-combination by using a *stochastic gradient descent* algorithm using the gradient for your cost function:\n\n1.  We use a \"mini-batch\" of samples for each round/step of the gradient descent.\n\n2.  We calculate squared residual of each feature of the output layer for each sample.\n\n3.  From that we calculate what the bias or weights from the output layer and the activation from the last hidden layer must have been to get this result. We average that out for all images in our mini-batch.\n\n4.  From that we calculate the weights, biases and activations of the upstream layers $\\rightarrow$ we *backpropagate*.\n\n### Initialization\n\nThe weights of the nodes are commonly initialized randomly with a certain distribution. The biases are commonly initialized as zero, thus 0-centering of the input data is recommended.\n\n## Types of NNs\n\n### Convolutional Neural Networks\n\n### Encoder-Decoder Models\n\n#### Autoencoder models\n\nContrary to the other architectures, autoencoders are used for unsupervised learning. Their goal is to compress and decompress data to learn the most important structures of the data. The layers therefore become smaller for the encoding step and the later layers get bigger again, up to the original representation of the data. The optimization problem is now: $$\\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2$$ with $x_i$ being the original datapoint and $\\hat{x}_i$ the reconstructed datapoint.\n\n![Model of an autoencoder. The encoder layers compress the data towards the code layer, the decoder layers decompress the data again. *Figure from [Michela Massi on wikimedia.org](https://commons.wikimedia.org/wiki/File:Autoencoder_schema.png).*](/figures/Autoencoder_schema.png){width=\"50%\"}\n\n#### Autoencoders for clustering\n\nYou can look at layers of a NN as ways to represent data in different form of complexity and compactness. The code layers of autoencoders are a very compact way to represent the data. You can then use the compressed representation of the code layer and do clustering on that data. Because the code layer is however not optimized for that task. [Song et al.](https://www.semanticscholar.org/paper/Auto-encoder-Based-Data-Clustering-Song-Liu/27a2c94a310d20ccae9c98e0f38d7684a16f9e61) combined the cost function of the **autoencoder and k-means clustering**: $$\\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda \\sum_{i=1}^N ||f(x_i) - c_i||^2$$ with $f(x_i)$ being the non-linearity of the code layer and $\\lambda$ is a weight constant.\\\nXXXX adapted spectral clustering (section [3.3](#Spectral%20Clustering)) using autoencoders by replacing the (linear) eigen-decomposition with the (non-linear) decomposition by the encoder. As in spectral clustering the Laplacian matrix is used as the the input to the decomposition step (encoder) and the compressed representation (code-layer) is fed into k-means clustering.\\\n**Deep subspace clustering** by [Pan et al.](https://arxiv.org/abs/1709.02508) employs autoencoders combined with sparse subspace clustering. They used autoencoders and optimized for a compact representation of the code layer: $$\\begin{split}\n                \\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda ||V||_1 \\\\\n                \\text{s.t.} F(X) = F(X)*V \\text{ and diag}(V)=0\n            \\end{split}$$ with V being the sparse representation of the code layer ($F(X)$) .\n\n<font color=\"grey\">\n\n### Generative adversarial networks\n\n### Recurrent neural networks (RNN)\n\nCompared to the fully connected or convolutional neural networks, RNNs can work on variable length inputs without padding the sequences.\n\n![A unit of an RNN. The left side shows a static view of a unit $h$. The right side shows how the activations of past inputs ($x_{t-1}$) influence the output ($o_t$) of the current input ($x_t$). $U$, $V$ and $W$ are weights (Beware: They stay the same for different inputs). *Figure from [fdeloche on wikimedia.org](https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg).*](/figures/rnn.png){width=\"70%\"}\n\nProblem: The influence of previous inputs vanishes (or potentially explodes) with the sequence length (unfolding). This makes the network hard to train. This is mitigated by LSTMs (below).\n\nmore explanation: [youtube: StatQuest](https://www.youtube.com/watch?v=AsNTP8Kwu80)\n\n#### Long short-term memory (LSTM) networks\n\nInstead of just using the influence of previous inputs (\"short-term memory\"), LSTMs incorporate influence of more distance inputs (\"long-term memory\"). $\\rightarrow$ There are two paths to influence the current state.\n\n![A unfolded view of a unit of an LSTM. $c_{t}$ (top horizontal path) is the *cell state* and represents the long-term memory. $h_t$ ( bottom horizontal line) is the *hidden* *state* and represents the short-term memory. The weights are not shown in this diagram. $F_t$ is the *forget gate* and determines the percentage of the long-term memory that is remembered based on the short-term memory and input. $I_t$ is the *input gate* and determines how/whether to update/create the long-term memory using the input and short-term memory. $O_t$ is the *output gate* and determines the short-term memory to be passed on to the next time-step. Thus the output of the cell is the modified long- and short-term memory. *Figure from [fdeloche on wikimedia.org](https://commons.wikimedia.org/wiki/File:Long_Short-Term_Memory.svg).*](/figures/LSTM.png){width=\"70%\"}\n\nmore explanation: [youtube: StatQuest](https://www.youtube.com/watch?v=YCzL96nL7j0)\n\n### Transformer models\n\nTransformers are encoder-decoder models with the following setup:\n\n-   **Encoder:** Containts one attention unit, the *multi-head attention*. It learns the relationships between elements in the input sequence.\n\n-   **Decoder:** Contains two attention units:\n\n    -   The *masked multi-head attention*: It learns relationship of the current element and previous words in the output sequence.\n\n    -   The *multi-head attention*: It learns the relationship of the (current and previous) elements in the output sequence and the learned representation of the input from the encoder.\n\nLike LSTMs, transformers use attention mechanisms to learn relationships between input elements. There are key differences, however:\n\n-   The sequences are passed in simultaneously (not sequentially as in RNNs). Many computations can be done in parallel.\n-   *Positional encoding*: Since the elements are fed-in in parallel, the position of the elements within the sequence is encoded.\n-   Masking: Since elements are fed-in in parallel, the elements that the model shall predict, are masked (overwritten with zeros) from the output-sequence during training.\n-   Self-attention: The significance of an element (e.g. word) is learned by specific elements around it. Contrary to RNNs, the dependencies don't grow/shrink linearly with $t$, but are independent of the distance.\n\n![Architecture of transformer models. The encoder is on the left. The decoder is on the right. The encoder-outputs are passed to it in sequence. Output-sequence elements that come after the current input element are masked. The decoder generates the next element of the output. There are commonly multiple encoder and decoder units in sequence and multiple attention units in parallel. *Figure from [Yuening Jia on wikimedia.org](https://commons.wikimedia.org/wiki/File:The-Transformer-model-architecture.png).*](/figures/transfomer-architecture.png){width=\"70%\"}\n\nDetailed explanation: [Youtube - CodeEmporium](https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4)\n\n#### Large Language Models (LLMs)\n\nLLMs are transformer models, that have been trained on a huge amount of unlabeled data.\n\nPros:\n\n-   They are very performant\n-   They are easily adaptable for other use cases.\n\nCons:\n\n-   The training data of the models is often unknown or insufficiently sanitized (containing false information, hate speech, outdated info, etc. )\n-   The models are so huge, that it takes a lot of compute power just to use it for predictions.\n\nTo deal with these problems, You can tune late layers in pre-trained LLMs to increase their accuracy for your field of application without having to train a huge model. \n\n<!-- integrate https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/ -->\n<!-- integrate replicate, langchain etc. -->\n\n## Learnig methods\n\nThere are specific methods for learning in neural networks.\n\n### Transfer learning {#transfer-learning}\n\nYou train a model on one (usually large) dataset and adapt it for a different, but related learning task. The feature space and distribution of the input and target data of the new task can be different from the old task.\n\n#### Instance Transfer\n\nYou take the samples from the dataset of the initial task and mix them into the dataset of the new task. \n\n#### Domain adaptation\n\nIn domain adaptation the feature space is similar to the original dataset.\n\n</font>\n\n\n### Meta-learning\n\nYou do not train a model directly on the target task, but train the model to distinguish different samples on many other tasks where you have enough training data (thus *learning to learn*). You build such a model with the aim to adapt it or directly use it for the target learning task with very few samples.\n\nDefinitions:\n\n* **Training Set:** The large set of samples used to train the model.\n* **k-way n-shot Support Set:** The very small set with n samples used to distinguish k classes. It is either used to adapt or directly query the pre-trained model. The class labels of the samples are only in the support set - not the training set.\n* **Query Set:** The set of samples used to query the model. \n\nYou commonly train or fine-tune a model using contrastive learning (chapter [contrastive learning](#contrastive-learning)). \n\n::: {.callout-note collapse=true appearance=\"default\"}  \n#### Common pre-trained models: \n\n##### NLP\n\n- SBERT (Sentence Bert) <!-- TODO: include link to huggingface -->\n- SRoBERTa <!-- TODO: include link  to huggingface -->\n\n##### Images\n<!-- TODO: include possible models --> \n\n##### Time Series\n<!-- TODO: Include possible models --> \n\n:::\n\n\n#### Few-shot learning\n\nYou have very few (1-20) samples per class in your support set. Before you use the approaches below, you should use [data augmentation](imagedata) to increase the sample size. \n\n##### Metric-based learning\n\nInstead of adapting the weights of the model, you provide the model with representative reference samples. You then compare the internal representation of the model of the reference samples and the new samples. The new samples are classified according to the most similar reference samples. \n\n**In context learning** is a special case of few-shot learning in large language models, where you provide the reference samples in the prompt to the model. This has the disadvantage, that your performance is dependent on prompt engineering and the model to be used will be very large (expensive and slow). \n\n##### Fine-tuning\n\nYou adapt the weights of the model to the new task. To successfully do this with very few samples, you need suitable pre-trained models for this. To prevent overfitting, we limit the parameters of the model we adapt and use regularization (i.e. entropy regularization).\nA common technique is to only fine tune the similarity and softmax function of the model. More details here: [youtube.com - Shusen Wang: Few-Shot Learning pt. 3]\n\n<!-- include https://huggingface.co/blog/setfit#training-your-own-model -->\n<!-- include https://neptune.ai/blog/understanding-few-shot-learning-in-computer-vision : Algorithms for Few-Shot image classification -->\n\n### Contrastive Learning {#contrastive-learning}\n\nYou train/fine-tune a model, so that similar samples are close together in feature space and dissimilar samples are far apart. This is done by using a contrastive loss function. The model is trained on pairs of samples. The loss function is minimized, when the distance between similar samples is small and the distance between dissimilar samples is large.\n\nMore info: [builtin.com](https://builtin.com/machine-learning/contrastive-learning)\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfeature_df = pd.DataFrame(data= {\"x\": [1, 1.5, 3.5, 4, 4.5, 6],\n                 \"y\": [1, 2, 9.5, 10.5, 9, 3],\n                 \"object\": ['Dog', 'Cat', 'Car', 'Train', 'Bus', 'Tree']})\n\n#create scatterplot\nplt.scatter(feature_df[\"x\"], feature_df[\"y\"])\n\n#add labels to every point\nfor idx, row in feature_df.iterrows():\n    plt.text(row[\"x\"]+0.2, row[\"y\"]-0.1, row[\"object\"], fontsize=10)\n\nplt.xlim(0, 8)\nplt.ylim(0, 12)\nplt.grid(visible=False)\n```\n\n::: {.cell-output .cell-output-display}\n![Conceptual example of a feature space  in contrastive learning. The model was trained to distinguish between \"animals\", \"vehicles\" and \"plants\".](0-Neural_files/figure-html/cell-6-output-1.png){width=288 height=194}\n:::\n:::\n\n\nCommon model architectures for contrastive learning are:\n\n* Siamese networks (use pairs of samples for training. Samples from same classes are labeled positive, samples from different classes are labeled negative)\n* Triplet networks (use triplets of samples containint an *anchor sample*, a *positive sample* (same class as anchor) and a *negative sample* (different class as anchor). The model is trained to minimize the distance between the anchor and the positive sample and maximize the distance between the anchor and the negative sample.)\n\n\n\n```{mermaid}\n%%| fig-cap: \"Conceptual example of a Siamese network. Note, that the base model is shared between the two inputs.\"\n\ngraph LR\n    A[Input 1] -- \"base model\n    `\" --> A1(\"Embedding 1\")\n\n    B[Input 2] -- \"base model\n    `\" --> B1(\"Embedding 2\")\n\n    A1 --> C(\"Difference vector\")\n\n    B1 --> C\n\n    C -- \"dense layers\n    sigmoid\" --> D[Similarity score]\n```\n\n",
    "supporting": [
      "0-Neural_files"
    ],
    "filters": [],
    "includes": {}
  }
}