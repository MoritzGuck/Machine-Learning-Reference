[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Reference",
    "section": "",
    "text": "Quick links\n\n\n\n\n\n  Statistics \n\n\n  Data cleaning\n\n\n  Classification\n\n\n  Regression\n\n\n\n\n  Clustering\n\n\n  Neural Networks\n\n\n  Check lists\n\n\n  Project Management\n\n\n\n\n\n\nQuick info\n\n\n\n\n\n\nWhat is this?\n\n\n\n\n\nThis is a reference on applied machine learning for daily use. The topics range from math fundamentals to complex machine learning models and explanation methods. The focus lies on concise explanations and practical code snippets.\n\n\n\n\n\n\n\n\n\nHow do I use it?\n\n\n\n\n\nAs an ML engineer or data scientist, you can …\n\nUse the search-field and quickly find solutions for your tasks and copy the code snippet into your project.\nRefresh you knowledge about a topic and find references for further reading.\n\nThe currently incomplete sections are marked in grey.\n\n\n\n\n\n\n\n\n\nI want to get involved\n\n\n\n\n\nThat’s awesome! r emo::ji(\"smiley\") Do you want to:\n\nWrite/adapt a chapter, section, paragraph?\nCreate figures?\nOr proofread new chapters?\n\nPlease write an email to ml_reference[ät]icloud.com\nYour benefits: Good karma, “While we teach, we learn”, Being listed as an author on this page.\n\n\n\n\n\n\n\n\n\nI want to report something\n\n\n\n\n\nYou found errors or unclear explanations? You have a good idea? Please file an issue under: github.com/MoritzGuck/Machine-Learning-Reference or write an email to ml_reference[ät]icloud.com.\n\n\n\n\n\n\n\n\n\nRelated work\n\n\n\n\n\n\nMachine Learning Glossary: An online book with more details on underlying mechanisms and a large chapter on fundamentals in maths and neural networks.\n\nR Cookbook: A comprehensive online book on R and specific problems.\n\nGoogle Machine Learning Glossary: Glossary with brief explanations on fundamental machine learning topics.\n\nDistill: Beautiful and easy to understand visualizations and articles on machine learning algorithms.\n\n\n\n\n   \n\nIcons created by Freepik - Flaticon"
  },
  {
    "objectID": "source/Intro/02-Setup-env.html#conda",
    "href": "source/Intro/02-Setup-env.html#conda",
    "title": "1  Setup Environment",
    "section": "1.1 Conda",
    "text": "1.1 Conda\nConda is a package and virtual environment manager. It resolves dependencies of your python and non-python packages.\nInstall:\nconda docs - Installation\n\n\n\n\n\n\nInstallation via UNIX CLI\n\n\n\n\n\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n\n\n\nList your existing environments:\nconda env list\nCreate new environment:\nCreate a yaml file with required packages like this one: github - ml-reference-env.yml\nCreate a conda env from that file:\nconda env create -f environment.yml\nChange to an existing environment:\nconda activate test\nconda env list\nCheck and update package versions:\nAfter activating an env:\npython --version\nconda update python\nInstall packages:\nLatest version:\nconda install pandas \nSpecific version:\nconda install pandas==2.0.0\nFrom different channel:\nconda install pandas -c conda-forge -y\nswitch back to base environment:\nconda deactivate\nRemoving environment:\nconda env remove -n env-name\nMore info: Intro to Conda virtual environments\nExport environment to conda:\nconda env export &gt; environment.yml"
  },
  {
    "objectID": "source/Maths/00-Statistics.html#probability-basics",
    "href": "source/Maths/00-Statistics.html#probability-basics",
    "title": "2  Statistics",
    "section": "2.1 Probability Basics",
    "text": "2.1 Probability Basics\n\nProbability interpretations\n\nFrequentist: Probability is the fraction of positive samples, if we measured infinitely many samples.\nObjectivist: Probabilities are due to inherent uncertainty properties. Probabilities are calculated by putting outcomes of interest into relation with all possible outcomes.\nSubjectivist: An agent’s rational degree of belief (not external). The belief needs to be coherent (i.e. if you make bets using your probabilities you should not be guaranteed to lose money) and therefore need to follow the rules of probability.\nBayesian: (Building on subjectivism) A reasonable expectation / degree of belief based on the information available to the statistician / system. It allows to give certainties to events, where we don’t have samples on (e.g. disappearance of the south pole until 2030).\n\nAlso the frequentist view is not free of subjectivity since you need to compare events on otherwise similar objects. Usually there are no completely similar objects, so you need to define them.\n\n\nProbability Space\nThe probability space is a triplet space containing a sample/outcome space \\Omega (containing all possible atomic events), a collection of events S (containing a subset of \\Omega to which we want to assign probabilities) and the mapping P between \\Omega and S.\n\n\nAxioms of Probability\nThe mapping P must fulfill the axioms of probability:\n\nP(a) \\geq 0\nP(\\Omega) = 1\na,b \\in S and a \\cap b = \\{\\} \\Rightarrow P(a \\cup b) = P(a) + P(b)\n\na, b are events.\n\n\nRandom Variable (RV)\nA RV is a function that maps points from the sample space \\Omega to some range (e.g. Real numbers or booleans). They are characterized by their distribution function. E.g. for a coin toss: X(\\omega) = \\begin{cases}\n                0, \\text{ if } \\omega = heads\\\\\n                1, \\text{ if } \\omega = tails.\n            \\end{cases}\n\n\nProposition\nA Proposition is a conclusion of a statistical inference/prediction that can be true or false (e.g. a classification of a datapoint). More formally: A disjunction of events where the logic model holds. An event can be written as a propositional logic model:\nA = true, B = false \\Rightarrow a \\land \\neg b. Propositions can be continuous, discrete or boolean."
  },
  {
    "objectID": "source/Maths/00-Statistics.html#probability-distributions",
    "href": "source/Maths/00-Statistics.html#probability-distributions",
    "title": "2  Statistics",
    "section": "2.2 Probability distributions",
    "text": "2.2 Probability distributions\nProbability distributions assign probabilities to to all possible points in \\Omega (e.g. P(Weather) = \\langle 0.3, 0.4, 0.2, 0.1 \\rangle, representing Rain, sunshine, clouds and snow). Joint probability distributions give you a probability for each atomic event of the RVs (e.g. P(weather, accident) gives you a 2\\times 4 matrix.)\n\nCumulative Distribution Function (CDF)\nThe CDF is defined as F_X(x) = P(X \\leq x) (See figure CDF).\n\n\n\nCumulative distribution function of a normal distribution for different mean (\\mu) and variance (\\sigma). Source: user Inductiveload on wikimedia.org.\n\n\n\n\nProbability Density Function (PDF)\nFor continuous functions the PDF is defined by p(x) =  {d \\over dx} p(X \\leq x). The probability of x being in a finite interval is P(a &lt; X \\leq b) = \\int_a^b p(x) dx A PDF is shown in the following figure.\n\n\n\nProbability density function of a normal distribution with variance (\\sigma). In red a range from a Box-plot is shown with quartiles (Q1, Q3) and interquartile range (IQR). For the cutoffs (borders to darker blue regions) the IQR (on top) and \\sigma are chosen. Another common cutoff is the confidence interval with light blue regions having a probability mass of 2 * \\alpha / 2. Source: user Jhguch on wikimedia.org.\n\n\n\n\nProperties of Distributions\n\nThe expected value (E) or mean (\\mu) is given by E[X] = \\sum_{x \\in X} x*p(x) for discrete RVs and E[X] = \\int_X x*p(x) dx for continuous RVs.\nThe variance measures the spread of a distribution: var[X] = \\sigma^2 = E[(X-\\mu)^2] = E[X]^2 - \\mu^2.\nThe standard deviation is given by: \\sqrt{var[X]} = \\sigma. It is interpreted as the deviation from the mean that needs to be expected.\nThe mode is the value with the highest probability (or the point in the PDF with the highest value):\nThe median is the point at which all point less than the median and all points greater than the median have the same probability (0.5).\nThe quantiles (Q) divide the datapoints into sets of equal number. The Q_1 quartile has 25% of the values below it. The interquartile range (IQR) is a measure to show the variability in the data (how distant the points from the first and last quartile are)\n\n\n\nCorrelation and covariance\n\n\n\nDirac delta function\nThe dirac delta is simply a function that is infinite at one point and 0 everywhere else: \\delta(x)=\\begin{cases} \\infty , \\; \\text{ if } x = 0 \\\\0, \\quad \\text{if } x \\neq 0 \\end{cases} \\qquad \\text{and } \\int_{-\\infty}^{\\infty} \\delta(x) dx = 1 (Needed for distributions further on)\n\n\nUniform distribution\nThe uniform distribution has the same probability throughout a specific interval: \\text{Unif}(a,b) = \\frac{1}{b-a} 1 \\mkern-6mu 1 (a &lt; x \\leq b) = \\begin{cases}\n                \\frac{1}{b-a}, \\quad \\text{if } x \\in [ a,b ] \\\\\n                0, \\qquad \\text{else}\n            \\end{cases} 1 \\mkern-6mu 1 is a vector of ones.\n\n\n\nUniform distribution. Source: user IkamusumeFan on wikimedia.org.\n\n\n\n\nDiscrete distributions\nUsed for random variables that have discrete states.\n\nBinomial distribution\nUsed for series of experiments with two outcomes (success or miss. e.g. a series of coin flips). X \\sim \\text{Bin}(n, \\theta ), \\quad \\text{Bin}(k|n,\\theta)={n \\choose k} \\theta^k (1-\\theta)^{n-k} , \\quad {n \\choose k} = \\frac{n!}{k!(n-k)!}, where n is the number of total experiments, k is the number of successful experiments and \\theta is the probability of success of an experiment.\n\n\n\nBinomial distribution of balls in Pascals triangles with different numbers of layers (The top one has 0 layers). Example: For a triangle with n=6 layers, the probability that a ball lands in the middle box k=3 is 20/64. Source: user Watchduck on wikimedia.org\n\n\n\n\nBernoulli distribution\nIs a special case of the binomial distribution with n=1 (e.g. one coin toss). X \\sim \\text{Ber}(\\theta ), \\quad \\text{Ber}(x | \\theta)=\\theta^{1 \\mkern-6mu 1 (x=1)} (1-\\theta)^{1 \\mkern-6mu 1(x=0)}= \\begin{cases}\n                    \\theta, \\qquad \\text{if } x=1 \\\\\n                    1 - \\theta, \\; \\text{if } x=0\n                \\end{cases}\n\n\nMultinomial distribution\nUsed for experiments with k different outcomes (e.g. dice rolls: Probability of different counts of the different sides). \\text{Mu}(x|n,\\theta) =  {n \\choose x_1, ..., x_K}\\prod_{i=1}^K\\theta_j^{x_j} = \\frac{n!}{x_1!, ..., x_k!}\\prod_{i=1}^K\\theta_j^{x_j}, where k is the number of outcomes, x_j is the number times that outcome j happens. X = (X_1, ..., X_K) is the random vector.\n\n\nMultinoulli distribution\nIs a special case of the multinomial distribution with n=1. The random vector is then represented in dummy- or one-hot-encoding (e.g. (0,0,1,0,0,0) if outcome 3 takes place). \\text{Mu}(x|1,\\theta) = \\prod_{j=0}^K \\theta_j^{1 \\mkern-6mu 1(x_j=1)}\n\n\nEmpirical distribution\nThe empirical distribution follows the empirical measurements strictly. The CDF jumps by 1/n every time a sample is “encountered” (see figure).\n\\text{p}_{\\text{emp}}(A) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{x_i}(A), \\quad \\delta_{x_i}=\\begin{cases}1, \\quad \\text{if } x \\in A \\\\0, \\quad \\text{if } x \\notin A \\end{cases}, w where x_1, ..., x_N is a data set with N points. The points can also be weighted: p(x) =  \\sum_{i=1}^N w_i \\delta_{x_i}(x)\n\n\n\nCumulative empirical distribution function (blue line) for samples drawn from a standard normal distribution (green line). The values of the drawn samples is shown as grey lines at the bottom. Source: user nagualdesign on wikimedia.org.\n\n\n\n\n\nContinuous distributions\nUsed for random variables that have continuous states.\n\nNormal/Gaussian distribution\nOften chosen for random noise because it is simple and needs few assumptions (see sect. CLT). The PDF is given by:\np(x|\\mu\\sigma^2)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right], where \\mu is the mean and \\sigma^2 is the variance. The CDF is given by: \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{\\infty}^xe^{\\frac{-t^2}{2}dt}\n\n\nMultivariate normal/Gaussian distribution\nFor T datapoints with k dimensions (features). The pdf is: p(x|\\mu,\\Sigma) = \\dfrac{1}{\\sqrt{(2\\pi)^k|\\Sigma|}}\\exp\\left[-\\dfrac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)\\right], where x now has multiple dimension (x_1, x_2, ..., x_k) and \\Sigma is the k \\times k covariance matrix: \\Sigma = \\text{E}[(X-\\mu)(X-\\mu)]. The covariance between features is: \\text{Cov}[X_i, X_j] = \\text{E}[(X_i-\\mu_i)(X_j-\\mu_j)]\n\n\nBeta distribution\ndefined for 0 \\leq x \\leq 1 (see figure Beta distribution). The pdf is: f(x|\\alpha, \\beta) = \\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} The beta function B is there to normalize and ensure that the total probability is 1 .\n\n\n\nProbability density function of a beta-distribution with different parameter values. Source: user MarkSweep on wikimedia.org.\n\n\n\n\nDirichlet distribution\nThe multivariate version of the Beta distribution. The PDF is: \\text{Dir}({x}|{\\alpha}) \\triangleq \\dfrac{1}{B({\\alpha})}\\prod\\limits_{i=1}^K x_i^{\\alpha_i-1},\\quad \\sum_{i=1}^K x_i =1, \\quad x_i \\geq 0 \\text{ }\\forall i\n\n\n\nProbability density function of a Dirichlet-distribution on a 2-simplex (triangle) with different parameter values. Clockwise from top left: \\alpha = (6,2,2), (3,7,5), (6,2,6), (2,3,4). Source: user ThG on wikimedia.org.\n\n\n\n\nMarginal distributions\nAre the probability distributions of subsets of the original distribution. Marginal distributions of normal distributions are also normal distributions.\n\n\n\nData following a 2D-Gaussian distribution. Marginal distributions are shown on the sides in blue and orange. Source: user Auguel on wikimedia.org."
  },
  {
    "objectID": "source/Maths/00-Statistics.html#CLT",
    "href": "source/Maths/00-Statistics.html#CLT",
    "title": "2  Statistics",
    "section": "2.3 Central limit theorem",
    "text": "2.3 Central limit theorem\nIn many cases the sum of random variables will follow a normal distribution as n goes to infinity."
  },
  {
    "objectID": "source/Maths/00-Statistics.html#bayesian-probability",
    "href": "source/Maths/00-Statistics.html#bayesian-probability",
    "title": "2  Statistics",
    "section": "2.4 Bayesian probability",
    "text": "2.4 Bayesian probability\nBaeysian probability represents the plausibility of a proposition based on the available information (i.e. the degree at which the information supports the proposition). The use of this form of statistics is especially useful if random variables cannot be assumed to be i.i.d. (i.e. When an event is not independent of the event before it (e.g. drawing balls without laying them back into the urn)).\n\nConditional/Posterior Probability\nExpresses the probability of one event (Y) under the condition that another event (E) has occurred. (e.g. C = “gets cancer”, S = “is a smoker” \\rightarrow p(C|S)=0.2, meaning: “given the sole information that someone is a smoker, their probability of getting cancer is 20%.”)\n\nThe conditional probability can be calculated like follows. By defining the joined probability like so: P(A \\cap B) = P(A \\mid B) P(B) you solve for P(A \\mid B): P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}=\\frac{P(A, B)}{P(B)}=\\alpha{P(A, B)}, where \\alpha is used as a normalization constant. If you have hidden variables (confounding factors) you need to sum them out like so: P(Y|E=e)=\\alpha P(Y,E=e)=\\alpha\\sum_h P(Y,E=e,H=h) where X contains all variables, Y is called query variable, E is called evidence variable, H=X-Y-E is called hidden variable or confounding factor. You get the joint probabilities by summing out the hidden variable. \n! Usually p(A|B) \\neq p(B|A)\n! Priors are often forgotten: E.g. P(\\text{\"COVID-19\"}) is confused with P(\\text{\"COVID-19\"}|\\text{\"Person is getting tested\"}) (because only people with symptoms go to the testing station).\n! Base rate neglect: Under-representing the prior probability. E.g. You have a test with a 5% false positive rate and a incidence of disease of 2% in the population. If you are tested positive in a population screening your probability of having the disease is only 29%.\nConditional distributions of Gaussian distributions are Gaussian distributions themselves.\n\n\nIndependence\nFor independent variables it holds: P(A|B)=P(A) or P(B|A)=P(B)\n\n\nConditional independence\nTwo events A and B are independent, given C: P(A|B,C)=P(A|C). A and B must not have any information on each other, given the information on C. E.g. for school children: P(\\text{\"vocabulary\"}|\\text{\"height\"}, \\text{\"age\"})= P(\\text{\"vocabulary\"}|\\text{\"age\"}).\n\n\nBayes Rule\nBayes rule is a structured approach to update prior beliefs / probabilities with new information (data). With the conditional probability from before (P(A,B)=P(A|B)P(B)=P(B|A)P(A)) we get Bayes rule by transforming the right-side equation to:P(\\text{hypothesis}|\\text{evidence}) =\\dfrac{P(\\text{evidence}|\\text{hypothesis})P(\\text{hypothesis})}{P(\\text{evidence})} often used as: P(\\text{model}|\\text{data}) =\\dfrac{P(\\text{data}|\\text{model})P(\\text{model})}{P(\\text{data})}\n\nTerminology:\n\nP(\\text{hypothesis}|\\text{evidence}) = Posterior (How probable hypothesis is after incorporating new evidence)\nP(\\text{evidence}|\\text{hypothesis}) = Likelihood (How probable the evidence is, if the hypothesis is true)\nP(\\text{hypothesis}) = Prior (How probable hypothesis was before seeing evidence)\nP(\\text{evidence}) = Marginal (How probable evidence is under all possible hypotheses)\n\\dfrac{P(\\text{evidence}|\\text{hypothesis})}{P(\\text{evidence})} = Support B provides for A\nP(\\text{data}|\\text{model})P(\\text{model}) = joint probability (P(A,B))\n\n\n\nExample for Bayes Rule using COVID-19 Diagnostics\nP(\\text{COVID-19}|\\text{cough}) =\\dfrac{P(\\text{cough}|\\text{COVID-19})P(\\text{COVID-19})}{P(\\text{cough})} = \\frac{0.7*0.01}{0.1}=0.07 Estimating P(\\text{COVID-19}|\\text{cough}) is difficult, because there can be an outbreak and the number changes. However, P(\\text{cough}|\\text{COVID-19}) stays stable, P(\\text{COVID-19}) and P(\\text{cough}) can be easily determined."
  },
  {
    "objectID": "source/Maths/00-Statistics.html#further-concepts",
    "href": "source/Maths/00-Statistics.html#further-concepts",
    "title": "2  Statistics",
    "section": "2.5 Further Concepts",
    "text": "2.5 Further Concepts\n\nConvergence in Probability of Random Variables\nYou expect your random variables (X_i) to converge to an expected random variable X. I.e. after looking at infinite samples, the probability that your random variable X_n differs more than a threshold \\epsilon from your target X should be zero. \\lim_{n \\rightarrow \\infty} P(|X_n - X| &gt; \\epsilon) = 0\n\n\nBernoulli’s Theorem / Weak Law of Large Numbers\n\\lim_{n \\rightarrow \\infty} P(|\\frac{\\sum_{i=1}^n X_i}{n} - \\mu| &gt; \\epsilon) = 0, where X_1,...,X_n are independent & identically distributed (i.i.d.) RVs. \\Rightarrow With enough samples, the sample mean will approach the true mean. The strong law of large numbers states that |\\frac{\\sum_{i=1}^n X_i}{n} - \\mu| &lt; \\epsilon for any \\epsilon &gt; 0."
  },
  {
    "objectID": "source/Maths/00-Statistics.html#statistical-hypothesis-tests",
    "href": "source/Maths/00-Statistics.html#statistical-hypothesis-tests",
    "title": "2  Statistics",
    "section": "2.6 Statistical hypothesis tests",
    "text": "2.6 Statistical hypothesis tests\n\nTerminology\n\nNull-hypothesis (over-simplified): “The differences between these two sets of samples are due to chance.” Or “There is no (positive/negative) effect of the independent variables on the dependent variable.”\nAlternative hypothesis (over-simplified): “The differences between the two sets of samples cannot be due to chance.” Or “There is a (positive/negative) effect …” (More examples)\nP-value: The probability to observe an at least as extreme value (e.g. sample mean of a distribution) as the measured one, if the null-hypothesis was true.\n\\alpha-Value / Significance level: Probability of falsely rejecting the null-hypothesis, given the null-hypothesis is true. \nStatistically significant: The result is significant if p &lt; \\alpha.\n\n\n\nt-test\nTests for significant difference between two independent sets of samples.\n \\text{t} = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{\\hat{\\sigma}^2_1}{n_1}+\\frac{\\hat{\\sigma}^2_2}{n_2}}} where \\bar{X} is the sample mean, \\hat{\\sigma}^2 is the sample variance, n is the sample size.\nfrom scipy import stats\nstats.ttest_ind(rvs1, rvs2, equal_var=True, alternative = \"two-sided\")\nMore info: scipy.org\nUsing the one-ample t-test, you can test if the sample mean (\\bar{X}) is significantly different to an expected mean (\\hat{\\mu}_0).\nfrom scipy import stats\nstats.ttest_1samp(rvs, popmean=0.0)\nMore info: scipy.org\nThe t-test for paired samples is used, when the samples have been matched (e.g. subjects are measured before and after treatment).\nfrom scipy import stats\nstats.ttest_rel(rvs1, rvs2, alternative = \"two-sided\")\n\n\nz-test\nSame as t-test, but requires that the standard distribution of the population is known. Therefore it is not used often (except for large sample sizes).\n z = \\frac{\\bar{X} - \\mu_0}{\\sigma} \n\n\nPearson Chi-squared (\\chi^2) test\nTests for significant relationships between categorical features. It is used with large sample sizes.\n \\chi^2 = \\sum^n_{i=1} \\frac{(O_i - E_i)^2}{E_i}   where \\chi^2 is the test-statistic, O_i is the number of observations of feature i, E_i (=Np_i) is the expected count of feature i.\nTo calculate it, you create contingency tables or freqency tables. They contain the frequency of the counts of each combination of variables.\n\nPythonR\n\n\nfrom scipy.stats import chi2_contingency\ncontingency_table = np.array([[10, 10, 20], [20, 20, 20]]) \ntest_stat, p_val, degOfFreedome, expected_freq = chi2_contingency(contingency_table)\n\n\ntable(mtcars$vs, mtcars$gear)\nchisq.test(contingency_table)\n\n\n\nMore info: scipy.org\nWhen the sample sizes are smaller, Fisher’s exact test is used. It uses the assumption of fixed marginal distributions.\n\nPython SciPyR\n\n\nfrom scipy.stats import fisher_exact\nres = fisher_exact(table, alternative='two-sided')\nres.pvalue\nMore info: scipy.org\n\n\ncontingency_table = table(mtcars$vs, mtcars$gear)\nfisher.test(contingency_table)"
  },
  {
    "objectID": "source/Maths/01-Linear-Algebra.html#vectors",
    "href": "source/Maths/01-Linear-Algebra.html#vectors",
    "title": "3  Linear Algebra",
    "section": "3.1 Vectors",
    "text": "3.1 Vectors\nThere are two relevant perspectives for us:\n\nMathematical: Generally quantities that cannot be expressed by single number. They are objects in a vector space. Such objects can also be e.g. functions.\nProgrammatical / Data: Vectors are ordered lists of numbers. You model each sample as such an ordered list of numbers and the numbers represent the feature-value of that feature.\n\nYour vectors are organized in a coordinate system and commonly rooted in the origin (point [0,0].\n\n\nLinear combinations\nYou create linear combinations of vectors by adding their components (entries in a coordinate). Th All points that you can reach by linear combinations are called the span of these vectors. If a vector lies in the span of another vector, they are linearly dependent.\nYou can scale (stretch or squish) vectors multiply vectors by scalars (i.e. numbers). A vector with length 1 is called unit vector. The unit vectors in each direction of the coordinate system are its basis vectors. The basis vectors stacked together form an identity matrix: a matrix with 1s on its diagonal. Since there are only values on its diagonal it is also a diagonal matrix.\n\n\nI = \\begin{bmatrix}\n1 \\quad 0 \\quad 0 \\\\\n0 \\quad 1 \\quad 0 \\\\\n0 \\quad 0 \\quad 1\n\\end{bmatrix}\n\n\n\nLinear transformations\nLinear transformations are functions that move points around in a vector space, while preserving the linear relationships between the points (straight lines stay straight, the origin stays the origin). They include rotations and reflections. You can understand the calculation of the linear transformation of a point as follows: You give the basis vectors a new location. You scale the new location basis vectors with the components of the respective dimension of the vector you want to transform. You take the linear combination of the scaled, transformed basis vectors:\n\\begin{bmatrix} a \\quad b \\\\ c \\quad d \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n  = x \\begin{bmatrix} a \\\\ c \\end{bmatrix} + y \\begin{bmatrix} b \\\\ d \\end{bmatrix} = \\begin{bmatrix} x a  + y b \\\\ x c + y d \\end{bmatrix}\n\n\nlikewise, you can view matrix vector multiplication as a transformation of your space. Full explanation: youtube.com - 3Blue1Brown Multiplying two matrices represents the sequential combination of two linear transformations in your vector space.\nA transpose A^T of a matrix A is achieved by mirroring the matrix on its diagonal and therefore swapping its rows and columns. This commonly makes sense when evaluating if elements of two matrices line up in regard to their scale. You can also check if matrices are orthogonal.\nAn orthogonal/orthonormal matrix is a matrix for which holds A^TA=AA^T=I, where I is the identity matrix. The columns of orthogonal matrices are linearly independent of each other.\nAn inverse matrix A^{-1} of a matrix A is the matrix that would yield no transformation at all, if multiplied with A.\nThe dot product of two vectors is calculated like a linear transformation between a 1 \\times 2 matrix and a 2 \\times 1 matrix. It therefor maps onto the 1-D Space and can be used as a measure of collinearity.\nThe cross product of two vectors is a perpendicular vector that describes the parallelogram that the two vectors span. Its magnitude can be seen as the area of the parallelogram. Beware: The order of the vectors in the operation matters. The cross product can be expressed by a determinant. If two vectors are collinear or perpendicular, the cross product is zero.\n\n\nDeterminants, rank and column space\nDeterminants can be used to measure how much a linear transformation compresses or stretches the space. If a transformation inverts the space, the determinant will be negative. If a determinant is 0 it means that the transformation maps the space onto a lower dimension.\n\\det \\left( \\begin{bmatrix} a \\quad b \\\\ c \\quad d \\end{bmatrix} \\right) = (a*d)-(c*b)  \nThe dimensions that come out of a transformation/matrix are its rank. All possible outputs of your matrix (the span constructed by its columns) is the column space. All vectors that are mapped to 0 (onto the origin) are the null space or kernel of the matrix.\nDeterminants can only be calculated for square matrices. An e.g. 3 \\times 2 matrix can be viewed as a transformation mapping from 2-D to 3-D space.\n\n\n\nSystem of equations\nLinear algebra can help you solve systems of equations.\n\n\\begin{array}{ll} 1x+2y+3z=4 \\\\ 4x+5y+6z=-7 \\\\8x + 9y +0z = 1 \\end{array}\n\\quad  \\rightarrow  \\quad\n\\begin{bmatrix} 1 \\quad 2 \\quad 3 \\\\ 4 \\quad 5 \\quad 6 \\\\ 8 \\quad \\ 9 \\quad 0 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ z\\end{bmatrix} =\n\\begin{bmatrix} 4 \\\\ -7 \\\\ 1 \\end{bmatrix}\n\\quad \\rightarrow \\quad\nA \\vec{x} = \\vec{v}\n\nYou can imagine this as as searching a vector \\vec{x} that will land on \\vec{v} after the transformation A.\nTo find \\vec{x} you need the inverse of A:\n\nA^{-1}A = \\begin{bmatrix} 1 \\quad 0 \\\\ 0 \\quad 1 \\end{bmatrix}\n\nYou now multiply the matrix equation with A^{-1} and get:\n\nA^{-1} A \\vec{x} = A^{-1} \\vec{v}\n\\quad \\rightarrow \\quad\n\\vec{x} = A^{-1} \\vec{v}\n\n\n\nEigenvalues and Eigenvectors\nFor a linear transformation A, the eigenvectors \\vec{v} represent the vectors that stay on their span (keep orientation) and the eigenvalues \\lambda are the scalars by which the eigenvectors get scaled.\n\nA \\vec{v} = \\lambda \\vec{v}\n\nTransforming \\lambda to a scaled identity matrix I and factoring out \\vec{v}, we get: \n(A - \\lambda I) \\vec{v} =  \\vec{0}\n This tells us, that the transformation (A - \\lambda I) needs to map the vector \\vec{v} onto a lower dimension.\nAn eigenbasis \\lambda I is a basis where the basis vectors are eigenvectors. They will sit on the diagonal of your basis matrix (\\rightarrow it will be a diagonal matrix).\n\n\nEigenvalue decomposition\nAn eigen(value)decomposition is the decomposition of a matrix into the matrix of eigenvalues and eigenvectors.\n\nAU = U \\Lambda  \\quad \\rightarrow \\quad A = U \\Lambda U^{-1}\n\nwhere U is the matrix of the eigenvectors of A and \\Lambda is the eigenbasis. Thus matrix operations can be computed more easily, since \\Lambda is a diagonal matrix.\n\n\nSingular value decomposition\nSingular Value decomposition is also applicable to a non-square m \\times n-matrix (with m rows and n columns). If you have a matrix with rank r, you can decompose it into\n\nA = U \\Sigma V^T\n where U is an orthogonal m \\times r matrix, \\Sigma is a diagonal r \\times r matrix and V^T is an orthogonal r \\times n matrix. U contains the left singular vectors, V the right singular vectors and \\Sigma the Singular Values.\nThis decomposition technique can be used to approximate the original matrix A with only the k largest singular values. This lets you work in a space with only k dimensions given by U_k \\Sigma_k. Thereby you can save computation time and memory space without loosing a lot of information.\n\n\n\nSVD and truncated SVD. The upper schema shows the decomposition of the matrix M with m rows and n columns into U, \\Sigma and V. The lower schema shows the truncated SVD for lower dimensional mapping U_t \\Sigma_t : The first t eigenvalues from \\Sigma have been chosen to reconstruct a compressed version \\bar{M}. This figure has been adapted from user Cmglee on wikimedia.org.\n\n\nFor more detailed explanation, see this Stack Exchange Thread.\nFor applications, please see SVD for lower dimensional mapping."
  },
  {
    "objectID": "source/Data/01-Similarity.html#metrics",
    "href": "source/Data/01-Similarity.html#metrics",
    "title": "4  Similarity and Distance Measures",
    "section": "4.1 Metrics",
    "text": "4.1 Metrics\nFor a distance measure to be called a metric d, the following criteria need to be fulfilled:\n\nPositivity: d(x_1,x_2)≥0\nd(x_1,x_2)=0 \\text{ if and only if } x_1 = x_2\nSymmetry: d(x_1, x_2) = d(x_2, x_1)\nTriangle inequality: d(x_1, x_3) ≤ d(x_1, x_2) + d(x_2, x_3)\n\nThere may be distance measures that do not fulfill these criteria, but those are not metrics."
  },
  {
    "objectID": "source/Data/01-Similarity.html#similarity-measures-on-vectors",
    "href": "source/Data/01-Similarity.html#similarity-measures-on-vectors",
    "title": "4  Similarity and Distance Measures",
    "section": "4.2 Similarity measures on vectors",
    "text": "4.2 Similarity measures on vectors\nThese measures are used in many objective functions to compare data points.\nfrom sklearn.metrics import pairwise_distances\nX1 = np.array([[2,3]])\nX2 = np.array([[2,4]])\npairwise_distances(X1,X2, metric=\"manhattan\")\nThe available metrics in sklearn are: ‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’, and from scipy: ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’\nMore info: scikit-learn.org\n\nManhattan distance\nThe distance is the sum of the absolute differences of the components (single coordinates) of the two points: d(A, B) = \\sum_{i=1}^d | A_i - B_i |\nMore info at wikipedia.org.\n\n\nHamming distance\nThis metric is used for pairs of strings and works equivalently to the Manhattan distance. It is the number of positions that are different between the strings.\nMore info at wikipedia.org.\n\n\nEuclidian distance\nd(A, B) = | A - B | = \\sqrt{\\sum_{i=1}^d (A_i-B_i)^2} \nMore info on the euclidian distance on wikipedia.org.\nThe usefulness of this metric can deteriorate in high dimensional spaces. See curse of dimensionality\n\n\nChebyshev distance\nThe Chebyshev distance is the largest difference along any of the components of the two vectors.\nd(A, B) = \\max_i(|A_i-B_i|) \nMore info at wikipedia.org.\n\n\nMinkowski Distance\nd(A, B) = (\\sum_{i=1}^d |A_i-B_i|^p)^\\frac{1}{p} \nFor p=2 the Minkowski distance is equal to the Euclidian distance, for p=1 it corresponds to the Manhattan distance and it converges to the Chebyshev distance for p \\to \\infty.   More info at wikipedia.org."
  },
  {
    "objectID": "source/Data/01-Similarity.html#kernels",
    "href": "source/Data/01-Similarity.html#kernels",
    "title": "4  Similarity and Distance Measures",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\nKernels are functions that output the relationship between points in your data. They correspond to mapping the data into high-dimensional space and allow to implicitly draw nonlinear decision boundaries with linear models. The kernel trick denotes, that you don’t have to map the points into high dimensional space explicitly.\n\nClosure properties of kernels\n\nIf k_1 and k_2 are kernels, then k_1 + k_2 is a kernel as well.\nIf k_1 and k_2 are kernels, then their product is a kernel as well.\nIf k is a kernel and \\alpha is a kernel, then \\alpha k is a kernel as well.\nIf you define k only on a set D, then points that are not in D will have a value of k_0=0 which is still a valid kernel.\n\n\n\n\nPolynomial kernel\nK(x_1, x_2) = \\left&lt; x_1, x_2 + c \\right&gt;^d   \\left&lt; \\right&gt; is the dot-product. The linear kernel is a polynomial kernel with d = 1.\n\n\nGaussian Radial Basis Function (RBF) kernel\nThis is the most widely used non-linear kernel.\nK(x_1, x_2) = exp \\left(- \\frac{ || x_1 - x_2||^2}{2 \\sigma^2}  \\right)\n\n\nconstant kernel\n\n\ndelta dirac kernel\n\n\nR convolution kernels\n\n\nString kernels"
  },
  {
    "objectID": "source/Data/02-Data-Analysis.html",
    "href": "source/Data/02-Data-Analysis.html",
    "title": "5  Data Analysis",
    "section": "",
    "text": "6 Exploratory data analysis (EDA)"
  },
  {
    "objectID": "source/Data/02-Data-Analysis.html#initial-analysis",
    "href": "source/Data/02-Data-Analysis.html#initial-analysis",
    "title": "5  Data Analysis",
    "section": "6.1 Initial analysis",
    "text": "6.1 Initial analysis\nAfter getting hold of the data, these are important properties to extract:\n\nPython PandasR\n\n\nimport pandas as pd\npd.options.display.float_format = '{:,.2f}'.format\nprint(\"First 5 samples:\")\nprint(df.head())\nprint(\":.. and last 5 samples:\")\nprint(df.tail())\nprint(\"First sample per month:\")\nprint(df.groupby(\"Month\").first())\n# The number of non-null values and the respective data type per column:\ndf.info() \n# The count, uniques, mean, standard deviation, min, max, quartiles per column:\ndf.describe(include='all') \nprint(\"rows: \"+ str(df.shape[0]))\nprint(\"columns: \"+ str(df.shape[1]))\nprint(\"empty rows: \"+ str(df.isnull().sum()))\n\n# Rarely used:\ndf[\"col1\"].unique() # returns unique values in a column\n\n\n\nSpecific summary statistic\n\nsapply(mtcars, mean, na.rm=TRUE) # statistics: mean, sd, var, min, max, median, range, and quantile\n\nSummary (Min, Max, Quartiles, Mean):\n\nsummary(mtcars)\n\n\n\n\n\nGo through this check-list after data import."
  },
  {
    "objectID": "source/Data/02-Data-Analysis.html#after-preprocessing",
    "href": "source/Data/02-Data-Analysis.html#after-preprocessing",
    "title": "5  Data Analysis",
    "section": "6.2 After preprocessing",
    "text": "6.2 After preprocessing\n\nUnivariate Analysis\nAnalyse only one attribute.\n\nCategorical / discrete data: Bar chart\nPlot the number of occurrences of each category / number. This helps you find the distribution of your data.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(df[\"sex\"])\nplt.ylabel(\"number of participants\")\n\n\n\nContinuous data\nA histogram groups data into ranges and plot number of occurrences in each range. This helps you find the distribution of your data.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')\nsns.histplot(data=df_USAhousing, x='median_house_value', bins=30)\nplt.xlabel('median value')\n\nMore info: seaborn.pydata.org\nA empirical cumulative distribution function shows the proportion of samples with values below a certain value.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.ecdfplot(data=train_df[\"feature\"].sample(10000))\nplt.xlabel('Sales per Customer')\n\nMore info: seaborn.pydata.org\n\n\n\nMultivariate Analysis\n\nContinuous vs Continuous\nScatter-plots plot the values of the datapoints of one attribute on the x-axis and the other attribute on the y-axis. This helps you find the correlations, order of the relationship, outliers etc.\nUse a pairplot to make a scatter plot of multiple features against each other.\nimport seaborn as sns\nsns.pairplot(df_USAhousing[[\"median_income\", \"median_house_value\", \"total_rooms\"]], diag_kind=\"hist\")\n\nAlternatively use joint plots, to visualize the marginal (univariate) distributions on the sides:\nsns.jointplot(data=df_USAhousing, x=\"median_income\", y=\"median_house_value\")\n\nHeatmaps plot the magnitude of values in different categories. It is commonly used in exploratory data analysis to show the correlation of the different attributes.\nimport seaborn as sns\nsns.heatmap(df.corr(), cmap=\"coolwarm\", vmin=-1, vmax=1, annot=True)\n\nMore info: seaborn.pydata.org\n\n\nContinuous vs. Categorical data\nOverlapping histograms plot the marginal distribution of the continuous distributions, using different colors for each category:\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.histplot(data=df_USAhousing, x='median_house_value', hue=\"ocean_proximity\", element=\"poly\",  bins=30)\nplt.xlabel('median value')\n\nUse separate violin plots for each of the different categories:\nimport seaborn as sns\nsns.catplot(data=df, x=\"cont_col\", y=\"cat_col\", hue=\"binary_col\", kind=\"violin\")\nUse heatmaps with two categorical feature as x- and y-axis respectively and a continuous attribute as magnitude (“heat”).\nimport seaborn as sns\nsns.heatmap(df.pivot(index=\"cat_col1\", columns=\"cat_col2\", values=\"cont_col\"), annot=True, linewidth=0.5)\n\n\nCategorical vs Categorical\nCategorical plots plot the count / percentage of different categorical attributes in side-by-side bar charts\nimport seaborn as sns\nsns.catplot(data=df, y=\"cat_col1\", hue=\"cat_col2\", kind=\"bar\")\nMore info: seaborn.pydata.org"
  },
  {
    "objectID": "source/Data/02-Data-Analysis.html#performance",
    "href": "source/Data/02-Data-Analysis.html#performance",
    "title": "5  Data Analysis",
    "section": "7.1 Performance",
    "text": "7.1 Performance\nSee chapters:\n\nEvaluation of classification models\nEvaluation of regression models\nEvaluation of clustering algorithms"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html",
    "href": "source/Data/03-Preprocessing.html",
    "title": "6  Preprocessing data",
    "section": "",
    "text": "7 Standardization\nMany machine learning models assume that the features are centered around 0 and that all have a similar variance. Therefore the data has to be centered and scaled to unit variance before training and prediction.\nMore info: scikit-learn.org\nAnother option for scaling is normalization. This is used, when the values have to fall strictly between a max and min value.\nMore info: scikit-learn.org\nYou need to split your training set into test- and training-samples. The algorithm uses the training samples with the known label/target value for fitting the parameters. The test-set is used to determine if the trained algorithm performs well on new samples as well. You need to give special considerations to the following points:\nUsually the label does not depend on all available features. To detect causal features, remove noisy ones and reduce the running and training costs of the algorithm, we reduce the amount of features to the relevant ones. This can be done a priori (before training) or using wrapper methods (integrated with the prediction algorithm to be used).\n! There are methods that have feature selection already built-in, such as decision trees."
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#joining-merging-separate-tables",
    "href": "source/Data/03-Preprocessing.html#joining-merging-separate-tables",
    "title": "6  Preprocessing data",
    "section": "6.1 Joining / merging separate tables",
    "text": "6.1 Joining / merging separate tables\nimport pandas as pd\nmerged_df = pd.merge(df1, df2, how = \"inner\", on = \"reference_column\")\nMore info: pandas.pydata.org"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#missing-wrong-data",
    "href": "source/Data/03-Preprocessing.html#missing-wrong-data",
    "title": "6  Preprocessing data",
    "section": "6.2 Missing & wrong data",
    "text": "6.2 Missing & wrong data\nSome algorithms assume that all features of all samples have numerical values. In these cases missing values have to be imputed (i.e. inferred) or (if affordable) the samples with missing feature values can be deleted from the data set.\n\nIterative imputor by sklearn\nFor features with missing values, this imputor imputes the missing values by modelling each feature using the existing values from the other features. It uses several iterations until the results converge.\n! This method scales with O(nd^3), where n is the number of samples and d is the number of features.\nfrom sklearn.experimental import enable_iterative_imputer # necessary since the imputor is still experimental\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor \nrf_estimator = RamdomForestRegressor(n_estimators = 8, max_depth = 6, bootstrap = True)\nimputor = IterativeImputer(random_state=0, estimator = rf_estimator, max_iter = 25)\nimputor.fit_transform(X)\nMore info: scikit-learn.org\n\n\n\nMedian / average imputation\nSimply replace missing values with the median or average of the feature:\n\nPandasR\n\n\nimport pandas as pd\ndf[\"feature\"] = df[\"feature\"].fillna(df[\"feature\"].median())\n\n\ndataset[, co_i] = ifelse(is.na(dataset[, co_i]), \n                       ave(dataset[, co_i], FUN = function(x) mean(x, na.rm = TRUE)),\n                       dataset[, co_i])\n\n\n\n\n\nDeleting missing values\nimport pandas as pd\ndf.dropna(how=\"any\") # how=\"all\" would delete a sample if all values were missing\nMore info: pandas.pydata.org\n\n\n\nDeleting duplicate entries\nDuplicate entries need to be removed (exception: time series), to avoid over representation and leakage into test set.\nimport pandas as pd\ndf.drop_duplicates(keep=False)\n\n\nReplacing data\nimport pandas as pd\ndf.Col.apply(lambda x: 0 if x=='zero' else 1)\n\n\nFilter out data\nimport pandas as pd\ndf = df[(df[\"Feature1\"] == 0) & (df[\"Feature2\"] != 0)]"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#continuous-data",
    "href": "source/Data/03-Preprocessing.html#continuous-data",
    "title": "6  Preprocessing data",
    "section": "6.3 Continuous data",
    "text": "6.3 Continuous data\n\nPolynomial transform\nYou spread out small and large values of a feature to help the algorithm to distinguish cases. It can also be used to combine two features to represent mutually supporting effects.\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\npoly.fit_transform(df[[\"feature1\", \"feature2\"]])\n\n\nReduce skew\nHeavy skew in a distribution can be a problem for many models (outlier effects). To reduce it you can use a power transform to map the data to a Gaussian distribution…\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\npt.fit_transform(df[\"skew_feature\"])\nMore info: scikit-learn.org\n… or a quantile transform to map the data to a uniform (or Gaussian) distribution\nfrom sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles=100, output_distribution=\"uniform\") # alternvative distribution: \"normal\"\nqt.fit_transform(df[\"skew_feature\"])\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#categorical-data",
    "href": "source/Data/03-Preprocessing.html#categorical-data",
    "title": "6  Preprocessing data",
    "section": "6.4 Categorical data",
    "text": "6.4 Categorical data\nThere are multiple ways to encode categorical data, especially non-vectorized data, to make it suitable for machine learning algorithms. The string values (e.g. “male”, “female”) of categorical features have to be converted into integers. This can be done by two methods:\n\nOrdinal encoding\nAn integer is assigned to each category (e.g. “male”=0, “female”=1)\n\n\nSklearn\nfrom sklearn.preprocessing import OrdinalEncoder\nord_enc = OrdinalEncoder(min_frequency=0.05)\nord_enc.fit(X) # multiple columns can be transformed at once\nX_transf = ord_enc.transform(X)\nMore info: scikit-learn.org\n\n\n\nR\ndataset$col = factor(dataset$col, \n                      labels=c(1,2,3))\n\n\nThis method is useful when the categories have an ordered relationship (e.g. “bad”, “medium”, “good”). If this is not the case (e.g. “dog”, “cat”, “bunny”) this is to be avoided since the algorithm might deduct an ordered relationship where there is none. For these cases one-hot-encoding is to be used.\nFor encoding the label for classification tasks, you can also use the scikit-learn’s LabelEncoder. More info here: scikit-learn.org\n\n\nOne-hot encoding\nOne-hot encoding assigns a separate feature-column for each category and encodes it binarily (e.g. if the sample is a dog, it has 1 in the dog-column and 0 in the cat and bunny column).\n\nsklearnPandas\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nonehot_enc = OneHotEncoder(handle_unknown='ignore')\nonehot_enc.fit(X)\nonehot_enc.transform(X)\nMore info: scikit-learn.org\n\n\n\nimport pandas as pd\npd.get_dummies(X, columns = [\"Sex\", \"Type\"], drop_first=True)\nMore info: pandas.pydata.org\n\n\n\n\n\nDiscretizing / binning data\nYou can discretize features and targets from continuous to discrete/categorical (e.g. age in years to child, teenager, adult, elderly).\npd.cut(df[\"Age\"], bins=[0,12, 20, 65, 150], labels =[\"child\", \"teenager\", \"adult\", \"elderly\"])\nMore info: pandas.pydata.org\n\nPros:\n\nIt makes sense for the specific problem (e.g. targeting groups for marketing).\nImproved signal-to-noise ratio (bins work like regularization).\npossibly highly non-linear relationship of continuous feature to target is hard to learn for model.\nBetter interpretability of features, results and model.\nCan be used to incorporate domain knowledge and make learning easier.\n\nCons:\n\nYour model and results lose information\nSenseless cut-offs between bins can create “artificial noise” and make learning harder.\n\nMore info: stackexchange.com\nSee also: wikipedia: Sampling (signal processing).\n\n\nCombining rare categories\nRare categories can lead to noise in the data and blow up the amount of features when using one-hot encoding. These categories should be combined, when there are only few occurrences (e.g. When analysing page visits, combine the categories “blackberry”, “jolla”, “windows phone” into the category “other”).\n\nPandassklearnPyCaret\n\n\nimport pandas as pd\nimport numpy as np\ncounts_ser = pd.value_counts(df[\"feature\"])\ncategories_to_mask = counts_ser[(counts_ser/counts_ser.sum()).lt(0.05)].index # using 5% cut-off\ndf[\"feature\"] = np.where(df[\"feature\"].isin(categories_to_mask),'other',df[\"feature\"])\nMore info: stackoverflow\n\n\nIn sklearn, rare categories can be filtered out when one-hot encoding the feature using the parameter min_frequency.\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore', min_frequency=0.05)\nenc.fit_transform(df[\"feature\"])\nMore info:scikit-learn.org\n\n\nUse the parameter rare_to_value of the setup function.\nfrom pycaret.time_series import ClassificationExperiment # or use other Experiment type\nexp = ClassificationExperiment()\nexp.setup(train_df, target=\"Sales\", rare_to_value = 0.05)\nMore info: PyCaret Docs"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#date--and-time-data",
    "href": "source/Data/03-Preprocessing.html#date--and-time-data",
    "title": "6  Preprocessing data",
    "section": "6.5 Date- and time-data",
    "text": "6.5 Date- and time-data\nYou can convert to the datetime format as follows:\nimport pandas as pd\npd.to_datetime(df.date_col, infer_datetime_format=True)\nYou create columns for year, month, day like this:\nimport pandas as pd\ndf['year'] = df.Date.dt.year\ndf['month'] = df.Date.dt.month\ndf['day'] = df.Date.dt.day"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#graph-representation-of-data",
    "href": "source/Data/03-Preprocessing.html#graph-representation-of-data",
    "title": "6  Preprocessing data",
    "section": "6.6 Graph representation of data",
    "text": "6.6 Graph representation of data\nThe similarity/distance between points can be represented in graphs. The data points are represented as nodes, the distances/similarities as edges."
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#text-data",
    "href": "source/Data/03-Preprocessing.html#text-data",
    "title": "6  Preprocessing data",
    "section": "6.7 Text data",
    "text": "6.7 Text data\nThese are the common steps of pre-processing text data:\n\n\n\n\nflowchart TD\n  A(Data Cleaning) --&gt; B(Tokenization) --&gt; C(Vectorization) --&gt; G(Model)\n  C --&gt; D(Padding) --&gt; E(Embedding) --&gt; G\n\n\n\n\n\n\nCleaning text data\nThe aim is to remove errors, parts that are irrelevant for the task and to standardize.\n\nclean-textPandas\n\n\nThe Clean-text only requires one command for several cleaning tasks:\nInstall the packge:\npip install clean-text\nUsage (see steps in parameters):\nfrom cleantext import clean\n\nclean(\"some input\",\n    fix_unicode=True,               # fix various unicode errors\n    to_ascii=True,                  # transliterate to closest ASCII representation\n    lower=True,                     # lowercase text\n    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n    no_urls=False,                  # replace all URLs with a special token\n    no_emails=False,                # replace all email addresses with a special token\n    no_phone_numbers=False,         # replace all phone numbers with a special token\n    no_numbers=False,               # replace all numbers with a special token\n    no_digits=False,                # replace all digits with a special token\n    no_currency_symbols=False,      # replace all currency symbols with a special token\n    no_punct=False,                 # remove punctuations\n    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n    replace_with_url=\"&lt;URL&gt;\",\n    replace_with_email=\"&lt;EMAIL&gt;\",\n    replace_with_phone_number=\"&lt;PHONE&gt;\",\n    replace_with_number=\"&lt;NUMBER&gt;\",\n    replace_with_digit=\"0\",\n    replace_with_currency_symbol=\"&lt;CUR&gt;\",\n    lang=\"en\"                       # set to 'de' for German special handling\n)\n\n# or simply:\nclean(\"some input\", all= True)\n\n# use within pandas:\ndf[\"text\"] = df[\"text\"].apply(lambda txt : cleantext.clean_words(txt))\nThe command clean_words additionally returns the words as a list.\nMore info:\naim - Guide to CleanText\nGitub - clean-text repo\n\n\nimport pandas as pd\nimport re\n\ndf[\"text\"] = df[\"text\"].str.lower()               # make all words lowercase\ndf[\"text\"] = df[\"text\"].str.replace('ü', 'u')     # replace characters\ndf[\"text\"] = df[\"text\"].str.replace(r\"https?:\\/\\/.\\S+\",\"\", regex = True) # remove URLs\ndf[\"text\"] = df[\"text\"].str.replace(r\"&lt;.*?&gt;\",\"\", regex = True) # remove html-tags\n\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf[\"text\"] = df[\"text\"].apply(lambda text : remove_emoji(text))\n\ndf[\"text\"] = df[\"text\"].str.strip()               # strip away leading and trailing spaces\ndf[\"text\"] = df[\"text\"].str.replace(r\"[^\\w\\s]\", \"\", regex = True) # remove punctuation\n\n# Rarely used\ndf[\"text\"] = df[\"text\"].str.lstrip(\"123456789\")   # strip away leading numbers rstrip for trailing numbers (all combinations of characters will be stripped)\ndf[\"text\"] = df[\"text\"].str.replace(r\"\\(.*?\\)\",\"\", regex = True) # remove everything between brackets\ndf[\"year\"] = df[\"year\"].str.extract(r'^(\\d{4})', expand=False) # extracts year numbers\n\n\n\n\n\nTokenization\nTokenization is the act of splitting a text into sentences or words (i.e. tokens).\n\nWord-Tokenization\n\nNLTKSpaCy\n\n\nSplit the text into words:\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nwords = word_tokenize(cleaned_text) \n\n\nSpaCy uses a sophisticated text annotation method.\n\nDownload trained English linguistic annotation model:\n\n!python -m spacy download en_core_web_sm\n\nTokenize text:\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text_doc)\ntokens = [(token.text, token.pos_, token.dep_) for token in doc]\nAttributes:\npos_: Part-of-speech (e.g. noun, adjective, punctuation),\ndep_: Syntactic dependency relation (e.g. “Does … have” \\rightarrow Does (auxiliary verb), have (root verb))\nMore info:\nSpaCy - Features\n\n\n\n\n\nSentence Tokenization\n\nNLTKSpaCy\n\n\nfrom nltk.tokenize import sent_tokenize\nsentences = sent_tokenize(sentences_text)\n\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text_doc)\nsentences = [sent for sent in doc.sents]\nMore info: Tutorial on SpaCy Sentencer\n\n\n\n\n\n\nVectorization\nTransform sequence of tokens into numerical vector that can be processed by models.\n\nWord count encoding\nThis is part of the bag-of-words method. It works as follows:\n\nCreate a vocabulary / corpus of all words in the training data.\nEach word in the vocabulary becomes its own feature\nFor each document, count how many times the word occurs.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nword_counts = count_vect.fit_transform(doc_array)\nMore info: sklearn - extract features from text\nPros:\n\nSimple and easily interpretable.\n\nCons:\n\nOrder and relation between words is lost\nSparse representation is not easily usable for many models. (Large vocabularies make it worse \\rightarrow Use stemming)\n\n\n\nTerm frequency-inverse document frequency (tf-idf)\nThis measure reflects the importance of a word to a document:\nTerm frequency: What is the frequency of this word in this document.\nInverse document frequency: How rare is this word among all documents.\nThus, terms that occur a lot in one document but rarely in others get a higher value.\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer() \nword_tf_idfs = tf_transformer.fit_transform(word_counts) # uses wordcounts from count-vectorizer\nMore info: sklearn - extract features from text\n\n\n\nPadding\nSince some sequences are shorter than others, we need to fill up the remaining parts of them ones with zeros. Thus we achieve sequences of the same length. First we need to make an ordinal encoding and create word-sequences.\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\n# Convert text to sequence\ntokenizer = Tokenizer(num_words = vocab_size)\ntokenizer.fit_on_texts(train_texts)\nX_train_sequences = tokenizer.texts_to_sequences(train_texts)\n# Padd the sequences\ntrain_texts_padded = pad_sequences(train_texts, padding='post', maxlen=max_sequence_length*1.5)\n\n\nEmbedding\nEmbedding is the mapping of words from the sparse one-hot-encoded space into a dense space, that should reflect the meaning of the words (i.e. similar words are close together).\nThis is done in neural networks via an embedding layer:\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=sequence_length))\n# ... add further layers ...\nmodel.compile()\nYou can reuse trained embeddings for other tasks. See transfer-learning More info: Google Machine Learning - Prepare Your Data"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#image-data",
    "href": "source/Data/03-Preprocessing.html#image-data",
    "title": "6  Preprocessing data",
    "section": "6.8 Image data",
    "text": "6.8 Image data\n\n\n\n\nflowchart LR\n  A(Resize) --&gt; B(Split channels) --&gt; C(Scale) --&gt; E(Augment) --&gt; G(Model)\n  A --&gt; D(gray scale) --&gt; C\n\n\n\n\n\n\nKeras ImageDataGeneratorKeras image-loaderOpenCV\n\n\nfrom tf.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(samplewise_std_normalization=True, \n                            rotation_range=180, \n                            shear_range=20, \n                            zoom_range=0.1, \n                            horizontal_flip=True, \n                            vertical_flip=True, \n                            validation_split=0.7)\nimgs_train = datagen.flow_from_directory(directory = \"data/dir\", \n                                         target_size=(256, 256), \n                                         batch_size=32, \n                                         class_mode=\"categorical\", # classes will be determined from subdirectory names\n                                         subset=\"training\")\nimgs_test = datagen.flow_from_directory(directory = \"data/dir\", \n                                         target_size=(256, 256), \n                                         batch_size=32, \n                                         class_mode=\"categorical\", # classes will be determined from subdirectory names\n                                         subset=\"validation\")\nMore info: keras.io\n\n\nLoad image dataset\nfrom tf.keras.utils import image_dataset_from_directory\nimgs_train, imgs_test = image_dataset_from_directory(directory=\"path/tofolder\", labels=\"inferred\",color_mode=\"rgb\", image_size=(256, 256), validation_split=0.7, subset=\"both\"\n    label_mode=\"int\")\nload single image\nfrom tf.keras.utils import load_img\nfrom tf.keras.utils.image import img_to_array\nimg = load_img(path=\"path/toimg.png\", grayscale=False, color_mode=\"rgb\", target_size=(256, 256))\nimg = img_to_array(img)\nMore info: keras.io\nAugmentation\nfrom tf.keras import layers, Sequential\nimport numpy as np\n\nresize_and_rescale = Sequential([\n  layers.Rescaling(1./255),\n  layers.RandomFlip(\"horizontal_and_vertical\"),\n  layers.RandomRotation(0.2),\n])\n\nimages = []\nfor idx in range(10):\n  augmented_image = data_augmentation(img)\n  images.append(augmented_image)\n  \nimg_ar = np.array(images)\nMore info: keras.io\n\n\n# Adapted from https://github.com/bnsreenu\nimport os\nimport numpy as np \nimport glob # To go through folders\nimport cv2\n\ntrain_split = 0.7\nimg_size = 256\n\nimages_train = []\nimages_test = []\nlabels_train = [] \nlabels_test = [] \n\nfor dir_path in img_dir:\n    label = dir_path.split(\"/\")[-1]\n    print(label)\n    img_paths = glob.glob(os.path.join(dir_path, \"*.jpeg\"))\n    for img_idx, img_path in enumerate(img_paths):\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (img_size, img_size))\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = img/(255/2)-1 # scales 0 to 255 range to -1 to 1 (more or less zero-centered)\n        if img_idx &lt; train_split * len(img_paths):\n            images_train.append(img)\n            labels_train.append(label)\n            # flip image horizontally: \n            images_train.append(cv2.flip(img, 1))\n            labels_train.append(label)\n            # flip image vertically: \n            images_train.append(cv2.flip(img, 0))\n            labels_train.append(label)\n        else: \n            images_test.append(img)\n            labels_test.append(label)\n            \nimages_train = np.array(images_train)\nlabels_train = np.array(labels_train)\nimages_test = np.array(images_test)\nlabels_test = np.array(labels_test)\n\n\n\nscikit-image - userguide\nNeptune.ai - Image processing methods you should know"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#a-priori-feature-selection",
    "href": "source/Data/03-Preprocessing.html#a-priori-feature-selection",
    "title": "6  Preprocessing data",
    "section": "9.1 A priori feature selection",
    "text": "9.1 A priori feature selection\nA cheap method is to remove all features with variance below a certain threshold.\nfrom sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(threshold=0.1)\nselector.fit_transform(X)\nMore info: scikit-learn.org\nThe Mutual information score works by choosing the features that have the highest dependency between the features and the label. {#mutual_info}\n I(X, Y)\n\\\\ = D_{KL} \\left( P(X=x, Y=y),\nP(X=x) \\otimes P(Y=y) \\right) \\\\ = \\sum_{y \\in Y} \\sum_{x \\in X}\n    { P(X=x, Y=y) \\log\\left(\\frac{P(X=x, Y=y)}{P(X=x)P(Y=y)}\\right) }\nwhere, D_{KL} is the Kullback–Leibler divergence (A measure of similarity between distributions). The \\log-Term is for quantifying how different the joint distribution is from the product of the marginal distributions.\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif # for regression use mutual_info_regression\nX_new = SelectKBest(mutual_info_classif, k=8).fit_transform(X, y)\nMore info: scikit-learn.org\nwikipedia.org/wiki/Mutual_information"
  },
  {
    "objectID": "source/Data/03-Preprocessing.html#wrapper-methods",
    "href": "source/Data/03-Preprocessing.html#wrapper-methods",
    "title": "6  Preprocessing data",
    "section": "9.2 wrapper methods",
    "text": "9.2 wrapper methods\nUsing greedy feature selection as a wrapper method, one commonly starts with 0 features and adds the feature that returns the highest score with the used classifier.\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier()\nselector = SequentialFeatureSelector(classifier, n_features_to_select=8)\nselector.fit_transform(X, y)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/00-Supervised.html",
    "href": "source/Supervised/00-Supervised.html",
    "title": "Supervised learning",
    "section": "",
    "text": "Classification is the assignment of objects (data points) to categories (classes). Regression allows you to assign a continuous output to your data by estimating the relationship between your features and the output.\nBoth require a training data-set of points with known class labels and a test data-set for evaluation."
  },
  {
    "objectID": "source/Supervised/01-General-methods.html",
    "href": "source/Supervised/01-General-methods.html",
    "title": "7  General methods and concepts",
    "section": "",
    "text": "8 Hyper-parameter tuning\nThe hyper-parameters (e.g. kernel, gamma, number of nodes in tree) are not trained by algorithm itself. An outer loop of hyper-parameter tuning is needed to find the optimal hyper parameters.\n! It is strongly recommended to separate another validation set from the training set for hyper-parameter tuning (you’ll end up with training-, validation- and test-set). See Cross Validation for best practice.\nThe candidates for hyper-parameters must not be evaluated on the same data that you trained it on (over-fitting risk). Thus, we separate another data-set from the training data: The validation set. This is reduces the amount of training data drastically. Therefore we use the approaches of Cross Validation and Bootstrapping."
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#grid-search",
    "href": "source/Supervised/01-General-methods.html#grid-search",
    "title": "7  General methods and concepts",
    "section": "8.1 Grid search",
    "text": "8.1 Grid search\nThe classic approach is exhaustive grid search: You create a grid of hyper-parameters and iterate over all combinations. The combination with the best score is used in the end. This approach causes big computational costs due to the combinatorial explosion.\nfrom sklearn.model_selection import GridSearchCV # combines grid search with cross-validation\nfrom sklearn.neighbors import KNeighborsClassifier\n\nkn_model = KNeighborsClassifier(n_neighbors=3)\nparameters = {\"n_neighbors\": range(2,10), \"p\": [1,2], \"weights\": [\"uniform\", \"distance\"]}\nclf = GridSearchCV(kn_model, parameters, cv=5)\nclf.fit(X_train, y_train)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#randomized-search",
    "href": "source/Supervised/01-General-methods.html#randomized-search",
    "title": "7  General methods and concepts",
    "section": "8.2 Randomized search",
    "text": "8.2 Randomized search\nThis approach is used, if there are too many combinations of hyper-parameters for tuning. You allocate a budget of iterations and the combinations of parameters are sampled randomly according to the distributions you provide.\nIf you want to evaluate on a large set of hyperparameters, you can use a halving strategy: You tune a large combination of parameters on few resources (e.g. samples, trees). The best performing half of candidates is re-evaluated on twice as many resources. This continues until the best-performing candidate is evaluated on the full amount of resources.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.experimental import enable_halving_search_cv  # since this method is still experimental\nfrom sklearn.model_selection import HalvingRandomSearchCV\nfrom sklearn.utils.fixes import loguniform\n\nrf_clf = RandomForestClassifier()\n\nparam_distributions = {\"max_depth\": [3, None],\n                       \"min_samples_split\": loguniform(1, 10)}\nhypa_search = HalvingRandomSearchCV(rf_clf, param_distributions,\n                               resource='n_estimators',\n                               max_resources=10,\n                               n_jobs=-1, # important since hyper-parameter tuning is very costly\n                               scoring = 'balanced_accuracy',\n                               random_state=0).fit(X, y)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#crossval",
    "href": "source/Supervised/01-General-methods.html#crossval",
    "title": "7  General methods and concepts",
    "section": "9.1 Cross Validation",
    "text": "9.1 Cross Validation\nIn k-fold Cross Validation, we split the training set into k sub-sets. We train on the samples in k-1 sub-sets and validate using the data in the remaining sub-set. We iterate until we have validated on each sub-set once. We then average out the k scores we obtain.\n\n\n\nSchema of the process for 5-fold Cross Validation. The data is first split into training- and test-data. The training data is split into 5 sub-sets. The algorithm is trained on 4 sub-sets and evaluated on the remaining sub-set. Each sub-set is used for validation once. Source: scikit-learn.org.\n\n\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nSVM_clf = svm.SVC (kernel='polynomial')\ncv_scores = cross_val_score(SVM_clf, X, y, cv = 7)\ncv_score = cv_scores.mean()\nMore info: scikit-learn.org\n! If you have time-series data (and other clearly not i.i.d.) data, you have to use special cross-validation strategies. There are further strategies worth considering.\n\nBootstrapping\nInstead of splitting the data into k subsets, you can also just sample data into training and validation sets.\nMore info: wikipedia.org."
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#errors-regularization",
    "href": "source/Supervised/01-General-methods.html#errors-regularization",
    "title": "7  General methods and concepts",
    "section": "9.2 Errors & regularization",
    "text": "9.2 Errors & regularization\nThere are irreducible errors and reducible errors. Irreducible errors stem from unknown variables or variables we have no data on. Reducible errors are deviations from our model to its desired behavior and can be reduced. Bias and variance are reducible errors.\n\\text{Error} = \\text{Bias} + \\text{Var} + \\text{irr. Error}"
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#bias-and-variance",
    "href": "source/Supervised/01-General-methods.html#bias-and-variance",
    "title": "7  General methods and concepts",
    "section": "9.3 Bias and Variance",
    "text": "9.3 Bias and Variance\n\nBias of an estimator\nBias tells you if your model oversimplifies the true relationship in your data (underfitting).\nYou have a model with a parameter \\hat{\\theta} that is an estimator for the true \\theta. You want to know whether your model over- or underestimates the true \\theta systematically.\n\\text{Bias}[\\hat{\\theta}]=\\text{E}_{X|\\mathcal{D}}[\\hat{\\theta}]- \\theta\nE.g. if the parameter captures how polynomial the model / relationship of your data is, a too high value means that your model is underfitting.\n\nMore info: wikipedia.org\n\n\nVariance of an estimator\nVariance tells you if your model learns from noise instead of the true relationship in your data (overfitting).\n\\text{Var}[\\hat{\\theta}]=\\text{E}_{X|\\mathcal{D}}[(\\text{E}_{X|\\mathcal{D}}[\\hat{\\theta}]- \\hat{\\theta})^2] i.e. If you would bootstrap your data, it would show you how much your parameter would jump around its mean, when it learns from the different sampled sets.\n\nYour goal is now to find the sweet spot between a too biased (too simple model) and a model with too high variance (too complex model).\n\n\n\n\nRelationship between bias, variance and the total error. The minimum of the total error lies at the best compromise between bias and variance. Source: User Bigbossfarin on wikimedia.org..\n\n\nMore info: wikipedia.org"
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#regularization",
    "href": "source/Supervised/01-General-methods.html#regularization",
    "title": "7  General methods and concepts",
    "section": "9.4 Regularization",
    "text": "9.4 Regularization\nTo combat overfitting, we can introduce a term into our loss-function that penalizes complex models. For linear regression, our regularized loss function is will be:\n\\min L(\\hat{y},y)= \\min_{W,b} f(WX+b,y)+\\lambda R(W) where f is the unregularized loss function, W is the weight matrix, X is the sample matrix and b is the bias or offset term of the model (bias term \\neq bias of estimator!). R is the regularization function and \\lambda is a parameter controlling its strength.\ni.e. The regularized loss function punishes large weights W and leads to flatter/smoother functions.\n\nMore info: wikipedia.org"
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#bagging",
    "href": "source/Supervised/01-General-methods.html#bagging",
    "title": "7  General methods and concepts",
    "section": "9.5 Bagging",
    "text": "9.5 Bagging\nTrain several instances of a complex estimator (aka. strong learner, like large decision trees or KNN with small radius) on a subset of the data. Then use a majority vote or average the scores for classifying to get the final prediction. By training on different subsets and averaging the results, the chances of overfitting are greatly reduced.\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nbagging = BaggingClassifier(KNeighborsClassifier(), max_features=0.5, n_estimators=20)\nMore info: scikit-learn.org\n\nA classic example for a bagging classifier is Random Forest Classifier or its variant Extremely Randomized Trees which further reduces variance and increases bias."
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#boosting",
    "href": "source/Supervised/01-General-methods.html#boosting",
    "title": "7  General methods and concepts",
    "section": "9.6 Boosting",
    "text": "9.6 Boosting\nCompared to bagging, we use weak learners that are not trained independently of each other. We start with a single weak learner (e.g. a small decision tree) and repeat the following steps:\n\nAdd an additional model and train it.\nIncrease weights of training samples that are falsely classified, decrease weights of correctly classified samples. (to be used by next added model.)\nReweight results from the models in the combined model to reduce the training error.\n\nThe final model is an weighted ensemble of weak classifiers.\nThe most popular ones are gradient boosted decision tree algorithms."
  },
  {
    "objectID": "source/Supervised/01-General-methods.html#stacking",
    "href": "source/Supervised/01-General-methods.html#stacking",
    "title": "7  General methods and concepts",
    "section": "9.7 Stacking",
    "text": "9.7 Stacking\nStacking closely resembles bagging: An ensemble of separately trained base models is used to create an ensemble model. However, the continuous (instead of discrete) outputs of commonly fewer heterogeneous models (instead of same type of models) are used. The continuous outputs are then fed into a final estimator (commonly logistic regression classifier).\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import StackingClassifier\n\nclassifiers = [\n    ('svc', SVC()),\n    ('knn', KNeighborsClassifier()),\n    ('dtc', DecisionTreeClassifier())\n    ]\n\nclf = StackingClassifier(\n    classifiers=estimators, final_estimator=LogisticRegression()\n    )\n\nclf.fit(X, y)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/02-Classification.html",
    "href": "source/Supervised/02-Classification.html",
    "title": "8  Classification",
    "section": "",
    "text": "9 Evaluation of Classifiers\nNaive Bayes classifies works on the assumption that the features are conditionally independent given the class label. For every point a simplified version of Bayes rule is used:\nP(Y=y_i|X=x) \\propto P(X=x|Y=y_i) * P(Y=y_i)  where Y is the RV for the class label and X is the RV that contains the feature values. This holds since P(X=x) is the same for all classes. Since the different features X_j are assumed to be independent they can be multiplied out. The label y_i with the highest probability is the predicted class label: \\arg \\max_{y_i} P(Y=y_i|X=x) \\propto P(Y=y_i) \\prod_{j=1}^{d} P(X_j=x_j|Y=y_i)   One usually estimates the value of P(Y) as the ferquency of the different classes in the training data or assumes that all classes are equally likely.\nTo estimate the P(X_j=x_j|Y=y_i) the following distributions are commonly used:\nFor discrete features, you need to use a smoothing prior (add 1 to every feature count) to avoid 0 probabilities for samples with features being 0 in the training data.\nPros:\nCons:\nMore info: scikit-learn.org\nContrary to Naive Bayes, the features in LDA are not assumed to be independently distributed. As with Bayes rule a distribution for each class is calculated according to Bayes rule. P(X=x|Y=y_i) is modeled as a multivariate Gaussian distribution. The Gaussians for each class are assumed to be the same. The log-posterior can be simplified to:\nlog(P(y=y_i|x) = - \\frac{1}{2} (x-\\mu_i)^t \\Sigma^{-1}(x-\\mu_i)+\\log P(y=y_i) \\mu_i is the mean of class i, (x-\\mu_i)^t \\Sigma^{-1}(x-\\mu_i) corresponds to the Mahalanobis distance. Thus, we assign the point to the class whose distribution it is closest to.\nLDA can also be thought of projecting the data into a space with k-1 dimensions (k being number of classes). More info: wikipedia.org. It can also be used as a dimensionality reduction method.\nMore info: scikit-learn.org\nSVCs use hyperplanes to separate data points according to their class label with a maximum margin (M) between the separating hyperplane (x^T\\beta + \\beta_0=0) and the points. If points cannot be perfectly separated by the decision boundary, a soft margin SVM is used with a slack variable \\xi that punishes points in the margin or on the wrong side of the hyperplane. The optimization problem is given by:\n\\begin{split}\n            \\max_{\\beta, \\beta_0, \\beta=1} M, \\\\\n            \\text{subject to } y_i(x_i^T \\beta + \\beta_0) \\ge 1 - \\xi_i, \\quad \\forall i, \\\\\\xi_i \\ge 0, \\quad \\sum \\xi_i \\le constant, \\quad i= 1, ..., N,\n        \\end{split}\nwhere \\beta are the coefficients and x are the N data points. The support vectors are the points that determine the orientation of the hyperplane (i.e. the closest points). The classification function is given by:\nG(x) = \\text{sign}[x^T\\beta + \\beta_0]\nIf you only calculate the inner part of the function you can get the distance of a point to your hyperplane (in SKlearn you need to divide by the norm vector w of your hyperplane to get the true distance). To get the probability of a point being in a class, you can use Platt’s algorithm. SVMs are sensitive to the scaling of the features. Therefore, the data should be normalized before classification.\nA decision tree uses binary rules to recursively split the data into regions that contain only a single class.\nPros:\nCons:\nTips:\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/02-Classification.html#confusion-matrix",
    "href": "source/Supervised/02-Classification.html#confusion-matrix",
    "title": "8  Classification",
    "section": "9.1 Confusion matrix",
    "text": "9.1 Confusion matrix\nThis gives a quick overview on the distribution of true positives (TP), false positives (FP) , TN true negatives, FN false negatives.\n\n\n\n\npredicted positive\npredicted negative\n\n\nactual positive\nTP\nFN\n\n\nactual negative\nFP\nTN\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nconfusion = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=categories_list)\nsns.heatmap(confusion, annot=True, xticklabels=categories_list, yticklabels=categories_list, fmt='.0f')"
  },
  {
    "objectID": "source/Supervised/02-Classification.html#basic-quality-measures",
    "href": "source/Supervised/02-Classification.html#basic-quality-measures",
    "title": "8  Classification",
    "section": "9.2 Basic Quality Measures",
    "text": "9.2 Basic Quality Measures\n\n\\text{Accuracy / Success Rate} = \\frac{ \\text{correct predictions}}{\\text{total predictions}} = \\frac{ \\text{TP}+\\text{TN}}{\\text{TP} + \\text{TN}+\\text{FP}+\\text{FN}}\nThis metric should only be used in this pure form, when the number of positive and negative samples are balanced.\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\ni.e. How many of your positive predictions are actually positive?\n\\text{True positive rate / Recall / Sensitivity} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\ni.e. How many of the positive samples did you catch?\n\\text{True negative rate / Specificity / Selectivity} = \\frac{\\text{TN}}{\\text{TN}+\\text{FP}}\ni.e. How many of the negative samples did you catch as negative (i.e. are truly negative)?\n\\text{F-score} = 2 \\frac{\\text{precision} * \\text{recall}}{\\text{precision}+\\text{recall}}\nThis is useful in cases of unbalanced classes to balance the trade-off between precision and recall.\n\nfrom sklearn.metrics import classification_report\nclassification_report(y_true, y_pred)\nmore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/02-Classification.html#area-under-the-curve",
    "href": "source/Supervised/02-Classification.html#area-under-the-curve",
    "title": "8  Classification",
    "section": "9.3 Area under the Curve",
    "text": "9.3 Area under the Curve\nThis class of measures represents the quality of the classifier for different threshold values \\theta by calculating the area under the curve spanned by different quality measures.\n\nArea under the Receiver Operating Characteristics Curve (AUROC or AUC)\nThe AUC can be interpreted as follows: When the classifier gets a positive and a negative point, the AUC shows the probability that the classifier will give a higher score to the positive point. A perfect classifier has an AUC of 1, and AUC of 0.5 represents random guessing.\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y, clf.decision_function(X)) # instead of dec. func. you can use clf.predict_proba(X)\nMore info: scikit-learn.org\n! This measure is not sensitive to class imbalance!\n\n\n\nArea under the precision recall curve. Source: user cmglee on wikipedia.org\n\n\n\n\nArea under the Precision-Recall Curve (AUPRC) / Average Precision (AveP)\nThis measure can be used for unbalanced data sets. It represents the average precision as a function of the recall. The value of 1 represents a perfect classifier.\nfrom sklearn.metrics import average_precision_score\naverage_precision_score(y_true, y_pred)\nMore info: scikit-learn.org\n\n\n\nThe precision-recall curve. Source: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/02-Classification.html#handling-unbalanced-data",
    "href": "source/Supervised/02-Classification.html#handling-unbalanced-data",
    "title": "8  Classification",
    "section": "9.4 Handling Unbalanced Data",
    "text": "9.4 Handling Unbalanced Data\nHaving many more samples in one class than the others during training can lead to high accuracy values event though the classifier performs poorly on the smaller classes. You can handle the unbalance by:\n\nup-sampling the smaller data set (creating more artificial samples for that class)\ngiving more weight to the samples in the smaller data set\nusing a quality measure that is sensitive to class imbalance\n\nOversampling using imbalanced-learn (see: )\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nfeatures_resampled, labels_resampled = ros.fit_resample(df[feature_cols], df[label_col])\nUnbalance-sensitive quality measures: Sensitivity, specificity, precision, recall, support and F-score\ny_true = df[label_col]\ny_pred = classifier.predict(df[feature_cols])\n\nfrom imblearn.metrics import sensitivity_specificity_support\nsensitivity, specificity, support = sensitivity_specificity_support(y_true, y_pred) \n\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred)"
  },
  {
    "objectID": "source/Supervised/02-Classification.html#nearest-neighbors-classifier",
    "href": "source/Supervised/02-Classification.html#nearest-neighbors-classifier",
    "title": "8  Classification",
    "section": "9.5 Nearest Neighbors Classifier",
    "text": "9.5 Nearest Neighbors Classifier\nThis classifier predicts the class label using the most common class label of its k nearest neighbors in the training data set.\nPros:\n\nClassifier does not take time for training.\nCan learn complex decision boundaries.\n\nCons:\n\nThe prediction is time consuming and scales with n.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nkn_model = KNeighborsClassifier(n_neighbors=5)\nkn_model.fit(X, y)\nkn_model.predict([[5,1]])\nscikit-learn.org"
  },
  {
    "objectID": "source/Supervised/02-Classification.html#random-forests",
    "href": "source/Supervised/02-Classification.html#random-forests",
    "title": "8  Classification",
    "section": "13.1 Random forests",
    "text": "13.1 Random forests\nRandom forests are a version of a bagging classifier employing decision trees. To reduce the variance, the separate trees can be assigned a limited number of features as well.\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=10, max_features=\"sqrt\", class_weight=\"balanced\")\nclf.fit(X, y)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/02-Classification.html#gradient-boosted-decision-trees-gbdts",
    "href": "source/Supervised/02-Classification.html#gradient-boosted-decision-trees-gbdts",
    "title": "8  Classification",
    "section": "13.2 Gradient boosted decision trees (GBDTs)",
    "text": "13.2 Gradient boosted decision trees (GBDTs)\nGradient boosted decision tree models are a form of boosting employing decision trees.\nimport lightgbm as lgbm\nclf = lgbm.LGBMClassifier(class_weight= \"balanced\")\nclf.fit(X, y)\nMore info: lightgbm documentation, Parameter tuning,\nFurther Parameter tuning\nSimilar model: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/03-Regression.html",
    "href": "source/Supervised/03-Regression.html",
    "title": "9  Regression",
    "section": "",
    "text": "10 Time Series Forecasting\nFor “normal” settings the order of the samples does not play a role (e.g. blood sugar level of one sample is independent of the others). In time series however, the samples need to be represented in an ordered vector or matrix (e.g. The temperature of Jan 2nd is not independent of the temperature on Jan 1st)."
  },
  {
    "objectID": "source/Supervised/03-Regression.html#eval-reg",
    "href": "source/Supervised/03-Regression.html#eval-reg",
    "title": "9  Regression",
    "section": "9.1 Evaluation of regression models",
    "text": "9.1 Evaluation of regression models\n\nMean squared error\nThis measure shows the deviation of the predicted value \\hat{y} to the target value y. The squaring penalized large deviations and avoids respective cancellation of positive and negative errors.\n MSE = 1/n \\sum_i (y_i - \\hat{y}_i)^2\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_true, y_pred)\nMore info: scikit-learn.org\n\n\nR^2 score / coefficient of determination\nThis measure shows how much of the variance of the target/dependent variable y can be explained by the model/independent variable \\hat{y}.\n\nR^2 = 1 - \\frac{\\text{Unexplained Variance}}{\\text{Total Variance}} = \\frac{SS_{res}}{SS_{tot}} \\\\\nSS_{res} = \\sum_i(y_i-\\hat{y}_i)^2 \\\\\nSS_{tot} = \\sum_i(y_i - \\bar{y}_i)^2\n where \\bar{y} is the mean of the target y.\nThe value commonly reaches from 0 (model always predicts the mean of y) to 1 (perfect fit of model to data). It can however be negative (e.g. wrong model, heavy overfitting, …). The adjusted R^2 compensates for the size of the model (more variables), favoring simpler models. More info: wikipedia.org\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_true, y_pred)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/03-Regression.html#linear-models",
    "href": "source/Supervised/03-Regression.html#linear-models",
    "title": "9  Regression",
    "section": "9.2 Linear Models",
    "text": "9.2 Linear Models\n\nOrdinary Least Squares\nLinear models are of the form: y_i = x_i \\beta + \\epsilon_i = \\beta_0 + \\beta_1 x_{i(1)} + \\beta_2 x_{i(2)} + \\epsilon_i\n\nx_{i(d)} is the dimension d of point i. x_i is called regressor, independent, exogenous or explanatory variable. The regressors can be non-linear functions of the data.\ny_i is the observed value for the depependent variable for point i.\n\\beta_0 is called bias or intercept (not the same as bias of a machine learning model).\n\\beta_{0, 1...n} are the regression coefficients, \\beta_{1,...n} are called slope. In linear models the regression coefficients need to be linear.\n\\epsilon is the error term or noise.\n\nPredicted values are denoted as \\hat{\\beta}, \\hat{y}. We try to minimize the \\epsilon-term using the least squared error method.\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(X, y)\nMore info: scikit-learn.org\nPros:\n\nEasy to interpret\nFast to train and predict\n\nCons:\n\nAssumption of linear relation between dependent and independet variables. (-&gt; possibly underfitting)\nSensitive to outliers (-&gt; possibly overfitting)\n\n\n\n\n\n\n\nTips for interpreting linear models\n\n\n\n\n\n\nWhen comparing the strength of different coefficients: Take the scale of the feature into consideration (e.g. don’t compare “m/s” and “km/h”).\nOnly when the features have been standardized / normalized, you can safely compare them.\nCheck for robustness of coefficients: Make cross validation and obsever their variability. High variability can be a sign of correlation with other features.\nCorrelation does not mean causation. r emo::ji(\"point_up_2\") r emo::ji(\"nerd\")"
  },
  {
    "objectID": "source/Supervised/03-Regression.html#gaussian-process-regression",
    "href": "source/Supervised/03-Regression.html#gaussian-process-regression",
    "title": "9  Regression",
    "section": "9.3 Gaussian process regression",
    "text": "9.3 Gaussian process regression\nGaussian process regression is based on Bayesian Probability: You generate many models and calculate the probability of your models given the samples. You make predictions based on the probabilities of your models.\nYou get non-linear functions to your data by using non-linear kernels: You assume that input data points that are similar, will have similar target values. The concept of similarity (e.g. same hour of the day) is encoded in the kernels that you use.\n\n\n\nSchema of the training process of Gaussian process regression. The left graph shows the prior samples of functions before. These functions are then conditioned on the data (graph in middle). The right graph shows the predictions with the credible intervals in gray. Source: user Cdipaolo96 on wikimedia.org .\n\n\nPros:\n\nThe model reports the predictions with a certain probability.\nHyperparameter tuning is built into the model.\n\nCons:\n\nTraining scales with O(n^3). (approximations are FITC and VFE)\nYou need to design or choose a kernel.\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, ExpSineSquared\nkernel = DotProduct() + WhiteKernel() + RBF() + ExpSineSquared() # The kernel hyperparameters are tuned by the model\ngpr = GaussianProcessRegressor(kernel=kernel)\ngpr.fit(X, y)\ngpr.predict(X, return_std=True)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Supervised/03-Regression.html#gradient-boosted-tree-regression",
    "href": "source/Supervised/03-Regression.html#gradient-boosted-tree-regression",
    "title": "9  Regression",
    "section": "9.4 Gradient boosted tree regression",
    "text": "9.4 Gradient boosted tree regression\nApart from classification, gradient boosted trees also allow for regression. It works like gradient boosted trees for classification: You iteratively add decision tree regressors that minimize the regression loss of the already fitted ensemble. A decision tree regressor is a decision tree that is trained on continuous data instead of discrete classification data, but its output is still discrete.\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(n_estimators = 500, min_samples_split =5, max_depth = 4, max_features=\"sqrt\", n_iter_no_change=15)\ngbr.fit(X, y)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html",
    "href": "source/Unsupervised/01-Clustering.html",
    "title": "10  Custering methods",
    "section": "",
    "text": "11 Evaluation of clustering algorithms\nGoal: Divide data into K clusters so that the variance within the clusters is minimized. The objective function:\nV(D) = \\sum_{i=1}^k \\sum{x_j \\in C_i} (x_j - \\mu_i)^2, where V is the variance, C_i is a cluster, \\mu_i is a cluster mean, x_j is a datapoint. The algorithm works as follows:\nMore info: scikit-learn.org\nA faster alternative is mini batch K-Means.\nYou represent data set D as a graph G=(V,E) and divide it up in connected sub-graphs that represent your clusters. Each edge e_{ij} (between nodes v_i and v_j) has a weight w_{ij} (which is commonly a similarity or distance measure).\nThe underlying assumption of SSP is that the different clusters reside in different subspaces of the data. Clusters are therefore perpendicular to each other and points in a cluster can only be reconstructed by combinations of points in the same cluster (\\rightarrow self-expressiveness, the reconstruction vectors ought to be sparse). For each point you try to find other points that can be used to recreate that point - these then form the same cluster. Doing that for all points gives you a data matrix X and a matrix of reconstruction vectors V:\nX = X*V\\text{ s.t. diag}(V)=0.\nYou now try to minimize the V-matrix according to the L1-norm (giving you a sparse matrix). This matrix can then be used for e.g. spectral clustering.\nMore details in the original paper on SSC-OMP.\nMore info: github.com\nSoft clustering assigns to each point the probabilities of belonging to each of the clusters instead of assigning it to only one cluster. This gives you a measure on how certain the algorithm is about the clustering of a point.\nSee chapter Neural Networks (5)"
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#silhouette-coefficient",
    "href": "source/Unsupervised/01-Clustering.html#silhouette-coefficient",
    "title": "10  Custering methods",
    "section": "11.1 Silhouette coefficient",
    "text": "11.1 Silhouette coefficient\nThe silhouette coefficient compares the average distance of a point and the points in its own cluster d(x,\\mu_{C}) to the average distance between the point and and the points of the second nearest Cluster d(x,\\mu_{C'}).\ns(x) = \\frac{d(x,\\mu_{C'})-d(x,\\mu_{C})}{\\max(d(x,\\mu_{C}), d(x,\\mu_{C'}))} where C is the own cluster and C' is the second nearest cluster. If a point is clearly in its own cluster, s(x) is close to 1. If a point is between two clusters, s(x) is close to 0. If a point is closer to another cluster, s(x) is negative.\nBy varying the number of clusters, one can find the number with the highest silhouette coefficients.\nPros:\n\nThe score is high for dense and highly separated clusters.\n\nCons:\n\nThe silhouette coefficient is mainly suitable for convex clusters, since it gives high values to this kind of clusters.\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nclu = KMeans(n_clusters=4)\nclu.fit(X)\nlabels = clu.labels_\nsilhouette_score(X, labels, metric='manhattan')\nMore info: scikit-learn.org\n\nA faster alternative is the Davies-Bouldin score, where values closer to 0 indicate a better clustering."
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#adjusted-mutual-information-score",
    "href": "source/Unsupervised/01-Clustering.html#adjusted-mutual-information-score",
    "title": "10  Custering methods",
    "section": "11.2 Adjusted mutual information score",
    "text": "11.2 Adjusted mutual information score\nIf you have labelled samples, you can use the mutual information score to test if the classes correspond to your clusters. The adjusted mutual information score adjusts for chance.\nfrom sklearn.metrics import adjusted_mutual_info_score\nadjusted_mutual_info_score(Y, clusters)"
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#basic-graph-based-clustering",
    "href": "source/Unsupervised/01-Clustering.html#basic-graph-based-clustering",
    "title": "10  Custering methods",
    "section": "13.1 Basic Graph-Based Clustering",
    "text": "13.1 Basic Graph-Based Clustering\nThe basic algorithm works like this:\n\nDefine a weight-threshold \\theta.\nFor all edges: if w_{ij} &gt; \\theta: remove e_{ij}.\nIf nodes are connected by a path (found via depth first search): Assign them to the same cluster."
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#dbscan",
    "href": "source/Unsupervised/01-Clustering.html#dbscan",
    "title": "10  Custering methods",
    "section": "13.2 DBScan",
    "text": "13.2 DBScan\nDensity-Based Spatial Clustering of Applications with Noise is a more noise robust version of basic graph-based clustering. You create clusters based on dense and connected regions. It works like this:\n\nA point is a core point if at least \\text{minPts} are within a radius of \\epsilon of the point (including the point itself).\nA point is directly reachable if it is not a core point but within \\epsilon from a core point.\nAll other points are not part of the cluster (and may not be part of any cluster).\n\n! For points between clusters, the assignment to a cluster depends on the order of point assignments.\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=3, min_samples=4)\ndbscan.fit(X)\nMore info: scikit-learn.org\n\nThere is a newer version of this algorithm (Hierarchical DBSCAN), that allows for clusters with varying density, more robustness in cluster assignment and makes tuning \\epsilon unnecessary."
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#cut-based-clustering",
    "href": "source/Unsupervised/01-Clustering.html#cut-based-clustering",
    "title": "10  Custering methods",
    "section": "13.3 Cut-Based Clustering",
    "text": "13.3 Cut-Based Clustering\nYou introduce a adjacency/similarity matrix W (measures similarity between data points) and define the number of clusters k. You now try to minimize the weight of edges \\kappa between the clusters C (equal to cutting edges between nodes that are least similar):\n\\begin{aligned}\n                \\begin{split}  \n                    \\min \\frac{1}{2} \\sum_{a=1}^k \\sum_{b=1}^k \\kappa(C_a, C_b) \\\\\n                    \\text{where } \\kappa(C_a, C_b) = \\sum_{v_i \\in C_a , v_j \\in C_b , a \\neq b} W_{ij} \\\\\n                    \\text{ and } \\kappa(C_a, C_a) = 0\n                \\end{split}\n            \\end{aligned} \\rightarrow You only add up the similarities/edge-weights between your clusters (but not within your clusters).\nFor constructing the similarity matrix, different kernels can be used (commonly the linear kernel or the Gaussian kernel)."
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#spectral-clustering",
    "href": "source/Unsupervised/01-Clustering.html#spectral-clustering",
    "title": "10  Custering methods",
    "section": "13.4 Spectral Clustering",
    "text": "13.4 Spectral Clustering\nSpectral clustering works by non-linearly mapping the matrix-representation of the graph onto a lower-dimensional space based on its spectrum (set of eigenvectors) and group the points there. The mapping preserves local distances, i.e. close points stay close to each other after the mapping. It employs three steps: Preprocessing, decomposition and grouping.\nPreprocessing\nWe create a Laplacian matrix L (Laplacian operator in matrix form, measuring how strongly a vertex differs from nearby vertices (because the edges are similarity measures)): L = D - W D_{ij} = \\begin{cases}\n        \\sum_{j=1}^N W_{ij} \\\\\n        0 \\text{ if } i \\neq j\n    \\end{cases} where D is the degree matrix (the (weighted) degree of each node is on the diagonal) and W is the adjacency/similarity matrix (measures similarity between data points).\nDecomposition\nYou first normalize the Laplacian to avoid big impacts of highly connected vertices/nodes. More info on the calculation on wikipedia.org.\nWe make eigenvalue decomposition:\nL U = \\Lambda U \\quad \\rightarrow \\quad L = U \\Lambda U^{-1} \nwhere U is the matrix of eigenvectors and \\Lambda is the diagonal matrix of eigenvalues. You can now find a lower-dimensional embedding by choosing the k smallest non-zero eigenvalues. The final data is now represented as a matrix of k eigenvectors.\nGrouping\nYou have multiple options:\n\nYou can cut the graph by using the chosen eigenvectors and splitting at 0 or median value.\nYou get the final cluster assignments by normalizing the now k-dimensional data and applying k-means clustering to it.\n\nfrom sklearn.cluster import SpectralClustering\nscl = SpectralClustering(n_clusters=4,\n        affinity='rbf',\n        assign_labels='cluster_qr', # assigns labels directly from Eig vecs,\n        n-jobs = -1)\nscl.fit(X)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#gaussian-mixture-models",
    "href": "source/Unsupervised/01-Clustering.html#gaussian-mixture-models",
    "title": "10  Custering methods",
    "section": "15.1 Gaussian Mixture Models",
    "text": "15.1 Gaussian Mixture Models\nGaussian mixture models try to find an ensemble of gaussian distributions that best describe your data. These distributions/components are used as your clusters. Your points belong to each cluster with a certain probability. To find these distributions, we use an expectation maximization algorithm:\n\nAssume the centers of your Gaussians (e.g. by k-means) and calculate for each point the probability of being generated by each distribution (p(x_i \\in C_k | \\phi_i, \\mu_k, \\sigma_k)).\nChange the parameters to maximize the likelihood of the data, given the cluster probabilities for all points.\n\nThe probability of a data point belonging to a cluster can be calculated via Bayes theorem.\nMore info on the theory: brilliant.org.\nfrom sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=4, covariance_type='full')\ngm.fit(X)\ngm.predict_proba(X)\nMore info: scikit-learn.org"
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#other-models",
    "href": "source/Unsupervised/01-Clustering.html#other-models",
    "title": "10  Custering methods",
    "section": "15.2 Other models",
    "text": "15.2 Other models\nOther models also have means to calculate cluster probabilities for points. For the HDBSCAN-algorithm see here."
  },
  {
    "objectID": "source/Unsupervised/01-Clustering.html#hierarchical-clustering",
    "href": "source/Unsupervised/01-Clustering.html#hierarchical-clustering",
    "title": "10  Custering methods",
    "section": "15.3 Hierarchical Clustering",
    "text": "15.3 Hierarchical Clustering\nInstead of clustering a point to only one cluster, you assign it to a hierarchy.\nPros:\n\nHierarchies of clusters reflects the data set and therefore the relationship between points better.\n\nCons:\n\nIt is more difficult to make clear statements of cluster membership \\rightarrow define a limit for hierarchical depth"
  },
  {
    "objectID": "source/Unsupervised/02-lower-dim-mapping.html",
    "href": "source/Unsupervised/02-lower-dim-mapping.html",
    "title": "11  Mapping to lower dimensions",
    "section": "",
    "text": "12 Outlier detection"
  },
  {
    "objectID": "source/Unsupervised/02-lower-dim-mapping.html#manifold-learning",
    "href": "source/Unsupervised/02-lower-dim-mapping.html#manifold-learning",
    "title": "11  Mapping to lower dimensions",
    "section": "11.1 Manifold learning",
    "text": "11.1 Manifold learning\n\nIsomap\n\n\nLocal linear embedding\n\n\nMulti dimensional scaling"
  },
  {
    "objectID": "source/Unsupervised/02-lower-dim-mapping.html#decomposition-techniques",
    "href": "source/Unsupervised/02-lower-dim-mapping.html#decomposition-techniques",
    "title": "11  Mapping to lower dimensions",
    "section": "11.2 Decomposition techniques",
    "text": "11.2 Decomposition techniques\n\nSingular value decomposition\nSingular value decomposition is used to compress large matrices of your data into smaller ones, with much less data, but without loosing a lot of information. Please visit the mathematical explanation for the underlying mechanisms.\n\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=10)\nsvd.fit_transform(X_train)\nsvd.transform(X_test)\n\n\n\nPrinciple Component analysis (PCA)"
  },
  {
    "objectID": "source/Unsupervised/02-lower-dim-mapping.html#local-outlier-factor",
    "href": "source/Unsupervised/02-lower-dim-mapping.html#local-outlier-factor",
    "title": "11  Mapping to lower dimensions",
    "section": "12.1 Local outlier factor",
    "text": "12.1 Local outlier factor"
  },
  {
    "objectID": "source/Unsupervised/02-lower-dim-mapping.html#isolation-forest",
    "href": "source/Unsupervised/02-lower-dim-mapping.html#isolation-forest",
    "title": "11  Mapping to lower dimensions",
    "section": "12.2 Isolation forest",
    "text": "12.2 Isolation forest"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html",
    "href": "source/Neural-Networks/0-Neural.html",
    "title": "Neural Networks",
    "section": "",
    "text": "13 Fundamentals\nOn the most basic level, neural networks consist of many simple models (e.g. linear and logistic models) that are chained together in a directed network. The models sit on the neurons (nodes) of the network. The most important components of neurons are:\nThe neurons (nodes) in the first layer uses as its input the sample values and feeds its output into the activation function of the next nodes in the next layer, a.s.o. The later layers should thereby learn more and more complicated concepts or structures.\nExplanation on the idea and mechanisms of neural networks: Stanford Computer Vision Class"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#non-linearities",
    "href": "source/Neural-Networks/0-Neural.html#non-linearities",
    "title": "Neural Networks",
    "section": "13.1 Non-Linearities",
    "text": "13.1 Non-Linearities\nDifferent non-linear functions can be used to generate the output of the neurons.\n\nSigmoid/Logistic Functions\nThis activation function is often used in the last layer of NNs for classification (since it scales the output between 0 and 1).\nf(x) = \\frac{1}{1+e^{-x}}\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nx = np.arange(-6, 6, 0.1)\nf = 1 / (1+math.e**(-x))\n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n\n\nText(0, 0.5, 'f(x)')\n\n\n\n\n\nPros:\n\nScales output between 0 and 1 (good for output layer in classification tasks)\nOutputs are bound between 0 and 1 \\rightarrow No explosion of activations\n\nCons:\n\nNo saturation / dying neuron / vanishing gradient: When f(x) = 0 or 1, the gradient of f(x) is 0. This blocks back-propagation (see here)\nOutput not centered around 0: All weight-updates during backpropagation are either positive or negative, leading to zig-zag SGD instead of direct descent to optimum (see here or here)\ncomputationally more expensive than ReLu\n\n\n\nTanh Functions\n f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nx = np.arange(-6, 6, 0.1)\nf = (math.e**x - math.e**(-x) ) / (math.e**(x)+math.e**(-x))\n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n\n\nText(0, 0.5, 'f(x)')\n\n\n\n\n\nPros:\n\nCentered around zero\n\nCons:\n\nsaturation / dying neuron / vanishing gradient problem\ncomputationally more expensive than ReLu\n\n\n\nRectifiers/ReLU\nf(x) = \\max(0,x)\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nx = np.arange(-6, 6, 0.1)\nf = [max(0,x_i) for x_i in x] \n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n\n\nText(0, 0.5, 'f(x)')\n\n\n\n\n\nPros:\n\nComputationally cheap\nNo saturation for positive values\n\nCons:\n\nNot zero centered\nSaturation for negative values\n\n\n\nLeaky ReLU\nf(x) = \\begin{cases}\n        \\alpha x \\text{ if } x &lt; 0 \\\\\n        x \\text{ if } x \\ge 0\n        \\end{cases}\n\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\na = 0.1\nx = np.arange(-6, 6, 0.1)\nf = [(a*x_i if x_i &lt; 0 else x_i) for x_i in x] \n\nsns.set(rc={'figure.figsize':(3,2)}, style=\"whitegrid\")\nsns.lineplot(x=x, y=f, )\nplt.xlabel('x')\nplt.ylabel('f(x)')\n\n\nText(0, 0.5, 'f(x)')\n\n\n\n\n\nPros:\n\nNo saturation problem\nfast to compute\nmore zero-centered than e.g. sigmoid-activation"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#terminology",
    "href": "source/Neural-Networks/0-Neural.html#terminology",
    "title": "Neural Networks",
    "section": "13.2 Terminology",
    "text": "13.2 Terminology\n\nInput layer/visible layer: Input variables\nHidden layer: Layers of nodes between input and output layer\nOutput layer: Layer of nodes that produce output variables\nSize: Number of nodes in the network\nWidth: Number of nodes in a layer\nDepth: Number of layers\nCapacity: The type of functions that can be learned by the network\nArchitecture: The arrangement of layers and nodes in the network"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#feedforward-neural-network-multi-layer-perceptron",
    "href": "source/Neural-Networks/0-Neural.html#feedforward-neural-network-multi-layer-perceptron",
    "title": "Neural Networks",
    "section": "13.3 Feedforward Neural Network / Multi-Layer Perceptron",
    "text": "13.3 Feedforward Neural Network / Multi-Layer Perceptron\nThis is the simplest type of proper neural networks. Each neuron of a layer is connected to each neuron of the next layer and there are no cycles. The outputs of the previous layer corresponds to the x in the activation function. Each output (x_i) of the previous layer gets it’s own weight (w_i) in each node and a bias (b) is added to each node. Neurons with a very high output are “active” neurons, those with negative outputs are “inactive”. The result is mapped to the probability range by (commonly) a sigmoid function. The output is then again given to the next layer.\nIf your input layer has 6400 features (80*80 image), a network with 2 hidden layers of 16 nodes will have 6400*16+16*16+16*10+16+16+10 = 102'858 parameters. This is a very high number of degrees of freedom and requires a lot of training samples.\n\nPyTorchKeras\n\n\nfrom torch import nn\n\n    class CustomNet(nn.Module):\n        def __init__(self):\n            super(CustomNet, self).__init__()\n            self.lin_layer_1 = nn.Linear(in_features=10, out_features=10)\n            self.relu = nn.ReLU()\n            self.lin_layer_2 = nn.Linear(in_features=10, out_features=10)\n\n        def forward(self, x):\n            x = self.lin_layer_1(x)\n            x = self.relu\n            x = self.lin_layer_2(x)\n            return x\n\n        def num_flat_features(self, x):\n            size = x.size()[1:] # Use all but the batch dimension\n            num = 1\n            for i in size:\n                num *= i\n            return num\n\n    new_net = CustomNet()\n\n\nExample of a small Keras model for text-classification.\nfrom keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 20 \nsequence_length = 50\nvocab_size = 5000 # length of word index / corpus\n\n# Specify model:\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=sequence_length))\nmodel.add(layers.SpatialDropout1D(0.1)) # Against overfitting\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n\n# Train model:\nhistory = model.fit(train_texts_padded, \n                    y_train,\n                    epochs=5,\n                    verbose=True,\n                    validation_data=(test_texts_padded, y_test),\n                    batch_size=100)\nloss, accuracy = model.evaluate(train_texts_padded, y_train)\nprint(\"Accuracy training: {:.3f}\".format(accuracy))\nloss, accuracy = model.evaluate(test_texts_padded, y_test)\nprint(\"Accuracy test:  {:.3f}\".format(accuracy))"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#backpropagation",
    "href": "source/Neural-Networks/0-Neural.html#backpropagation",
    "title": "Neural Networks",
    "section": "13.4 Backpropagation",
    "text": "13.4 Backpropagation\nThis is the method by which neural networks learn the optimal weights and biases of the nodes. The components are a cost function and a gradient descent method.\nThe cost function analyses the difference between the designated activation in the output layer (according to the label of the data) and the actual activation of that layer. Commonly a residual sum of squares is used.\nYou get the direction of the next best parameter-combination by using a stochastic gradient descent algorithm using the gradient for your cost function:\n\nWe use a “mini-batch” of samples for each round/step of the gradient descent.\nWe calculate squared residual of each feature of the output layer for each sample.\nFrom that we calculate what the bias or weights from the output layer and the activation from the last hidden layer must have been to get this result. We average that out for all images in our mini-batch.\nFrom that we calculate the weights, biases and activations of the upstream layers \\rightarrow we backpropagate."
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#initialization",
    "href": "source/Neural-Networks/0-Neural.html#initialization",
    "title": "Neural Networks",
    "section": "13.5 Initialization",
    "text": "13.5 Initialization\nThe weights of the nodes are commonly initialized randomly with a certain distribution. The biases are commonly initialized as zero, thus 0-centering of the input data is recommended."
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#convolutional-neural-networks",
    "href": "source/Neural-Networks/0-Neural.html#convolutional-neural-networks",
    "title": "Neural Networks",
    "section": "14.1 Convolutional Neural Networks",
    "text": "14.1 Convolutional Neural Networks"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#encoder-decoder-models",
    "href": "source/Neural-Networks/0-Neural.html#encoder-decoder-models",
    "title": "Neural Networks",
    "section": "14.2 Encoder-Decoder Models",
    "text": "14.2 Encoder-Decoder Models\n\nAutoencoder models\nContrary to the other architectures, autoencoders are used for unsupervised learning. Their goal is to compress and decompress data to learn the most important structures of the data. The layers therefore become smaller for the encoding step and the later layers get bigger again, up to the original representation of the data. The optimization problem is now: \\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2 with x_i being the original datapoint and \\hat{x}_i the reconstructed datapoint.\n\n\n\nModel of an autoencoder. The encoder layers compress the data towards the code layer, the decoder layers decompress the data again. Figure from Michela Massi on wikimedia.org.\n\n\n\n\nAutoencoders for clustering\nYou can look at layers of a NN as ways to represent data in different form of complexity and compactness. The code layers of autoencoders are a very compact way to represent the data. You can then use the compressed representation of the code layer and do clustering on that data. Because the code layer is however not optimized for that task. Song et al. combined the cost function of the autoencoder and k-means clustering: \\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda \\sum_{i=1}^N ||f(x_i) - c_i||^2 with f(x_i) being the non-linearity of the code layer and \\lambda is a weight constant.\nXXXX adapted spectral clustering (section 3.3) using autoencoders by replacing the (linear) eigen-decomposition with the (non-linear) decomposition by the encoder. As in spectral clustering the Laplacian matrix is used as the the input to the decomposition step (encoder) and the compressed representation (code-layer) is fed into k-means clustering.\nDeep subspace clustering by Pan et al. employs autoencoders combined with sparse subspace clustering. They used autoencoders and optimized for a compact representation of the code layer: \\begin{split}\n                \\min_{W,b} \\frac{1}{N}*\\sum_{i=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda ||V||_1 \\\\\n                \\text{s.t.} F(X) = F(X)*V \\text{ and diag}(V)=0\n            \\end{split} with V being the sparse representation of the code layer (F(X)) ."
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#generative-adversarial-networks",
    "href": "source/Neural-Networks/0-Neural.html#generative-adversarial-networks",
    "title": "Neural Networks",
    "section": "14.3 Generative adversarial networks",
    "text": "14.3 Generative adversarial networks"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#recurrent-neural-networks-rnn",
    "href": "source/Neural-Networks/0-Neural.html#recurrent-neural-networks-rnn",
    "title": "Neural Networks",
    "section": "14.4 Recurrent neural networks (RNN)",
    "text": "14.4 Recurrent neural networks (RNN)\nCompared to the fully connected or convolutional neural networks, RNNs can work on variable length inputs without padding the sequences.\n\n\n\nA unit of an RNN. The left side shows a static view of a unit h. The right side shows how the activations of past inputs (x_{t-1}) influence the output (o_t) of the current input (x_t). U, V and W are weights (Beware: They stay the same for different inputs). Figure from fdeloche on wikimedia.org.\n\n\nProblem: The influence of previous inputs vanishes (or potentially explodes) with the sequence length (unfolding). This makes the network hard to train. This is mitigated by LSTMs (below).\nmore explanation: youtube: StatQuest\n\nLong short-term memory (LSTM) networks\nInstead of just using the influence of previous inputs (“short-term memory”), LSTMs incorporate influence of more distance inputs (“long-term memory”). \\rightarrow There are two paths to influence the current state.\n\n\n\nA unfolded view of a unit of an LSTM. c_{t} (top horizontal path) is the cell state and represents the long-term memory. h_t ( bottom horizontal line) is the hidden state and represents the short-term memory. The weights are not shown in this diagram. F_t is the forget gate and determines the percentage of the long-term memory that is remembered based on the short-term memory and input. I_t is the input gate and determines how/whether to update/create the long-term memory using the input and short-term memory. O_t is the output gate and determines the short-term memory to be passed on to the next time-step. Thus the output of the cell is the modified long- and short-term memory. Figure from fdeloche on wikimedia.org.\n\n\nmore explanation: youtube: StatQuest"
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#transformer-models",
    "href": "source/Neural-Networks/0-Neural.html#transformer-models",
    "title": "Neural Networks",
    "section": "14.5 Transformer models",
    "text": "14.5 Transformer models\nTransformers are encoder-decoder models with the following setup:\n\nEncoder: Containts one attention unit, the multi-head attention. It learns the relationships between elements in the input sequence.\nDecoder: Contains two attention units:\n\nThe masked multi-head attention: It learns relationship of the current element and previous words in the output sequence.\nThe multi-head attention: It learns the relationship of the (current and previous) elements in the output sequence and the learned representation of the input from the encoder.\n\n\nLike LSTMs, transformers use attention mechanisms to learn relationships between input elements. There are key differences, however:\n\nThe sequences are passed in simultaneously (not sequentially as in RNNs). Many computations can be done in parallel.\nPositional encoding: Since the elements are fed-in in parallel, the position of the elements within the sequence is encoded.\nMasking: Since elements are fed-in in parallel, the elements that the model shall predict, are masked (overwritten with zeros) from the output-sequence during training.\nSelf-attention: The significance of an element (e.g. word) is learned by specific elements around it. Contrary to RNNs, the dependencies don’t grow/shrink linearly with t, but are independent of the distance.\n\n\n\n\nArchitecture of transformer models. The encoder is on the left. The decoder is on the right. The encoder-outputs are passed to it in sequence. Output-sequence elements that come after the current input element are masked. The decoder generates the next element of the output. There are commonly multiple encoder and decoder units in sequence and multiple attention units in parallel. Figure from Yuening Jia on wikimedia.org.\n\n\nDetailed explanation: Youtube - CodeEmporium\n\nLarge Language Models (LLMs)\nLLMs are transformer models, that have been trained on a huge amount of unlabeled data.\nPros:\n\nThey are very performant\nThey are easily adaptable for other use cases.\n\nCons:\n\nThe training data of the models is often unknown or insufficiently sanitized (containing false information, hate speech, outdated info, etc. )\nThe models are so huge, that it takes a lot of compute power just to use it for predictions.\n\nTo deal with these problems, You can tune late layers in pre-trained LLMs to increase their accuracy for your field of application without having to train a huge model."
  },
  {
    "objectID": "source/Neural-Networks/0-Neural.html#transfer-learning",
    "href": "source/Neural-Networks/0-Neural.html#transfer-learning",
    "title": "Neural Networks",
    "section": "15.1 Transfer learning",
    "text": "15.1 Transfer learning\nYou train a model on one (usually large) dataset and adapt it for a related learning task. The feature space and distribution of the input and target data of the new task can be different from the old task.\n\nDomain adaptation\nIn domain adaptation the feature space is similar to the original dataset."
  },
  {
    "objectID": "source/Other/0-ML_PM.html",
    "href": "source/Other/0-ML_PM.html",
    "title": "13  ML Project Management",
    "section": "",
    "text": "More info: Google Cloud: How Google Does Machine Learning"
  },
  {
    "objectID": "source/Other/0-ML_PM.html#the-stages-from-manual-to-ml",
    "href": "source/Other/0-ML_PM.html#the-stages-from-manual-to-ml",
    "title": "13  ML Project Management",
    "section": "15.1 The stages from ManuaL to ML",
    "text": "15.1 The stages from ManuaL to ML\nA good business process entails a feedback loop from the client (receiver of the output) to further optimize the process:\nInput \\rightarrow Process \\rightarrow Output \\rightarrow Feedback \\rightarrow Optimization \\rightarrow new Process \\rightarrow …\nBusiness processes evolve along these phases, whose steps should not be skipped:\n\nIndividual employee works on tasks, commonly informal rules and heuristics are used.\nTeam works on the tasks. The process is formalized and standardized to ensure quality and effective collaboration. Don’t stay here for too long, since it is not scaleable.\nDigitization is used to automate (parts of) the process. This step should be done before ML, since you need the data and architecture for your ML part anyway. Here you are more flexible and can fail and adapt quicker. Don’t stay here to long, since you cannot assess quaility of your process well.\nAnalytics are used to measure the performance of the process and if its optimizations are successful. Don’t skip this since you need these indicators for your ML optimizationa and monitoring anyway. Don’t stay here too long to miss out on automation and scalability.\nMachine Learning is used to automate and optimize analysis, insights and decision making on the data. You still need some people from step 2 to analyse outcomes, failures and react to it (monitoring).\n\nMore info: Google Cloud: How Google Does Machine Learning"
  },
  {
    "objectID": "source/Other/0-ML_PM.html#phases-of-the-ml-project",
    "href": "source/Other/0-ML_PM.html#phases-of-the-ml-project",
    "title": "13  ML Project Management",
    "section": "15.2 Phases of the ML project",
    "text": "15.2 Phases of the ML project\n\nFraming the problem\nData collection & management\nBuilding infrastructure (data pipeline, databases, training & deployment pipelines should at least work the same way as the designated product unless its a quick’n’dirty PoC)\nData ingestion, transformation & feature engineering\nModel selection, training, testing & evaluation\nDeployment & integration\nMonitoring\n\nMore info: Google Cloud: How Google Does Machine Learning"
  },
  {
    "objectID": "source/Other/0-ML_PM.html#how-to-frame-ml-problems",
    "href": "source/Other/0-ML_PM.html#how-to-frame-ml-problems",
    "title": "13  ML Project Management",
    "section": "15.3 How to frame ML problems",
    "text": "15.3 How to frame ML problems\n\nThe ML-view:\n\nWhat is being predicted?\nWhat data do we need as target and input?\n\nSoftware development view:\n\nWhat info do we need from users to make a decision? (This defines the API)\nWho will use the service? How many people will that be?\nHow is the process conducted today?\n\nData view:\n\nWhat data needs to be collected? From where?\nHow do we need to transform the data to analyze it & make decisions on it? (Feature engineering)\nHow do we react to the outputs of the algorithm? (e.g. kick off automatic process, inform stakeholders…)\n\n\nMore info: Google Cloud: How Google Does Machine Learning"
  },
  {
    "objectID": "source/Other/0-Checklists.html",
    "href": "source/Other/0-Checklists.html",
    "title": "14  Checklists",
    "section": "",
    "text": "15 Tips for machine learning projects\nSelected advice from paper from Guyon and Elisseeff:\nOther advice:"
  },
  {
    "objectID": "source/Other/0-Checklists.html#general-advice",
    "href": "source/Other/0-Checklists.html#general-advice",
    "title": "14  Checklists",
    "section": "15.1 General advice",
    "text": "15.1 General advice\nGeneral advice for machine learning from Pedro Domingos:\n\nLet your knowledge about the problem help you choose the candidate algorithms. E.g. You know the rules on which comparing samples makes most sense \\rightarrow Choose instance based learners. If you know that statistical dependencies are relevant \\rightarrow choose Graph based models.\nDon’t underestimate the impact of feature engineering: Many domain specific features can boost the accuracy.\nGet more samples and candidate features (instead of focussing on the algorithm)\nDon’t confuse correlation with causation. Just because your model can predict something, it does not mean that the features cause the target and you thus cannot easily deduct a clear action from it."
  },
  {
    "objectID": "source/Other/0-Checklists.html#common-mistakes",
    "href": "source/Other/0-Checklists.html#common-mistakes",
    "title": "14  Checklists",
    "section": "15.2 Common mistakes",
    "text": "15.2 Common mistakes\nBe aware: This list will never capture everything that can go wrong. ;-)\n\nData Leakage: Information from Samples in your test data have leaked into your training data.\n\nYou have not deleted duplicates beforehand\nYou falsely assumed that your samples where drawn independently and have sampled the training set randomly. (E.g. multiple samples from the same patient, time series data)\nYou have the class label encoded in the training features in a way that you will not find in “Nature”.\nYou just used the wrong training / test set while programming.\nYou did feature engineering like finding n-grams or Max, Min of data using your test-set data.\nRemedy: Careful preliminary data analysis, deduplication,\n\nUsing the wrong quality measures on unbalanced data: E.g. Accuracy on unbalanced data is not a reasonable quality measure.\nInconsistent preprocessing: If you preprocess your training data in a certain way, you have to do the same with the test- and prediction-data.\n\nRemedy: Use one preprocessing pipeline that you can use for training, testing and prediction.\n\nCurse of dimensionality:\n\nYou use too many features for the amount of samples that you have\nYour distance measure is not suitable for high-dimensional space (e.g. Hamming distance, Euclidean distance)\nRemedy: Use lower-dimensional mapping, feature selection.\n\nOverfitting:\n\nYou use a too complex algorithm (too many degrees of freedom) for the amount of data you have\nYou have too many features\nRemedy: Get more samples, reduce the dimensionanlity, feature selection, regularization, bagging, boosting, stacking.\n\nBad Data:\n\nYour data is not representative of what you would find in the “real world”. (skewed population, too old data, only of specific sensors, locations…)\nYour have many missing values among your features.\nThe data that you have is only remotely linked to the target that you want to predict.\nThere are erroneous entries in your data.\nRemedy: Clean data at source, impute data, clean data during preprocessing, get more representative data, limit scope of application."
  },
  {
    "objectID": "source/tutorials/creating_grouning_db.html#intro",
    "href": "source/tutorials/creating_grouning_db.html#intro",
    "title": "15  Exploring German LLM Expert Bots",
    "section": "Intro",
    "text": "Intro\n\nWhat we need:\n\nA large language model that can answer questions. -&gt; Question\nAnswering LLM (QA-LLM)\nA large language model that can map text to vectors. -&gt;\nVector-LLM\nA database to store the vectors. -&gt; Vector-DB\n\nWe will be using the framework Langchain for splitting texts,\ngrounding and connecting to models and sources. Langchain provides us\nwith a couple of benefits over specific APIs from different\nproviders:\n\nStandardised interface for different document-loaders, LLMs and\nvector-DBs\nEasy integration of different LLMs into different vector-DBs\nThe high level API avoids boilerplate code.\n\nHere is a good intro to the topic: langchain:\nRetrieval.\n\n\nHow to do it:\n\nCreate a grounding database for your QA-LLM.\n\nGather the information you want your QA-llm to answer questions\nabout.\nCut the data into snippets that are small enough to be processed by\nyour QA-llm.\nMap each snippet to a vector using your Vector-llm. The vector\nrepresents the meaning/topic of the snippet.\nStore the vectors in a database.\n\nAnswer questions using the grounding database.\n\nGiven a question, map it to a vector using your Vector-llm. Then\nsearch the database for the snippets with the most similar vectors.\nPrepend the snippet to your question.\nFeed the augmented question to your QA-llm.\n\n\n\n\n\n\ngraph TB\n  subgraph SB[\"build grounding database\"]\n  A(Data Sources) --&gt;|Load| B(Text-Files)\n  B --&gt;|Chunk| C(Snippets)\n  C --&gt;|\"vector-map (Vector-LLM)\"| D[(Vector DB)]\n\n  end\n\n  subgraph SU[\"use grounding database\"]\n  D --&gt;|retrieve| E(Relevant snippets)\n  E --&gt;|insert| F(Augmented Prompt)\n  F --&gt;|\"query (QA-LLM)\"| G(Result)\n  end\n\n  style SB fill:#F2F2F2\n  style SU fill:#F2F2F2\n  linkStyle 0,1,2,3,4,5 stroke:#BFBFBF \n\n\n\n\n\n\n\nShow code: Base libraries to import\n# Importing libraries\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport random\nimport textwrap\n\n\n\n\nShow code: Print results more nicely\ndef nice_print(text):\n    print(textwrap.fill(text, 120))"
  },
  {
    "objectID": "source/tutorials/creating_grouning_db.html#creating-a-grounding-database",
    "href": "source/tutorials/creating_grouning_db.html#creating-a-grounding-database",
    "title": "15  Exploring German LLM Expert Bots",
    "section": "1. Creating a grounding database",
    "text": "1. Creating a grounding database\n\n1.1. Gather the information\nIn this step we will load the data that we want our model to answer\nquestions about. We will use the data from the public, German Helsana\nwebsite (largest health insurance in Switzerland).\n\nfrom urllib.parse import urlparse\n\n\n# Required functions for loading the data:\ndef download_webpage(url):\n    response = urllib.request.urlopen(url)\n    webContent = response.read().decode(\"UTF-8\")\n\n    file_path = \"data/html/\" + get_page_name(url)\n    f = open(file_path, \"w\")\n    f.write(webContent)\n    f.close\n\ndef get_page_name(url):\n    parsed_url = urlparse(url)\n    page_name = parsed_url.path.split(\"/\")[-1]\n    return page_name\n\n\n\nShow code: Web-pages to crawl\n# List of web-pages where we will find the information for our knowledge base\nurls = [\n    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/basis.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/benefit-plus-hausarzt.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/benefit-plus-telmed.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/benefit-plus-flexmed.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/premed-24.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/grundversicherung/uebersicht-grundversicherungen.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/spezialversicherungen.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/leistungsuebersicht.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/top.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/sana.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/completa.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/world.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/ambulant/primeo.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-eco.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-halbprivat.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-privat.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/spitalversicherung/hospital-flex.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/zahnversicherung.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/weitere/advocare-plus.html\",\n    \"https://www.helsana.ch/de/private/versicherungen/zusatzversicherungen/weitere/advocare-extra.html\",\n]\n\n\n\n# Downloading \nfor url in urls:\n    download_webpage(url)\n\n\n\n1.2. Cut the data into snippets\nNow we need to cut the contents from the page into snippets that are\nsmall enough to be processed by our QA-llm but large enough to contain\nthe relevant context.\n\n1.2.1. Minimalist approach (not used)\nThis is probably the simplest approach to cut the webpages into\nsnippets.\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom bs4 import BeautifulSoup\n\n\nwebsite_texts = []\nfor html_document_path in os.listdir(\"data/html\"):\n    soup = BeautifulSoup(\n        open(\"./data/html/\" + html_document_path), features=\"html.parser\"\n    )\n\n    website_text = soup.get_text()\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=800,\n        chunk_overlap=50,\n        length_function=len,\n        add_start_index=True,\n    )\n    page_texts = text_splitter.create_documents([website_text])\n    website_texts = website_texts + page_texts\n\nRegrettably, the vector-LLM had a hard time to reasonably map the\nsnippets to meaningful vectors and the QA-LLM was not able to answer\nquestions based on the snippets. The reasons were probably, that the\nsnippets started in the middle of paragraphs and titles from the\nparagraphs were missing. The QA-LLM then used snippets to answer\nquestions from wrong topics.\nTherefore, I did not use this approach, but used approach\n1.2.2. instead.\n\n\n1.2.2. Subsection based splitting (used)\nTo embed the meaning and context clearer, we prepend the title and\nsubtile of the relevant snippet to each text.\n\n# This algorithm might be stupid but it works. :-D\ndef parse_website_texts(soup):\n    \"\"\"Cut website texts into snippets. \n\n    Args:\n        soup (BeautifulSoup.soup): Soup object from the Beautiful soup webscraper. \n         It contains the different elements of the html-code of the website.\n\n    Returns:\n        : list(str): List of strings. \n         Each strings contains the title, subtitle and text of a snippet.\n    \"\"\"\n    level_dict = {\n        \"h1\": 0,\n        \"h2\": 1,\n        \"h3\": 2,\n        \"p\": 3,\n        \"li\": 3,\n    }  # hierarchical level from html tags\n    element_list = soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"])\n    prev_level = 9999\n    webpage_snippet_list = []\n    snippet_texts_list = []\n    # Pattern of each snippet: h1-title, h2-subtitle, h3-subsubtitle, paragraph-text:\n    base_element_list = [\"\",\"\", \"\", \"\"]  \n    for element in element_list:\n        current_level = level_dict[element.name]\n\n        if current_level &lt; prev_level:  # i.e. we are at a new topic\n            # save the previous snippet as one string\n            webpage_string_snip = \" \".join(snippet_texts_list)\n            webpage_string_snip = webpage_string_snip.replace(\"\\n\", \" \")\n            if webpage_string_snip != \"\":\n                webpage_snippet_list.append(webpage_string_snip)\n            # update base_element_list\n            base_element_list[prev_level:current_level] = \"\"\n            base_element_list[current_level] = element.text\n            snippet_texts_list = base_element_list\n\n        # update snippet_texts_list\n        else:\n            snippet_texts_list = snippet_texts_list + [element.text]\n        prev_level = current_level\n    return webpage_snippet_list\n\n\nfrom bs4 import BeautifulSoup\n\nwebsite_texts = []\nfor html_document_path in os.listdir(\"data/html\"):\n    soup = BeautifulSoup(\n        open(\"./data/html/\" + html_document_path), features=\"html.parser\"\n    )\n    website_texts_page = parse_website_texts(soup)\n\n    website_texts = website_texts + website_texts_page\n\nHere is an example of the snippets (heading + subheadings +\nparagraphs):\n\nfor text in random.sample(website_texts, 5):\n    print(text)\n\nHelsana Advocare PLUS Häufig gestellte Fragen    Wer kann diese Versicherung abschliessen?    Sie können die Versicherung abschliessen, wenn Sie folgende Voraussetzungen erfüllen: Sie leben in der Schweiz (offizieller Wohnsitz). Sie haben bereits eine der Zusatzversicherungen TOP, OMNIA oder COMPLETA oder beantragen diese zeitgleich mit Helsana Advocare PLUS.\nSANA Weitere Zusatzversicherungen TOP  Ihr Zusatz zur Grundversicherung: Wichtige ambulante Leistungen sind gedeckt.\nBeneFit PLUS Telmed    Bei gesundheitlichen Problemen rufen Sie immer zuerst das unabhängige Zentrum für Telemedizin an: 0800 800 090. Sie erhalten rund um die Uhr medizinische Unterstützung und einen attraktiven Prämienrabatt.    24/7 kostenlose, verbindliche medizinische Telefonberatung     Digitale Services, wie z. B. Symptom-Checker und Videokonsultation     Attraktiver Prämienrabatt  \nBeneFit PLUS Telmed  Prämie berechnen  Ihre Prämie CHF 0 CHF 500 CHF 300 CHF 500 CHF 1000 CHF 1500 CHF 2000 CHF 2500 eingeschlossen ausgeschlossen\nBeneFit PLUS Flexmed Weitere Modelle der Grundversicherung BeneFit PLUS Hausarzt  Der Hausarzt oder die HMO-Gruppenpraxis ist Ihre erste Anlaufstelle.\n\n\n\n\n\n1.3. Map each snippet to a vector\nHere we use another large language model to map each snippet to a\nvector. This vector represents the meaning/topic of the snippet. I used\nthe FastEmbed model for practical reasons. However, I would recommend to\nuse a more powerful model like this one: https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n\nfrom langchain.embeddings.fastembed import FastEmbedEmbeddings\n\n\n# Choosing a suitable embedding model can make a big difference in retrieval performance.\n# For simplicity, we use the FastEmbedEmbeddings model off-the-shelf.\nembedder = FastEmbedEmbeddings() \n\n# Just to show a vector, we embed the first snippet:\nembeddings = embedder.embed_documents(website_texts[0])\nprint(embeddings[0][0:5])  # The first 5 dimensions of the vector\n\n[-0.013905464671552181, 0.038332026451826096, 0.01669456996023655, 0.010435071773827076, -0.01078716292977333]\n\n\n\n\n1.4. Store embeddings in a vector database\nWe could just keep the embeddings in memory or store them in a file.\nIt is however more efficient to store them in a database. I used the\nopen-source chroma database for this purpose.\n\nfrom langchain.vectorstores import Chroma\n\n\nchroma_db = Chroma.from_texts(website_texts, embedder)"
  },
  {
    "objectID": "source/tutorials/creating_grouning_db.html#answering-questions-using-the-grounding-database",
    "href": "source/tutorials/creating_grouning_db.html#answering-questions-using-the-grounding-database",
    "title": "15  Exploring German LLM Expert Bots",
    "section": "2. Answering questions using the grounding database",
    "text": "2. Answering questions using the grounding database\n\n2.1. The manual approach (not recommended, just for\nunderstanding)\n\n2.1.1. Given a question, map it to a vector and search the\ndatabase\nNow we will use the Vector-llm to map the question to a vector. Then\nwe will search the database for the snippets with the most similar\nvectors. These contain the information that we need to answer the\nquestion.\nFirst we need a question. Here we ask whether the additional\ninsurance model “Completa” of the Helsana health insurance cover the\ncosts of glasses.\n\nquestion = \"Sind Kosten für meine Brille von Completa gedeckt?\"\n\nNow we task the chroma-db to find snippets, which are similar to the\ntopic of the question:\n\nn_neighbors = 5\nsimilar_docs = chroma_db.similarity_search(question, k=n_neighbors)\n\nfor doc in similar_docs:\n    nice_print(doc.page_content)\n\nCOMPLETA Weitere Zusatzversicherungen COMPLETA PLUS  COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA.\nCOMPLETA Häufig gestellte Fragen    Wann lohnt sich ein Upgrade zu COMPLETA PLUS?     COMPLETA PLUS erweitert den\nDeckungsumfang von COMPLETA. Die Zusatzversicherung lohnt sich, wenn Ihnen Leistungen rund um Gesundheitsförderung\nwichtig sind. Sie erhalten unter anderem zusätzliche Kostenbeiträge für Gesundheitsförderung, Komplementärmedizin,\nPrävention sowie für Brillen, Kontaktlinsen und Augenlaserkorrekturen.\nCOMPLETA Häufig gestellte Fragen    Lohnt sich ein Upgrade zu COMPLETA, wenn ich bereits TOP oder SANA habe?    Wenn Sie\ngerne möglichst breit abgesichert sind, dann lohnt sich COMPLETA für Sie. Damit schliessen Sie die meisten\nDeckungslücken der Grundversicherung. COMPLETA vereint die Vorteile von TOP und SANA. Besser noch: Viele Vergütungen\nsind noch grosszügiger, beispielsweise für medizinische Hilfsmittel oder Präventionsmassnahmen wie Check-ups. Und für\nBrillen und Kontaktlinsen erhalten Sie gar doppelt so viel Geld erstattet wie mit TOP. Zudem werden Behandlungen im\nAusland sowie durch Nichtvertragsärzte unterstützt.\nCOMPLETA Häufig gestellte Fragen    Wer kann COMPLETA abschliessen?    Sie können die Versicherung abschliessen, wenn\nSie in der Schweiz wohnhaft sind (offizieller Wohnsitz) und über eine Gesundheitsdeklaration mit positivem\nAufnahmebescheid verfügen.\nCOMPLETA Versicherte Leistungen Prämie berechnen  Nachfolgende Leistungen erstatten wir Ihnen ergänzend zu den\ngesetzlichen Leistungen der Grundversicherung aus der Zusatzversicherung COMPLETA zurück:\n\n\n\n\n2.1.2. Prepend the relevant snippets to your question\nTo give our QA-model the information it needs to answer the question,\nwe prepend the relevant snippets to the question. Additionally we add\nsome instructions to the model to make sure it understands what we want\nfrom it.\n\ndocs_in_prompt = \"\"\nfor id, doc in enumerate(similar_docs):\n    docs_in_prompt += str(id + 1) + \": \" + doc.page_content + \"\\n\"\ncontext_instruction = \"\"\"Du bist ein hilfreicher Assistent. Benutze die Informationen der Helsana Gesundheitsversicherung um die darauf folgende Frage zu beantworten.\"\"\"\naugmented_prompt = f\"{context_instruction} \\nDie Informationen: {docs_in_prompt}Frage: {question} Assistant:\"\nprint(augmented_prompt)\n\nDu bist ein hilfreicher Assistent. Benutze die Informationen der Helsana Gesundheitsversicherung um die darauf folgende Frage zu beantworten. \nDie Informationen: 1: COMPLETA Weitere Zusatzversicherungen COMPLETA PLUS  COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. \n2: COMPLETA Weitere Zusatzversicherungen COMPLETA PLUS  COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. \n3: COMPLETA Häufig gestellte Fragen    Wann lohnt sich ein Upgrade zu COMPLETA PLUS?     COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. Die Zusatzversicherung lohnt sich, wenn Ihnen Leistungen rund um Gesundheitsförderung wichtig sind. Sie erhalten unter anderem zusätzliche Kostenbeiträge für Gesundheitsförderung, Komplementärmedizin, Prävention sowie für Brillen, Kontaktlinsen und Augenlaserkorrekturen.\n4: COMPLETA Häufig gestellte Fragen    Wann lohnt sich ein Upgrade zu COMPLETA PLUS?     COMPLETA PLUS erweitert den Deckungsumfang von COMPLETA. Die Zusatzversicherung lohnt sich, wenn Ihnen Leistungen rund um Gesundheitsförderung wichtig sind. Sie erhalten unter anderem zusätzliche Kostenbeiträge für Gesundheitsförderung, Komplementärmedizin, Prävention sowie für Brillen, Kontaktlinsen und Augenlaserkorrekturen.\n5: COMPLETA Häufig gestellte Fragen    Lohnt sich ein Upgrade zu COMPLETA, wenn ich bereits TOP oder SANA habe?    Wenn Sie gerne möglichst breit abgesichert sind, dann lohnt sich COMPLETA für Sie. Damit schliessen Sie die meisten Deckungslücken der Grundversicherung. COMPLETA vereint die Vorteile von TOP und SANA. Besser noch: Viele Vergütungen sind noch grosszügiger, beispielsweise für medizinische Hilfsmittel oder Präventionsmassnahmen wie Check-ups. Und für Brillen und Kontaktlinsen erhalten Sie gar doppelt so viel Geld erstattet wie mit TOP. Zudem werden Behandlungen im Ausland sowie durch Nichtvertragsärzte unterstützt.\nFrage: Sind Kosten für meine Brille von Completa gedeckt? Assistant:\n\n\nNow we could use this prompt and send it to out QA-llm. But there is\na better automated way:\n\n\n\n2.2. The automated approach (recommended)\nNow we will try out different QA-llms to answer the question and see\nwhich model performs the best.\nWe will try:\n\nOpenAI ChatGPT-3.5\nLlama2 70B\nMistral 7B and Mistral 7B tuned on German texts\n\n\nOpenAI GPT-3.5\nCurrently one of the strongest but also biggest models. We can only\nrun it via an API-request to the OpenAI-Server. You can find advice for\nprompting the model here: Promptingguide.ai:\nchatgpt\n\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\n\nTo use the OpenAI API, you need to register (OpenAI API) and save the\nAPI-key as an environment variable:\n\nopneai_llm = OpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n\nSince langchain lets us swap out models easily, we write one\nquery-function for all our models:\n\ndef query_llm(model, prompt_template):\n    qa_chain = RetrievalQA.from_chain_type(\n        model,\n        retriever=chroma_db.as_retriever(\n            search_type=\"mmr\", search_kwargs={\"k\": 6, \"lambda_mult\": 0.25}\n        ),\n        chain_type_kwargs={\"prompt\": prompt_template},\n    )\n    response = qa_chain({\"query\": question})\n    nice_print(response[\"result\"])\n\nNow we can write a prompt-template where we can plug-in our question\nand context. As with the manual approach, the QA-model gets our context\nand instructions to answer our question:\n\nopenai_prompt_template = PromptTemplate.from_template(\n    \"\"\"Du bist ein hilfreicher Assistent der Fragen beantwortet. Benutze die folgenden Stücke von Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du es nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten.  \n    ### Frage: {question} \n    ### Context: {context} \n    ### Antwort: \"\"\"\n)\n\nquery_llm(opneai_llm, openai_prompt_template)\n\n Ja, Kosten für eine Brille von Completa sind gedeckt, sofern die Angaben über die Brillenstärke auf der Rechnung\nausgewiesen sind.\n\n\nThe answer looks pretty correct to me. Only missing is the\ninformation that the insurance covers the costs up to 300 CHF per\nannum.\n\n\nLLama-2 70B\nThis is a large model but still smaller than GPT-3.5. It is still too\nbig to run it on our machine. Therefore we will run it in the cloud (in\nour case on an Nvidia A100 GPU on replicate.com). You can find more\ninfo on LLama here: Hugging\nFace: Llama2\n\nllama_llm = Replicate(\n    model=\"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n    model_kwargs={\"temperature\": 0.75},\n)\n\nDifferent models profit from different prompt formats. Thus, we\ncreate a different template here:\n\nllama_prompt_template = PromptTemplate.from_template(\n    \"\"\"[INST] Du bist ein hilfreicher Assistent. Benutze den folgenden Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du die Antwort nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten. \n    Frage: {question} \n    Context: {context} \n    Antwort:[/INST]\"\"\"\n)\n\nquery_llm(llama_llm, llama_prompt_template)\n\n Ja, die Kosten für Ihre Brille sind von Completa gedeckt. According to the information provided, you will receive 90%\nof the costs up to a maximum of 150 francs per calendar year for your glasses or contact lenses. However, it is\nimportant to note that there may be a deductible and coinsurance applicable to the coverage. Additionally, if you have\nalready exhausted your annual limit for glasses or contact lenses under your TOP policy, you may not be able to claim\nthe full amount under Completa. It's always best to check\n\n\nThe answer is correct. However it suddenly starts writing in English.\nLet’s try to fix this by using few shot-learning: We will provide it a\nfew examples of questions and answers in German.\n\nllama_prompt_template = PromptTemplate.from_template(\n    \"\"\"[INST] Du bist ein hilfreicher Assistent. Benutze den folgenden Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du die Antwort nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten. Antworte ausschliesslich auf Deutsch. Beispiel \n    Frage: Sind Notfälle im Ausland vom Modell BeneFit PLUS abgedeckt? \n    Context: Bei einem Notfall erhalten Sie in den EU/EFTA-Staaten den jeweiligen Sozialtarif. In allen übrigen Ländern erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. \n    Antwort:[/INST] Ja, Notfälle im Ausland sind abgedeckt. In nicht-EU/EFTA-Staaten erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. [INST]\n    Frage: Ich bin Hospital Privat versichert. Ich hatte einen Unfall und war auf eine Haushaltshilfe angewiesen. Erstattet mir die Helsana die Kosten zurück? \n    Context: Kann ein akuter stationärer Spital- oder Kuraufenthalt durch eine ärztlich verordnete Haushaltshilfe verhindert oder zumindest verkürzt werden, erhalten Sie bis max. 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. \n    Antwort: [/INST] Ja, die Helsana erstattet Ihnen bis zu 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr.[INST] \n    Frage: {question} \n    Context: {context} \n    Antwort: [/INST]\"\"\"\n)\n\nquery_llm(llama_llm, llama_prompt_template)\n\n Ja, die Kosten für Ihre Brille sind von Completa gedeckt. Sie erhalten 90% der Kosten bis max. 150 Franken pro\nKalenderjahr für Ihre Brillengläser und Kontaktlinsen. Wenn Sie jedoch eine Upgrade zu COMPLETA PLUS erwogen, lohnt sich\ndies, da Viele Vergütungen noch grosszügiger sind, wie zum Beispiel für medizinische Hilfsmittel oder\nPräventionsmassnahmen wie Check-ups. Und für Brillen und Kontaktlinsen erhalten Sie\n\n\nThis looks already much better. For this specific question, the model\ngives a correct answer with all the relevant context. There is just one\nwrong word in the answer (erwogen). Let’s go one step smaller.\n\n\nMistral 7B\nThis is a much smaller model than GPT-3.5 but still pretty strong. We\ncould run this model on a standard notebook (16GB RAM required), but it\nis faster to run it on GPUs in the cloud (in our case on a Nvidia A40\nGPU on replicate.com). Using GPUs generated a speedup of up to 50x in\nour case. More info here: Mistral: Mistral\n7B\n\nfrom langchain.chains import LLMChain\nfrom langchain.llms import Replicate\nfrom langchain.prompts import PromptTemplate\n\n\nmistral_llm = Replicate(\n    model=\"mistralai/mistral-7b-instruct-v0.1:83b6a56e7c828e667f21fd596c338fd4f0039b46bcfa18d973e8e70e455fda70\",\n    model_kwargs={\"temperature\": 0.7, \"max_length\": 500},\n)\n\nLet’s use the few-shot learning approach directly:\n\nmistral_prompt_template = PromptTemplate.from_template(\n    \"\"\"&lt;s&gt;[INST] Du bist ein hilfreicher Assistent. Benutze den folgenden Context zur Helsana um die darauf folgende Frage zu beantworten. Wenn du die Antwort nicht findest, schreibe, dass du die Antwort nicht weisst. Benutze maximal drei Sätze und sei präzise beim Antworten. Beispiel \n    Frage: Sind Notfälle im Ausland vom Modell BeneFit PLUS abgedeckt? \n    Context: Bei einem Notfall erhalten Sie in den EU/EFTA-Staaten den jeweiligen Sozialtarif. In allen übrigen Ländern erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. \n    Antwort:[/INST] Ja, Notfälle im Ausland sind abgedeckt. In nicht-EU/EFTA-Staaten erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. [INST]\n    Frage: Ich bin Hospital Privat versichert. Ich hatte einen Unfall und war auf eine Haushaltshilfe angewiesen. Erstattet mir die Helsana die Kosten zurück? \n    Context: Kann ein akuter stationärer Spital- oder Kuraufenthalt durch eine ärztlich verordnete Haushaltshilfe verhindert oder zumindest verkürzt werden, erhalten Sie bis max. 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. \n    Antwort: [/INST] Ja, die Helsana erstattet Ihnen bis zu 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. &lt;/s&gt; [INST] \n    Frage: {question} \n    Context: {context} \n    Antwort: [/INST]\"\"\"\n)\n\nquery_llm(mistral_llm, mistral_prompt_template)\n\nJa, Brillen und Kontaktlinsen von Completa sind von COMPLETA gedeckt. Für Brillengläser und Kontaktlinsen erhalten Sie\nbis zu 90% der Kosten bis zu 150 Franken pro Jahr. Für Kinder und Jugendliche bis 18 Jahre erhalten Sie 180 Franken pro\nJahr.\n\n\nThis answer is already better but still a bit misleading. (And I have\nto admit that I generated a few more incomplete answers.) Let us try to\nuse a model that is tuned on German texts.\n\n\nGerman Mistral 7B\nLet us use a version of Mistral that has been tuned for German texts.\nMaybe it will perform better. More info on the model here: Hugging Face:\nem_german_leo_mistral\n\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\n\n# This time we will run it on our own device:\ngerman_mistral_llm = Ollama(\n    model=\"germanleo\",\n    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n)\n\n\ngerman_mistral_prompt_template = PromptTemplate.from_template(\n    \"\"\"Du bist ein hilfreicher Assistent. Für die folgende Aufgabe stehen dir zwischen den tags BEGININPUT und ENDINPUT mehrere Quellen zur Verfügung. Die eigentliche Aufgabe oder Frage ist zwischen BEGININSTRUCTION und ENDINCSTRUCTION zu finden. Beantworte diese wortwörtlich mit einem Zitat aus den Quellen. Sollten diese keine Antwort enthalten, antworte, dass auf Basis der gegebenen Informationen keine Antwort möglich ist!\n    BEGININSTRUCTION Sind Notfälle im Ausland vom Modell BeneFit PLUS abgedeckt? ENDINCSTRUCTION BEGININPUT Bei einem Notfall erhalten Sie in den EU/EFTA-Staaten den jeweiligen Sozialtarif. In allen übrigen Ländern erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages. ENDINPUT \n    ASSISTANT: \"Ja, Notfälle im Ausland sind abgedeckt. In nicht-EU/EFTA-Staaten erstatten wir maximal das Doppelte des in der Schweiz versicherten Betrages.\"\n    BEGININSTRUCTION Ich bin Hospital Privat versichert. Ich hatte einen Unfall und war auf eine Haushaltshilfe angewiesen. Erstattet mir die Helsana die Kosten zurück? ENDINCSTRUCTION BEGININPUT Kann ein akuter stationärer Spital- oder Kuraufenthalt durch eine ärztlich verordnete Haushaltshilfe verhindert oder zumindest verkürzt werden, erhalten Sie bis max. 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. ENDINPUT \n    ASSISTANT: \"Ja, die Helsana erstattet Ihnen bis zu 200 Franken pro Tag während max. 30 Tagen pro Kalenderjahr. Voraussetzung ist eine ärztliche Verordnung.\" \n    BEGININSTRUCTION {question} ENDINCSTRUCTION\n    BEGININPUT {context} ENDINPUT \n    ASSISTANT: \"\"\"\n)\n\nquery_llm(german_mistral_llm, german_mistral_prompt_template)\n\n \"Ja, die Kosten für Ihre Brille von Completa sind gedeckt. Kompleta PLUS erweitert den Deckungsumfang und lohnt sich insbesondere dann, wenn Sie gerne möglichst breit abgesichert sind.\" \"Ja, die Kosten für Ihre Brille von Completa sind gedeckt. Kompleta PLUS erweitert den Deckungsumfang und lohnt sich\ninsbesondere dann, wenn Sie gerne möglichst breit abgesichert sind.\"\n\n\nThis looks already a bit better. The answer contains irrelevant\ninformation and some important information is missing.\nA side-mark:\nOn my device (without using possible optimizations), the model took\n3:34 minutes to answer the questions. So even for a small model like\nthis, it is not really feasible to use it in a chatbot on commodity\nhardware."
  },
  {
    "objectID": "source/tutorials/creating_grouning_db.html#conclusions",
    "href": "source/tutorials/creating_grouning_db.html#conclusions",
    "title": "15  Exploring German LLM Expert Bots",
    "section": "Conclusions",
    "text": "Conclusions\n\nGrounding:\n\nGrounding your model helps it giving you correct answers to very\nspecific questions.\nFormatting the text in a way that the model can understand it is\ncrucial.\n\nLarger Models perform better:\n\nGPT3.5 is better at answering questions off-the-shelf.\nLlama-2 70B is able to answer the questions correctly. However, it\nstruggles to provide correct German texts.\nMistral 7B struggles to answer questions correctly. More effort than\nI took is required.\n\nDifferent models need different prompt templates.\nWith few-shot learning you can improve the quality of the response\ndramatically.\nA high-level framework like langchain makes switching models and\nsources easy."
  },
  {
    "objectID": "source/tutorials/creating_grouning_db.html#outlook",
    "href": "source/tutorials/creating_grouning_db.html#outlook",
    "title": "15  Exploring German LLM Expert Bots",
    "section": "Outlook",
    "text": "Outlook\n\nNew Models:\n\nJust today Mistral released a new multi-lingual model with very\nimpressive reported performance. It’s size is between Mistral 7B and\nLLama2 70B. This might solve our problems with Mistral: Mistral: Mixtral of\nexperts\nThere are LLama-Models that have been pretrained on German texts and\nmight provide a better performance with our German prompts and\nresponses: Huggingface:\nLLama2 German Assistant\n\nFine Tuning models: Since the models from Mistral\nare substantially smaller, they can more easily be retrained\n(fine-tuned) to achieve higher accuracy. Furthermore, parameter tuning\ncould help lessening hallucinations of the model (i.e. making things\nup).\nBenchmarking: To properly evaluate the performance\nof the different approaches (Vector-LLM and QA-LLM), a larger test-set\nand requirements for correct answers would be needed."
  },
  {
    "objectID": "source/tutorials/creating_grouning_db.html#acknowledgments",
    "href": "source/tutorials/creating_grouning_db.html#acknowledgments",
    "title": "15  Exploring German LLM Expert Bots",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA special thank you to Moritz Settele and Koen Tersago from\nmorrow ventures for their helpful feedback on this\nblog-post."
  }
]