---
editor_options: 
  markdown: 
    wrap: 72
---

# Supervised Learning

Classification is the assignment of objects (data points) to categories
(classes). Regression allows you to assign a continuous output to your
data by estimating the relationship between your features and the
output.\
Both require a training data-set of points with known class labels and a
test data-set for evaluation.

## Classification Methods

### Evaluation of Classifiers

#### Confusion matrix

This gives a quick overview on the distribution of true positives
($TP$), false positives ($FP$) , $TN$ true negatives, $FN$ false
negatives.

|                 |                      |                    |
|-----------------|----------------------|--------------------|
|                 | *predicted positive* | *predicted negative* |
| *actual positive* | TP                   | FN                 |
| *actual negative* | FP                   | TN                 |

#### Basic Quality Measures

-   $\text{Accuracy / Success Rate} = \frac{ \text{correct predictions}}{\text{total predictions}} = \frac{ \text{TP}+\text{TN}}{\text{TP} + \text{TN}+\text{FP}+\text{FN}}$\
    This metric should only be used in this pure form, when the number
    of positive and negative samples are balanced.

-   $\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}$\
    i.e. How many of your positive predictions are actually positive?

-   $\text{True positive rate / Recall / Sensitivity} = \frac{\text{TP}}{\text{TP}+\text{FN}}$\
    i.e. How many of the positive samples did you catch?

-   $\text{True negative rate / Specificity / Selectivity} = \frac{\text{TN}}{\text{TN}+\text{FP}}$\
    i.e. How many of the negative samples did you catch as negative
    (i.e. are truly negative)?

-   $\text{F-score} = 2 \frac{\text{precision} * \text{recall}}{\text{precision}+\text{recall}}$\
    This is useful in cases of unbalanced classes to balance the
    trade-off between precision and recall.

#### Area under the Curve

This class of measures represents the quality of the classifier for
different threshold values $\theta$ by calculating the area under the
curve spanned by different quality measures.

##### Area under the Receiver Operating Characteristics Curve (AUROC or AUC)

The AUC can be interpreted as follows: When the classifier gets a
positive and a negative point, the AUC shows the probability that the
classifier will give a higher score to the positive point. A perfect
classifier has an AUC of 1, and AUC of 0.5 represents random guessing.

**!** This measure is not sensitive to class imbalance!

![Area under the precision recall curve. Source: [user cmglee on
wikipedia.org](https://commons.wikimedia.org/wiki/File:Roc_curve.svg)](figures/Roc_curve.png){width="44%"}

##### Area under the Precision-Recall Curve (AUPRC) / Average Precision (AveP)

This measure can be used for unbalanced data sets. It represents the
average precision as a function of the recall. The value of 1 represents
a perfect classifier.

![The precision-recall curve. Source:
[scikit-learn.org](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#:~:text=The%20precision%2Drecall%20curve%20shows,a%20low%20false%20negative%20rate.)](figures/pr_curve.png){width="65%"}

#### Handling Unbalanced Data

Having many more samples in one class than the others during training
can lead to high accuracy values event though the classifier performs
poorly on the smaller classes. You can handle the unbalance by:

-   up-sampling the smaller data set (creating more artificial samples
    for that class)

-   giving more weight to the samples in the smaller data set

-   using a quality measure that is sensitive to class imbalance

Oversampling using imbalanced-learn (see: )

``` python
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=0)
features_resampled, labels_resampled = ros.fit_resample(df[feature_cols], df[label_col])
```

Unbalance-sensitive quality measures: Sensitivity, specificity,
precision, recall, support and F-score

``` python
y_true = df[label_col]
y_pred = classifier.predict(df[feature_cols])

from imblearn.metrics import sensitivity_specificity_support
sensitivity, specificity, support = sensitivity_specificity_support(y_true, y_pred) 

from sklearn.metrics import precision_recall_fscore_support
precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred) 
```


### Nearest Neighbors Classifier

This classifier predicts the class label using the most common class
label of its $k$ nearest neighbors in the training data set.

**Pros:**

-   Classifier does not take time for training.
-   Can learn complex decision boundaries.

**Cons:**

-   The prediction is time consuming and scales with n.

``` python
from sklearn.neighbors import KNeighborsClassifier
kn_model = KNeighborsClassifier(n_neighbors=5)
kn_model.fit(X, y)
kn_model.predict([[5,1]])
```

[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\

### Naive Bayes Classifier

Naive Bayes classifies works on the assumption that the features are
conditionally independent given the class label. For every point a
simplified version of Bayes rule is used:

$$P(Y=y_i|X=x) \propto P(X=x|Y=y_i) * P(Y=y_i) $$ where $Y$ is the RV
for the class label and $X$ is the RV that contains the feature values.
This holds since $P(X=x)$ is the same for all classes. Since the
different features $X_j$ are assumed to be independent they can be
multiplied out. The label $y_i$ with the highest probability is the
predicted class label:
$$\arg \max_{y_i} P(Y=y_i|X=x) \propto P(Y=y_i) \prod_{j=1}^{d} P(X_j=x_j|Y=y_i)  $$
One usually estimates the value of $P(Y)$ as the ferquency of the
different classes in the training data or assumes that all classes are
equally likely.\
To estimate the $P(X_j=x_j|Y=y_i)$ the following distributions are
commonly used:

-   For binary features: Bernoulli distribution

-   For discrete features: Multinomial distribution

-   For continuous features: Normal / Gaussian distribution

For discrete features, you need to use a [smoothing
prior](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)
(add 1 to every feature count) to avoid 0 probabilities for samples with
features being 0 in the training data.

**Pros:**

-   Naive Bayes training is fast.

-   Combine descrete and continuous features since a different
    distribution can be used for each feature.

-   You can have straight decision boundaries (when classes have same
    variance), circular decision boundaries (different variance, same
    mean) and parabolic decision boundaries (different mean, different
    variance).

**Cons:**

-   The probability estimates from Naive Bayes are [usually
    bad](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes).

-   The independence assumption between the features is usually not
    given in real life.

``` python
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\

### Linear discriminant analysis (LDA)

Contrary to Naive Bayes, the features in LDA are not assumed to be
independently distributed. As with Bayes rule a distribution for each
class is calculated according to Bayes rule. $P(X=x|Y=y_i)$ is modeled
as a multivariate Gaussian distribution. The Gaussians for each class
are assumed to be the same. The log-posterior can be simplified to:

$$log(P(y=y_i|x) = - \frac{1}{2} (x-\mu_i)^t \Sigma^{-1}(x-\mu_i)+\log P(y=y_i)$$
$\mu_i$ is the mean of class $i$, $(x-\mu_i)^t \Sigma^{-1}(x-\mu_i)$
corresponds to the [Mahalanobis
distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). Thus, we
assign the point to the class whose distribution it is closest to.\
LDA can also be thought of projecting the data into a space with $k-1$
dimensions ($k$ being number of classes). More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Linear_discriminant_analysis).
It can also be used as a dimensionality reduction method.

``` python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
clf = LinearDiscriminantAnalysis()
clf.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)\

### Support Vector Classifier (SVC)

SVCs use hyperplanes to separate data points according to their class
label with a maximum margin ($M$) between the separating hyperplane
($x^T\beta + \beta_0=0$) and the points. If points cannot be perfectly
separated by the decision boundary, a soft margin SVM is used with a
slack variable $\xi$ that punishes points in the margin or on the wrong
side of the hyperplane. The optimization problem is given by
[@Hastie2009] : $$\begin{split}
            \max_{\beta, \beta_0, \beta=1} M, \\
            \text{subject to } y_i(x_i^T \beta + \beta_0) \ge 1 - \xi_i, \quad \forall i, \\\xi_i \ge 0, \quad \sum \xi_i \le constant, \quad i= 1, ..., N, 
        \end{split}$$ where $\beta$ are the coefficients and $x$ are the
$N$ data points. The support vectors are the points that determine the
orientation of the hyperplane (i.e. the closest points). The
classification function is given by:
$$G(x) = \text{sign}[x^T\beta + \beta_0]$$ If you only calculate the
inner part of the function you can get the distance of a point to your
hyperplane (in SKlearn you need to divide by the norm vector $w$ of your
hyperplane to get the true distance). To get the probability of a point
being in a class, you can use Platt's algorithm [@Platt1999]. SVMs are
sensitive to the scaling of the features. Therefore, the data should be
normalized before classification.\

``` python
from sklearn import svm
# train the model
svc_model = svm.SVC()
svc_model.fit(train_df[feautre_cols], train_df[label_col])
# test the model
y_predict = svc_model.predict(test_df[feature_cols])
```

<!-- ### Stochastic Gradient Descent -->

### Decision Trees

A decision tree uses binary rules to recursively split the data into
regions that contain only a single class.

![Decision trees work like flowcharts and have a root (the top node),
branches (possible outcomes of a test), nodes (tests on one attribute),
leafs (the class labels). This one shows the decision tree for the
classifier predicting if a patient survives the sinking of the Titanic.
Source: [user Gilgoldm on
wikipedia.org](https://commons.wikimedia.org/wiki/File:Decision_Tree.jpg)](figures/Decision_Tree.jpg){width="60%"}

**Pros:**

-   Interpretable results, if trees are not too big.

-   Cheap to train and predict.

-   Can handle categorical and continuous data at the same time.

-   Can be used for [multi-output
    problems](https://scikit-learn.org/stable/modules/tree.html#multi-output-problems)
    (e.g. color and shape of object).

**Cons:**

-   Overfitting risks
-   Some concepts are hard to learn (X-OR relationships, Decision
    boundaries are not smooth)
-   Unstable predictions: Small changes in data can lead to vastly
    different decision trees.

**Tips:**

-   Doing PCA helps the tree find separating features.

-   Visualize the produced tree.

-   Setting a lower boundary on the split-sizes of the data, reduces the
    chance of overfitting.

-   Balance the dataset or weight the samples according to class sizes
    to avoid constructing biased trees.

``` python
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(max_depth=10, min_samples_split=0.01, class_weight="balanced")
clf = clf.fit(X, Y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\

#### Random forests

Random forests are a version of a [bagging](#bagging) classifier
employing decision trees. To reduce the variance, the separate trees can
be assigned a limited number of features as well.

``` python
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(max_depth=10, max_features="sqrt", class_weight="balanced")
clf.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\

#### Gradient boosted decision trees (GBDTs)

Gradient boosted decision tree models are a form of
[boosting](#boosting) employing decision trees.

``` python
import lightgbm as lgbm
clf = lgbm.LGBMClassifier(class_weight= "balanced")
clf.fit(X, y)
```

More info: [lightgbm
documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm-lgbmclassifier),
[Parameter
tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html),\
[Further Parameter
tuning](https://neptune.ai/blog/lightgbm-parameters-guide)\
Similar model:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\

## Regression

<!-- Look at this https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py -->

### Evaluation of regression models

#### Mean squared error

This measure shows the deviation of the predicted value $\hat{y}$ to the
target value $y$. The squaring penalized large deviations and avoids
respective cancellation of positive and negative errors.

$$ MSE = 1/n \sum_i (y_i - \hat{y}_i)^2$$

``` python
from sklearn.metrics import mean_squared_error
mean_squared_error(y_true, y_pred)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)

#### R\^2 score / coefficient of determination

This measure shows how much of the [variance](#dist_prop) of the
target/dependent variable $y$ can be explained by the model/independent
variable $\hat{y}$.

$$ 
R^2 = 1 - \frac{\text{Unexplained Variance}}{\text{Total Variance}} = \frac{SS_{res}}{SS_{tot}} \\
SS_{res} = \sum_i(y_i-\hat{y}_i)^2 \\
SS_{tot} = \sum_i(y_i - \bar{y}_i)^2
$$ where $\bar{y}$ is the mean of the target $y$.

The value commonly reaches from 0 (model always predicts the mean of
$y$) to 1 (perfect fit of model to data). It can however be negative
(e.g. wrong model, heavy overfitting, ...). The *adjusted R\^2*
compensates for the size of the model (more variables), favoring simpler
models. More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Coefficient_of_determination)

``` python
from sklearn.metrics import r2_score
r2 = r2_score(y_true, y_pred)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)

<font color="grey">

#### Visual tools

### Linear Models

#### Ordinary Least Squares

#### Lasso regression

#### Ridge regression

##### Kernel ridge regression

#### Bayesian regression

#### ANOVA

#### Generalized linear models

</font>

### Gaussian process regression

Gaussian process regression is based on Bayesian Probability: You
generate many models and calculate the probability of your models given
the samples. You make predictions based on the probabilities of your
models.

You get non-linear functions to your data by using non-linear kernels:
You assume that input data points that are similar, will have similar
target values. The concept of similarity (e.g. same hour of the day) is
encoded in the kernels that you use.

![Schema of the training process of Gaussian process regression. The
left graph shows the prior samples of functions before. These functions
are then conditioned on the data (graph in middle). The right graph
shows the predictions with the credible intervals in gray. *Source:
[user Cdipaolo96 on
wikimedia.org](https://commons.wikimedia.org/wiki/File:Gaussian_Process_Regression.png)
.*](figures/Gaussian_Process_Regression.png)

**Pros:**

-   The model reports the predictions with a certain probability.

-   Hyperparameter tuning is built into the model.

**Cons:**

-   Training scales with $O(n^3)$. (approximations are [FITC and
    VFE](https://bwengals.github.io/fitc-and-vfe.html))

-   You need to design or choose a kernel.

``` python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, ExpSineSquared
kernel = DotProduct() + WhiteKernel() + RBF() + ExpSineSquared() # The kernel hyperparameters are tuned by the model
gpr = GaussianProcessRegressor(kernel=kernel)
gpr.fit(X, y)
gpr.predict(X, return_std=True)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)\

### Gradient boosted tree regression

Apart from classification, gradient boosted trees also allow for
regression. It works like gradient boosted trees for classification: You
iteratively add decision tree regressors that minimize the regression
loss of the already fitted ensemble. A [decision tree
regressor](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)
is a decision tree that is trained on continuous data instead of
discrete classification data, but its [output is still
discrete](https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047).

``` python
from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor(n_estimators = 500, min_samples_split =5, max_depth = 4, max_features="sqrt", n_iter_no_change=15)
gbr.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\

### Time Series Forecasting

For "normal" settings the order of the samples does not play a role
(e.g. blood sugar level of one sample is independent of the others). In
time series however, the samples need to be represented in an ordered
vector or matrix (e.g. The temperature of Jan 2nd is not independent of
the temperature on Jan 1st).

``` python
import pandas as pd
df = pd.read_csv("data.csv", header=0, index_col=0, names=["date", "sales"])
sales_series = df["sales"] # pandas series make working with time series easier 
```

<font color="grey">

#### ARIMA(X) Model

univariate time series model with exogenous regressor.

#### VARIMA(X) Model

Multivariate time series model, where the variables can influence each
other and the target can influence the variables and vice versa.

#### Prophet-Model

```{=html}
<!-- Explain Prophet model from Facebook. Source:
<https://otexts.com/fpp3/prophet.html> -->
```
</font>
