---
editor_options: 
  markdown: 
    wrap: 72
---

# Supervised learning

Classification is the assignment of objects (data points) to categories
(classes). Regression allows you to assign a continuous output to your
data by estimating the relationship between your features and the
output.\
Both require a training data-set of points with known class labels and a
test data-set for evaluation.

## General methods and concepts

### Hyper-parameter tuning

The hyper-parameters (e.g. kernel, gamma, number of nodes in tree) are
not trained by algorithm itself. An outer loop of hyper-parameter tuning
is needed to find the optimal hyper parameters.\
**!** It is strongly recommended to separate another validation set from
the training set for hyper-parameter tuning (you'll end up with
training-, validation- and test-set). See [Cross Validation](#crossval)
for best practice.

#### Grid search

The classic approach is exhaustive grid search: You create a grid of
hyper-parameters and iterate over all combinations. The combination with
the best score is used in the end. This approach causes big
computational costs due to the combinatorial explosion.

``` python
from sklearn.model_selection import GridSearchCV # combines grid search with cross-validation
from sklearn.neighbors import KNeighborsClassifier

kn_model = KNeighborsClassifier(n_neighbors=3)
parameters = {"n_neighbors": range(2,10), "p": [1,2], "weights": ["uniform", "distance"]}
clf = GridSearchCV(kn_model, parameters, cv=5)
clf.fit(X_train, y_train)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\

#### Randomized search

This approach is used, if there are too many combinations of
hyper-parameters for tuning. You allocate a budget of iterations and the
combinations of parameters are sampled randomly according to the
distributions you provide.

If you want to evaluate on a large set of hyperparameters, you can use a
halving strategy: You tune a large combination of parameters on few
resources (e.g. samples, trees). The best performing half of candidates
is re-evaluated on twice as many resources. This continues until the
best-performing candidate is evaluated on the full amount of resources.

``` python
from sklearn.ensemble import RandomForestClassifier
from sklearn.experimental import enable_halving_search_cv  # since this method is still experimental
from sklearn.model_selection import HalvingRandomSearchCV
from sklearn.utils.fixes import loguniform

rf_clf = RandomForestClassifier()

param_distributions = {"max_depth": [3, None],
                       "min_samples_split": loguniform(1, 10)}
hypa_search = HalvingRandomSearchCV(rf_clf, param_distributions,
                               resource='n_estimators',
                               max_resources=10,
                               n_jobs=-1, # important since hyper-parameter tuning is very costly
                               scoring = 'balanced_accuracy',
                               random_state=0).fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/grid_search.html#searching-for-optimal-parameters-with-successive-halving)\

### Model selection

The candidates for hyper-parameters must not be evaluated on the same
data that you trained it on (over-fitting risk). Thus, we separate
another data-set from the training data: The validation set. This is
reduces the amount of training data drastically. Therefore we use the
approaches of Cross Validation and Bootstrapping.

#### Cross Validation {#crossval}

In k-fold Cross Validation, we split the training set into k sub-sets.
We train on the samples in k-1 sub-sets and validate using the data in
the remaining sub-set. We iterate until we have validated on each
sub-set once. We then average out the k scores we obtain.

![Schema of the process for 5-fold Cross Validation. The data is first
split into training- and test-data. The training data is split into 5
sub-sets. The algorithm is trained on 4 sub-sets and evaluated on the
remaining sub-set. Each sub-set is used for validation once. *Source:
[scikit-learn.org](https://scikit-learn.org/stable/modules/cross_validation.html).*](figures/cross_validation.png){width="60%"}

``` python
from sklearn import svm
from sklearn.model_selection import cross_val_score
SVM_clf = svm.SVC (kernel='polynomial')
cv_scores = cross_val_score(SVM_clf, X, y, cv = 7)
cv_score = cv_scores.mean()
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics)

**!** If you have time-series data (and other clearly not i.i.d.) data,
you have to use [special cross-validation
strategies](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split).
There are [further
strategies](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split)
worth considering.

#### Bootstrapping

Instead of splitting the data into k subsets, you can also just sample
data into training and validation sets.\
More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).

### Errors & regularization

There are irreducible errors and reducible errors. Irreducible errors
stem from unknown variables or variables we have no data on. Reducible
errors are deviations from our model to its desired behavior and can be
reduced. Bias and variance are reducible errors.

$$\text{Error} = \text{Bias} + \text{Var} + \text{irr. Error}$$

#### Bias and Variance

##### Bias of an estimator

Bias tells you if your model oversimplifies the true relationship in
your data (underfitting).\
You have a model with a parameter $\hat{\theta}$ that is an estimator
for the true $\theta$. You want to know whether your model over- or
underestimates the true $\theta$ systematically.

$$\text{Bias}[\hat{\theta}]=\text{E}_{X|\mathcal{D}}[\hat{\theta}]- \theta$$

E.g. if the parameter captures how polynomial the model / relationship
of your data is, a too high value means that your model is
underfitting.\

More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Bias_of_an_estimator)

##### Variance of an estimator

Variance tells you if your model learns from noise instead of the true
relationship in your data (overfitting).

$$\text{Var}[\hat{\theta}]=\text{E}_{X|\mathcal{D}}[(\text{E}_{X|\mathcal{D}}[\hat{\theta}]- \hat{\theta})^2]$$
i.e. If you would bootstrap your data, it would show you how much your
parameter would jump around its mean, when it learns from the different
sampled sets.\

Your goal is now to find the sweet spot between a too biased (too simple
model) and a model with too high variance (too complex model).\

![Relationship between bias, variance and the total error. The minimum
of the total error lies at the best compromise between bias and
variance. *Source: [User Bigbossfarin on
wikimedia.org.](https://commons.wikimedia.org/wiki/File:Bias_and_variance_contributing_to_total_error.svg).*](figures/Bias_and_variance_contributing_to_total_error.png){width="60%"}

More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)

#### Regularization

To combat overfitting, we can introduce a term into our loss-function
that penalizes complex models. For linear regression, our regularized
loss function is will be:

$$\min L(\hat{y},y)= \min_{W,b} f(WX+b,y)+\lambda R(W)$$ where $f$ is
the unregularized loss function, $W$ is the weight matrix, $X$ is the
sample matrix and $b$ is the bias or offset term of the model (bias term
$\neq$ bias of estimator!). $R$ is the regularization function and
$\lambda$ is a parameter controlling its strength.\
i.e. The regularized loss function punishes large weights $W$ and leads
to flatter/smoother functions.\

More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Regularization_(mathematics))

#### Bagging {#bagging}

Train several instances of a complex estimator (aka. strong learner,
like large decision trees or KNN with small radius) on a subset of the
data. Then use a majority vote or average the scores for classifying to
get the final prediction. By training on different subsets and averaging
the results, the chances of overfitting are greatly reduced.

``` python
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
bagging = BaggingClassifier(KNeighborsClassifier(), max_features=0.5, n_estimators=20)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\

A classic example for a bagging classifier is [Random Forest
Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
or its variant [Extremely Randomized
Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
which further reduces variance and increases bias.

#### Boosting {#boosting}

Compared to bagging, we use weak learners that are not trained
independently of each other. We start with a single weak learner (e.g. a
small decision tree) and repeat the following steps:

1.  Add an additional model and train it.
2.  Increase weights of training samples that are falsely classified,
    decrease weights of correctly classified samples. (to be used by
    next added model.)
3.  Reweight results from the models in the combined model to reduce the
    training error.

The final model is an weighted ensemble of weak classifiers.\
The most popular ones are [gradient boosted decision
tree](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)
algorithms.

#### Stacking

Stacking closely resembles bagging: An ensemble of separately trained
base models is used to create an ensemble model. However, the continuous
(instead of discrete) outputs of commonly fewer heterogeneous models
(instead of same type of models) are used. The continuous outputs are
then fed into a final estimator (commonly logistic regression
classifier).

``` python
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier

classifiers = [
    ('svc', SVC()),
    ('knn', KNeighborsClassifier()),
    ('dtc', DecisionTreeClassifier())
    ]

clf = StackingClassifier(
    classifiers=estimators, final_estimator=LogisticRegression()
    )

clf.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)


## Classification

### Evaluation of Classifiers {#eval-clas}

#### Confusion matrix

This gives a quick overview on the distribution of true positives
($TP$), false positives ($FP$) , $TN$ true negatives, $FN$ false
negatives.

|                 |                      |                    |
|-----------------|----------------------|--------------------|
|                 | *predicted positive* | *predicted negative* |
| *actual positive* | TP                   | FN                 |
| *actual negative* | FP                   | TN                 |

#### Basic Quality Measures

-   $\text{Accuracy / Success Rate} = \frac{ \text{correct predictions}}{\text{total predictions}} = \frac{ \text{TP}+\text{TN}}{\text{TP} + \text{TN}+\text{FP}+\text{FN}}$\
    This metric should only be used in this pure form, when the number
    of positive and negative samples are balanced.

-   $\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}$\
    i.e. How many of your positive predictions are actually positive?

-   $\text{True positive rate / Recall / Sensitivity} = \frac{\text{TP}}{\text{TP}+\text{FN}}$\
    i.e. How many of the positive samples did you catch?

-   $\text{True negative rate / Specificity / Selectivity} = \frac{\text{TN}}{\text{TN}+\text{FP}}$\
    i.e. How many of the negative samples did you catch as negative
    (i.e. are truly negative)?

-   $\text{F-score} = 2 \frac{\text{precision} * \text{recall}}{\text{precision}+\text{recall}}$\
    This is useful in cases of unbalanced classes to balance the
    trade-off between precision and recall.

```python
from sklearn.metrics import classification_report
classification_report(y_true, y_pred)
```
more info: [scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)

#### Area under the Curve

This class of measures represents the quality of the classifier for
different threshold values $\theta$ by calculating the area under the
curve spanned by different quality measures.

##### Area under the Receiver Operating Characteristics Curve (AUROC or AUC)

The AUC can be interpreted as follows: When the classifier gets a
positive and a negative point, the AUC shows the probability that the
classifier will give a higher score to the positive point. A perfect
classifier has an AUC of 1, and AUC of 0.5 represents random guessing.

```python
from sklearn.metrics import roc_auc_score
roc_auc_score(y, clf.decision_function(X)) # instead of dec. func. you can use clf.predict_proba(X)
```
More info: [scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)

**!** This measure is not sensitive to class imbalance!

![Area under the precision recall curve. Source: [user cmglee on
wikipedia.org](https://commons.wikimedia.org/wiki/File:Roc_curve.svg)](figures/Roc_curve.png){width="44%"}

##### Area under the Precision-Recall Curve (AUPRC) / Average Precision (AveP)

This measure can be used for unbalanced data sets. It represents the
average precision as a function of the recall. The value of 1 represents
a perfect classifier.

```python
from sklearn.metrics import average_precision_score
average_precision_score(y_true, y_pred)
```
More info: [scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score)

![The precision-recall curve. Source:
[scikit-learn.org](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#:~:text=The%20precision%2Drecall%20curve%20shows,a%20low%20false%20negative%20rate.)](figures/pr_curve.png){width="65%"}

#### Handling Unbalanced Data

Having many more samples in one class than the others during training
can lead to high accuracy values event though the classifier performs
poorly on the smaller classes. You can handle the unbalance by:

-   up-sampling the smaller data set (creating more artificial samples
    for that class)

-   giving more weight to the samples in the smaller data set

-   using a quality measure that is sensitive to class imbalance

Oversampling using imbalanced-learn (see: )

``` python
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=0)
features_resampled, labels_resampled = ros.fit_resample(df[feature_cols], df[label_col])
```

Unbalance-sensitive quality measures: Sensitivity, specificity,
precision, recall, support and F-score

``` python
y_true = df[label_col]
y_pred = classifier.predict(df[feature_cols])

from imblearn.metrics import sensitivity_specificity_support
sensitivity, specificity, support = sensitivity_specificity_support(y_true, y_pred) 

from sklearn.metrics import precision_recall_fscore_support
precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred) 
```


### Nearest Neighbors Classifier

This classifier predicts the class label using the most common class
label of its $k$ nearest neighbors in the training data set.

**Pros:**

-   Classifier does not take time for training.
-   Can learn complex decision boundaries.

**Cons:**

-   The prediction is time consuming and scales with n.

``` python
from sklearn.neighbors import KNeighborsClassifier
kn_model = KNeighborsClassifier(n_neighbors=5)
kn_model.fit(X, y)
kn_model.predict([[5,1]])
```

[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\

### Naive Bayes Classifier

Naive Bayes classifies works on the assumption that the features are
conditionally independent given the class label. For every point a
simplified version of Bayes rule is used:

$$P(Y=y_i|X=x) \propto P(X=x|Y=y_i) * P(Y=y_i) $$ where $Y$ is the RV
for the class label and $X$ is the RV that contains the feature values.
This holds since $P(X=x)$ is the same for all classes. Since the
different features $X_j$ are assumed to be independent they can be
multiplied out. The label $y_i$ with the highest probability is the
predicted class label:
$$\arg \max_{y_i} P(Y=y_i|X=x) \propto P(Y=y_i) \prod_{j=1}^{d} P(X_j=x_j|Y=y_i)  $$
One usually estimates the value of $P(Y)$ as the ferquency of the
different classes in the training data or assumes that all classes are
equally likely.\
To estimate the $P(X_j=x_j|Y=y_i)$ the following distributions are
commonly used:

-   For binary features: Bernoulli distribution

-   For discrete features: Multinomial distribution

-   For continuous features: Normal / Gaussian distribution

For discrete features, you need to use a [smoothing
prior](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)
(add 1 to every feature count) to avoid 0 probabilities for samples with
features being 0 in the training data.

**Pros:**

-   Naive Bayes training is fast.

-   Combine descrete and continuous features since a different
    distribution can be used for each feature.

-   You can have straight decision boundaries (when classes have same
    variance), circular decision boundaries (different variance, same
    mean) and parabolic decision boundaries (different mean, different
    variance).

**Cons:**

-   The probability estimates from Naive Bayes are [usually
    bad](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes).

-   The independence assumption between the features is usually not
    given in real life.

``` python
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\

### Linear discriminant analysis (LDA)

Contrary to Naive Bayes, the features in LDA are not assumed to be
independently distributed. As with Bayes rule a distribution for each
class is calculated according to Bayes rule. $P(X=x|Y=y_i)$ is modeled
as a multivariate Gaussian distribution. The Gaussians for each class
are assumed to be the same. The log-posterior can be simplified to:

$$log(P(y=y_i|x) = - \frac{1}{2} (x-\mu_i)^t \Sigma^{-1}(x-\mu_i)+\log P(y=y_i)$$
$\mu_i$ is the mean of class $i$, $(x-\mu_i)^t \Sigma^{-1}(x-\mu_i)$
corresponds to the [Mahalanobis
distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). Thus, we
assign the point to the class whose distribution it is closest to.\
LDA can also be thought of projecting the data into a space with $k-1$
dimensions ($k$ being number of classes). More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Linear_discriminant_analysis).
It can also be used as a dimensionality reduction method.

``` python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
clf = LinearDiscriminantAnalysis()
clf.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)\

### Support Vector Classifier (SVC)

SVCs use hyperplanes to separate data points according to their class
label with a maximum margin ($M$) between the separating hyperplane
($x^T\beta + \beta_0=0$) and the points. If points cannot be perfectly
separated by the decision boundary, a soft margin SVM is used with a
slack variable $\xi$ that punishes points in the margin or on the wrong
side of the hyperplane. The optimization problem is [given by](https://hastie.su.domains/Papers/ESLII.pdf): 

$$\begin{split}
            \max_{\beta, \beta_0, \beta=1} M, \\
            \text{subject to } y_i(x_i^T \beta + \beta_0) \ge 1 - \xi_i, \quad \forall i, \\\xi_i \ge 0, \quad \sum \xi_i \le constant, \quad i= 1, ..., N, 
        \end{split}$$ 
        
where $\beta$ are the coefficients and $x$ are the
$N$ data points. The support vectors are the points that determine the
orientation of the hyperplane (i.e. the closest points). The
classification function is given by:

$$G(x) = \text{sign}[x^T\beta + \beta_0]$$ 

If you only calculate the
inner part of the function you can get the distance of a point to your
hyperplane (in SKlearn you need to divide by the norm vector $w$ of your
hyperplane to get the true distance). To get the probability of a point
being in a class, you can use [Platt's algorithm](https://en.wikipedia.org/wiki/Platt_scaling). SVMs are
sensitive to the scaling of the features. Therefore, the data should be
normalized before classification.\

``` python
from sklearn import svm
# train the model
svc_model = svm.SVC()
svc_model.fit(train_df[feautre_cols], train_df[label_col])
# test the model
y_predict = svc_model.predict(test_df[feature_cols])
```

<!-- ### Stochastic Gradient Descent -->

### Decision Trees

A decision tree uses binary rules to recursively split the data into
regions that contain only a single class.

![Decision trees work like flowcharts and have a root (the top node),
branches (possible outcomes of a test), nodes (tests on one attribute),
leafs (the class labels). This one shows the decision tree for the
classifier predicting if a patient survives the sinking of the Titanic.
Source: [user Gilgoldm on
wikipedia.org](https://commons.wikimedia.org/wiki/File:Decision_Tree.jpg)](figures/Decision_Tree.jpg){width="60%"}

**Pros:**

-   Interpretable results, if trees are not too big.

-   Cheap to train and predict.

-   Can handle categorical and continuous data at the same time.

-   Can be used for [multi-output
    problems](https://scikit-learn.org/stable/modules/tree.html#multi-output-problems)
    (e.g. color and shape of object).

**Cons:**

-   Overfitting risks
-   Some concepts are hard to learn (X-OR relationships, Decision
    boundaries are not smooth)
-   Unstable predictions: Small changes in data can lead to vastly
    different decision trees.

**Tips:**

-   Doing PCA helps the tree find separating features.

-   Visualize the produced tree.

-   Setting a lower boundary on the split-sizes of the data, reduces the
    chance of overfitting.

-   Balance the dataset or weight the samples according to class sizes
    to avoid constructing biased trees.

``` python
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(max_depth=10, min_samples_split=0.01, class_weight="balanced")
clf = clf.fit(X, Y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\

#### Random forests

Random forests are a version of a [bagging](#bagging) classifier
employing decision trees. To reduce the variance, the separate trees can
be assigned a limited number of features as well.

``` python
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(max_depth=10, max_features="sqrt", class_weight="balanced")
clf.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\

#### Gradient boosted decision trees (GBDTs)

Gradient boosted decision tree models are a form of
[boosting](#boosting) employing decision trees.

``` python
import lightgbm as lgbm
clf = lgbm.LGBMClassifier(class_weight= "balanced")
clf.fit(X, y)
```

More info: [lightgbm
documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm-lgbmclassifier),
[Parameter
tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html),\
[Further Parameter
tuning](https://neptune.ai/blog/lightgbm-parameters-guide)\
Similar model:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\

## Regression

<!-- Look at this https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py -->

### Evaluation of regression models {#eval-reg}

#### Mean squared error

This measure shows the deviation of the predicted value $\hat{y}$ to the
target value $y$. The squaring penalized large deviations and avoids
respective cancellation of positive and negative errors.

$$ MSE = 1/n \sum_i (y_i - \hat{y}_i)^2$$

``` python
from sklearn.metrics import mean_squared_error
mean_squared_error(y_true, y_pred)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)

#### R\^2 score / coefficient of determination

This measure shows how much of the [variance](#dist_prop) of the
target/dependent variable $y$ can be explained by the model/independent
variable $\hat{y}$.

$$ 
R^2 = 1 - \frac{\text{Unexplained Variance}}{\text{Total Variance}} = \frac{SS_{res}}{SS_{tot}} \\
SS_{res} = \sum_i(y_i-\hat{y}_i)^2 \\
SS_{tot} = \sum_i(y_i - \bar{y}_i)^2
$$ where $\bar{y}$ is the mean of the target $y$.

The value commonly reaches from 0 (model always predicts the mean of
$y$) to 1 (perfect fit of model to data). It can however be negative
(e.g. wrong model, heavy overfitting, ...). The *adjusted R\^2*
compensates for the size of the model (more variables), favoring simpler
models. More info:
[wikipedia.org](https://en.wikipedia.org/wiki/Coefficient_of_determination)

``` python
from sklearn.metrics import r2_score
r2 = r2_score(y_true, y_pred)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)



<!-- #### Visual tools --> 

### Linear Models

#### Ordinary Least Squares

Linear models are of the form: 
$$y_i = x_i \beta + \epsilon_i = \beta_0 + \beta_1 x_{i(1)} + \beta_2 x_{i(2)} + \epsilon_i$$ 

- $x_{i(d)}$ is the dimension $d$ of point $i$. $x_i$ is called *regressor, independent, exogenous or explanatory variable*. The regressors can be non-linear functions of the data.
- $y_i$ is the observed value for the *depependent variable* for point $i$. 
- $\beta_0$ is called bias or intercept (not the same as bias of a machine learning model).
- $\beta_{0, 1...n}$ are the *regression coefficients*, $\beta_{1,...n}$ are called *slope*.  In linear models the regression coefficients need to be linear. 
- $\epsilon$ is the *error term* or *noise*. 

*Predicted* values are denoted as $\hat{\beta}, \hat{y}$.
We try to minimize the $\epsilon$-term using the least squared error method.

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept=True)
model.fit(X, y)
```
More info: [scikit-learn.org]()

**Pros: **

- Easy to interpret

- Fast to train and predict

**Cons: **

- Assumption of linear relation between dependent and independet variables. (-> possibly underfitting)

- Sensitive to outliers (-> possibly overfitting)

**Tips for interpreting linear models:**

- When comparing the strength of different coefficients: Take the scale of the feature into consideration (e.g. don't compare "m/s" and "km/h"). 

- Only when the features have been standardized / normalized, you can safely compare them. 

- Check for robustness of coefficients: Make cross validation and obsever their variability. High variability can be a [sign of correlation](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#the-problem-of-correlated-variables) with other features. 

- [Correlation does not mean causation](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation). `r emo::ji("point_up_2")` `r emo::ji("nerd")` 


<font color="grey">

<!--
#### Lasso regression

#### Ridge regression

##### Kernel ridge regression

#### Bayesian regression

#### ANOVA
-->
</font>

### Gaussian process regression

Gaussian process regression is based on Bayesian Probability: You
generate many models and calculate the probability of your models given
the samples. You make predictions based on the probabilities of your
models.

You get non-linear functions to your data by using non-linear kernels:
You assume that input data points that are similar, will have similar
target values. The concept of similarity (e.g. same hour of the day) is
encoded in the kernels that you use.

![Schema of the training process of Gaussian process regression. The
left graph shows the prior samples of functions before. These functions
are then conditioned on the data (graph in middle). The right graph
shows the predictions with the credible intervals in gray. *Source:
[user Cdipaolo96 on
wikimedia.org](https://commons.wikimedia.org/wiki/File:Gaussian_Process_Regression.png)
.*](figures/Gaussian_Process_Regression.png)

**Pros:**

-   The model reports the predictions with a certain probability.

-   Hyperparameter tuning is built into the model.

**Cons:**

-   Training scales with $O(n^3)$. (approximations are [FITC and
    VFE](https://bwengals.github.io/fitc-and-vfe.html))

-   You need to design or choose a kernel.

``` python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, ExpSineSquared
kernel = DotProduct() + WhiteKernel() + RBF() + ExpSineSquared() # The kernel hyperparameters are tuned by the model
gpr = GaussianProcessRegressor(kernel=kernel)
gpr.fit(X, y)
gpr.predict(X, return_std=True)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)\

### Gradient boosted tree regression

Apart from classification, gradient boosted trees also allow for
regression. It works like gradient boosted trees for classification: You
iteratively add decision tree regressors that minimize the regression
loss of the already fitted ensemble. A [decision tree
regressor](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)
is a decision tree that is trained on continuous data instead of
discrete classification data, but its [output is still
discrete](https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047).

``` python
from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor(n_estimators = 500, min_samples_split =5, max_depth = 4, max_features="sqrt", n_iter_no_change=15)
gbr.fit(X, y)
```

More info:
[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\

### Time Series Forecasting

For "normal" settings the order of the samples does not play a role
(e.g. blood sugar level of one sample is independent of the others). In
time series however, the samples need to be represented in an ordered
vector or matrix (e.g. The temperature of Jan 2nd is not independent of
the temperature on Jan 1st).

``` python
import pandas as pd
df = pd.read_csv("data.csv", header=0, index_col=0, names=["date", "sales"])
sales_series = df["sales"] # pandas series make working with time series easier 
```

<font color="grey">

#### ARIMA(X) Model

univariate time series model with exogenous regressor.

#### VARIMA(X) Model

Multivariate time series model, where the variables can influence each
other and the target can influence the variables and vice versa.

#### Prophet-Model

```{=html}
<!-- Explain Prophet model from Facebook. Source:
<https://otexts.com/fpp3/prophet.html> -->
```
</font>
