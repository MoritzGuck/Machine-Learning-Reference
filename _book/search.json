[{"path":"index.html","id":"summary","chapter":"Summary","heading":"Summary","text":"reference machine learning approaches methods. \ntopics range math fundamentals complex machine learning models\nexplanation methods. underlying formulas mechanisms provided method \nwell code snippets respective python libraries. goal give data scientists \ncatalog find solutions current problems, refresh knowledge give references reading. \nfind errors unclear explanations text, please file issue\n: https://github.com/MoritzGuck/Machine-Learning-Reference.currently incomplete sections marked grey.","code":""},{"path":"probability-theory-linear-algebra.html","id":"probability-theory-linear-algebra","chapter":"1 Probability Theory & Linear Algebra","heading":"1 Probability Theory & Linear Algebra","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"probability-theory","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1 Probability Theory","text":"probability measure frequent likely event take\nplace.","code":""},{"path":"probability-theory-linear-algebra.html","id":"probability-basics","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.1 Probability Basics","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"probability-interpretations","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.1.1 Probability interpretations","text":"Frequentist: Fraction positive samples, measured\ninfinitely many samples.Frequentist: Fraction positive samples, measured\ninfinitely many samples.Objectivist: Probabilities due inherent uncertainty\nproperties. Probabilities calculated putting outcomes \ninterest relation possible outcomes.Objectivist: Probabilities due inherent uncertainty\nproperties. Probabilities calculated putting outcomes \ninterest relation possible outcomes.Subjectivist: agent’s rational degree belief (\nexternal). belief needs coherent (.e. make bets\nusing probabilities guaranteed lose money)\ntherefore need follow rules probability.Subjectivist: agent’s rational degree belief (\nexternal). belief needs coherent (.e. make bets\nusing probabilities guaranteed lose money)\ntherefore need follow rules probability.Bayesian: (Building subjectivism) reasonable expectation /\ndegree belief based information available \nstatistician / system. allows give certainties events,\ndon’t samples (e.g. disappearance south pole\n2030).Bayesian: (Building subjectivism) reasonable expectation /\ndegree belief based information available \nstatistician / system. allows give certainties events,\ndon’t samples (e.g. disappearance south pole\n2030).Also frequentist view free subjectivity since need \ncompare events otherwise similar objects. Usually \ncompletely similar objects, need define .","code":""},{"path":"probability-theory-linear-algebra.html","id":"probability-space","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.1.2 Probability Space","text":"probability space triplet space containing sample/outcome\nspace \\(\\Omega\\) (containing possible atomic events), collection \nevents \\(S\\) (containing subset \\(\\Omega\\) want assign\nprobabilities) mapping \\(P\\) \\(\\Omega\\) \\(S\\).","code":""},{"path":"probability-theory-linear-algebra.html","id":"axioms-of-probability","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.1.3 Axioms of Probability","text":"mapping \\(P\\) must fulfill axioms probability:\\(P() \\geq 0\\)\\(P() \\geq 0\\)\\(P(\\Omega) = 1\\)\\(P(\\Omega) = 1\\)\\(,b \\S\\) \\(\\cap b = \\{\\}\\)\n\\(\\Rightarrow P(\\cup b) = P() + P(b)\\)\\(,b \\S\\) \\(\\cap b = \\{\\}\\)\n\\(\\Rightarrow P(\\cup b) = P() + P(b)\\)\\(\\), \\(b\\) events.","code":""},{"path":"probability-theory-linear-algebra.html","id":"random-variable-rv","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.1.4 Random Variable (RV)","text":"RV function maps points sample space \\(\\Omega\\)\nrange (e.g. Real numbers booleans). characterized \ndistribution function. E.g. coin toss:\n\\[X(\\omega) = \\begin{cases}\n                0, \\text{ } \\omega = heads\\\\\n                1, \\text{ } \\omega = tails.\n            \\end{cases}\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"proposition","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.1.5 Proposition","text":"Proposition conclusion statistical inference/prediction \ncan true false (e.g. classification datapoint). \nformally: disjunction events logic model holds. event\ncan written propositional logic model:\\(= true, B = false \\Rightarrow \\land \\neg b\\). Propositions can \ncontinuous, discrete boolean.","code":""},{"path":"probability-theory-linear-algebra.html","id":"probability-distributions","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2 Probability distributions","text":"Probability distributions assign probabilities possible points\n\\(\\Omega\\) (e.g. \\(P(Weather) = \\langle 0.3, 0.4, 0.2, 0.1 \\rangle\\),\nrepresenting Rain, sunshine, clouds snow). Joint probability\ndistributions give probability atomic event RVs\n(e.g. \\(P(weather, accident)\\) gives \\(2\\times 4\\) matrix.)","code":""},{"path":"probability-theory-linear-algebra.html","id":"cumulative-distribution-function-cdf","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.1 Cumulative Distribution Function (CDF)","text":"CDF defined \\(F_X(x) = P(X \\leq x)\\) (See figure\n\\(CDF\\)).Cumulative distribution function normal distribution \ndifferent mean (\\(\\mu\\)) variance (\\(\\sigma\\)). Source: user\nInductiveload \nwikimedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"probability-density-function-pdf","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.2 Probability Density Function (PDF)","text":"continuous functions PDF defined \n\\[p(x) =  {d \\dx} p(X \\leq x).\\] probability x \nfinite interval \\[P(< X \\leq b) = \\int_a^b p(x) dx\\] PDF shown\nfollowing figure.Probability density function normal distribution variance\n(\\(\\sigma\\)). red range Box-plot shown \nquartiles (Q1, Q3) interquartile range (IQR). \ncutoffs (borders darker blue regions) IQR (top) \\(\\sigma\\)\nchosen. Another common cutoff confidence interval light\nblue regions probability mass \\(2 * \\alpha / 2\\). Source:\nuser Jhguch \nwikimedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"dist_prop","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.3 Properties of Distributions","text":"expected value (\\(E\\)) mean (\\(\\mu\\)) given \n\\(E[X] = \\sum_{x \\X} x*p(x)\\) discrete RVs \n\\(E[X] = \\int_X x*p(x) dx\\) continuous RVs.expected value (\\(E\\)) mean (\\(\\mu\\)) given \n\\(E[X] = \\sum_{x \\X} x*p(x)\\) discrete RVs \n\\(E[X] = \\int_X x*p(x) dx\\) continuous RVs.variance measures spread distribution:\n\\(var[X] = \\sigma^2 = E[(X-\\mu)^2] = E[X]^2 - \\mu^2\\).variance measures spread distribution:\n\\(var[X] = \\sigma^2 = E[(X-\\mu)^2] = E[X]^2 - \\mu^2\\).standard deviation given : \\(\\sqrt{var[X]} = \\sigma\\).standard deviation given : \\(\\sqrt{var[X]} = \\sigma\\).mode value highest probability (point\nPDF highest value):mode value highest probability (point\nPDF highest value):median point point less median\npoints greater median probability\n(\\(0.5\\)).median point point less median\npoints greater median probability\n(\\(0.5\\)).quantiles (\\(Q\\)) divide datapoints sets equal\nnumber. \\(Q_1\\) quartile 25% values . \ninterquartile range (IQR) measure show variability\ndata (distant points first last quartile\n)quantiles (\\(Q\\)) divide datapoints sets equal\nnumber. \\(Q_1\\) quartile 25% values . \ninterquartile range (IQR) measure show variability\ndata (distant points first last quartile\n)","code":""},{"path":"probability-theory-linear-algebra.html","id":"diracdelta","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.4 Dirac delta function","text":"dirac delta simply function infinite one point\n0 everywhere else:\n\\[\\delta(x)=\\begin{cases} \\infty , \\; \\text{ } x = 0 \\\\0, \\quad \\text{} x \\neq 0 \\end{cases} \\qquad \\text{} \\int_{-\\infty}^{\\infty} \\delta(x) dx = 1\\]\n(Needed distributions )","code":""},{"path":"probability-theory-linear-algebra.html","id":"uniform-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.5 Uniform distribution","text":"uniform distribution probability throughout specific\ninterval:\n\\[\\text{Unif}(,b) = \\frac{1}{b-} 1 \\mkern-6mu 1 (< x \\leq b) = \\begin{cases}\n                \\frac{1}{b-}, \\quad \\text{} x \\[ ,b ] \\\\\n                0, \\qquad \\text{else}\n            \\end{cases}\\] \\(1 \\mkern-6mu 1\\) vector ones.Uniform distribution. Source: user IkamusumeFan \nwikimedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"discrete-distributions","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.6 Discrete distributions","text":"Used random variables discrete states.","code":""},{"path":"probability-theory-linear-algebra.html","id":"binomial-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.6.1 Binomial distribution","text":"Used series experiments two outcomes (success miss. e.g.\nseries coin flips).\n\\[X \\sim \\text{Bin}(n, \\theta ), \\quad \\text{Bin}(k|n,\\theta)={n \\choose k} \\theta^k (1-\\theta)^{n-k} , \\quad {n \\choose k} = \\frac{n!}{k!(n-k)!},\\]\n\\(n\\) number total experiments, \\(k\\) number \nsuccessful experiments \\(\\theta\\) probability success \nexperiment.Binomial distribution balls Pascals\ntriangles \ndifferent numbers layers (top one 0 layers). Example: \ntriangle \\(n=6\\) layers, probability ball lands \nmiddle box \\(k=3\\) \\(20/64\\). Source: user Watchduck \nwikimedia.org","code":""},{"path":"probability-theory-linear-algebra.html","id":"bernoulli-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.6.2 Bernoulli distribution","text":"special case binomial distribution \\(n=1\\) (e.g. one coin\ntoss).\n\\[X \\sim \\text{Ber}(\\theta ), \\quad \\text{Ber}(x | \\theta)=\\theta^{1 \\mkern-6mu 1 (x=1)} (1-\\theta)^{1 \\mkern-6mu 1(x=0)}= \\begin{cases}\n                    \\theta, \\qquad \\text{} x=1 \\\\\n                    1 - \\theta, \\; \\text{} x=0\n                \\end{cases}\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"multinomial-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.6.3 Multinomial distribution","text":"Used experiments k different outcomes (e.g. dice rolls:\nProbability different counts different sides).\n\\[\\text{Mu}(x|n,\\theta) =  {n \\choose x_1, ..., x_K}\\prod_{=1}^K\\theta_j^{x_j} = \\frac{n!}{x_1!, ..., x_k!}\\prod_{=1}^K\\theta_j^{x_j},\\]\n\\(k\\) number outcomes, \\(x_j\\) number times \noutcome \\(j\\) happens. \\(X = (X_1, ..., X_K)\\) random vector.","code":""},{"path":"probability-theory-linear-algebra.html","id":"multinoulli-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.6.4 Multinoulli distribution","text":"special case multinomial distribution \\(n=1\\). random\nvector represented dummy- one-hot-encoding (e.g.\n\\((0,0,1,0,0,0)\\) outcome 3 takes place).\n\\[\\text{Mu}(x|1,\\theta) = \\prod_{j=0}^K \\theta_j^{1 \\mkern-6mu 1(x_j=1)}\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"empirical-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.6.5 Empirical distribution","text":"empirical distribution follows empirical measurements strictly.\nCDF jumps 1/n every time sample “encountered” (see figure).\\[\\text{p}_{\\text{emp}}() = \\frac{1}{N} \\sum_{=1}^N \\delta_{x_i}(), \\quad \\delta_{x_i}=\\begin{cases}1, \\quad \\text{} x \\\\\\0, \\quad \\text{} x \\notin \\end{cases},\\]\nw \\(x_1, ..., x_N\\) data set N points. points can also\nweighted: \\[p(x) =  \\sum_{=1}^N w_i \\delta_{x_i}(x)\\]Cumulative empirical distribution function (blue line) samples\ndrawn standard normal distribution (green line). values \ndrawn samples shown grey lines bottom. Source: user\nnagualdesign \nwikimedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"continuous-distributions","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.7 Continuous distributions","text":"Used random variables continuous states.","code":""},{"path":"probability-theory-linear-algebra.html","id":"normalgaussian-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.7.1 Normal/Gaussian distribution","text":"Often chosen random noise simple needs \nassumptions (see sect. CLT). PDF given :\\[p(x|\\mu\\sigma^2)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right],\\]\n\\(\\mu\\) mean \\(\\sigma^2\\) variance. CDF given\n:\n\\[\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{\\infty}^xe^{\\frac{-t^2}{2}dt}\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"multivariate-normalgaussian-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.7.2 Multivariate normal/Gaussian distribution","text":"T datapoints k dimensions (features). pdf :\n\\[p(x|\\mu,\\Sigma) = \\dfrac{1}{\\sqrt{(2\\pi)^k|\\Sigma|}}\\exp\\left[-\\dfrac{1}{2}(x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)\\right],\\]\nx now multiple dimension (\\(x_1, x_2, ..., x_k\\)) \\(\\Sigma\\)\n\\(k \\times k\\) covariance matrix:\n\\(\\Sigma = \\text{E}[(X-\\mu)(X-\\mu)]\\). covariance features :\n\\(\\text{Cov}[X_i, X_j] = \\text{E}[(X_i-\\mu_i)(X_j-\\mu_j)]\\)","code":""},{"path":"probability-theory-linear-algebra.html","id":"beta-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.7.3 Beta distribution","text":"defined \\(0 \\leq x \\leq 1\\) (see figure Beta\ndistribution). pdf :\n\\[f(x|\\alpha, \\beta) = \\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\]\nbeta function \\(B\\) \nnormalize ensure total probability 1 .Probability density function beta-distribution different\nparameter values. Source: user MarkSweep \nwikimedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"dirichlet-distribution","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.7.4 Dirichlet distribution","text":"multivariate version Beta distribution. PDF :\n\\[\\text{Dir}({x}|{\\alpha}) \\triangleq \\dfrac{1}{B({\\alpha})}\\prod\\limits_{=1}^K x_i^{\\alpha_i-1},\\quad \\sum_{=1}^K x_i =1, \\quad x_i \\geq 0 \\text{ }\\forall \\]Probability density function Dirichlet-distribution \n2-simplex (triangle) different parameter values. Clockwise top\nleft: \\(\\alpha\\) = (6,2,2), (3,7,5), (6,2,6), (2,3,4). Source: user ThG\n\nwikimedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"marginal-distributions","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.2.7.5 Marginal distributions","text":"probability distributions subsets original\ndistribution. Marginal distributions normal distributions also\nnormal distributions.Data following 2D-Gaussian distribution. Marginal distributions \nshown sides blue orange. Source: user Auguel \nwikimedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"CLT","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.3 Central limit theorem","text":"many cases sum random variables follow normal\ndistribution n goes infinity.","code":""},{"path":"probability-theory-linear-algebra.html","id":"bayesian-probability","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.4 Bayesian probability","text":"Baeysian probability represents plausibility proposition based\navailable information (.e. degree information\nsupports proposition). use form statistics \nespecially useful random variables assumed ..d.\n(.e. event independent event (e.g.\ndrawing balls without laying back urn)).","code":""},{"path":"probability-theory-linear-algebra.html","id":"conditionalposterior-probability","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.4.1 Conditional/Posterior Probability","text":"Expresses probability one event (\\(Y\\)) condition \nanother event (\\(E\\)) occurred. (e.g. \\(C\\) = “gets cancer”, \\(S\\) = “\nsmoker” \\(\\rightarrow\\) \\(p(C|S)=0.2\\), meaning: “given sole\ninformation someone smoker, probability getting\ncancer 20%.”)\nconditional probability can calculated like follows. defining\njoined probability like : \\[P(\\cap B) = P(\\mid B) P(B)\\] \nsolve \\(P(\\mid B)\\):\n\\[P(\\mid B) = \\frac{P(\\cap B)}{P(B)}=\\frac{P(, B)}{P(B)}=\\alpha{P(, B)},\\]\n\\(\\alpha\\) used normalization constant. hidden\nvariables (confounding factors) need sum like :\n\\[P(Y|E=e)=\\alpha P(Y,E=e)=\\alpha\\sum_h P(Y,E=e,H=h)\\] \\(X\\)\ncontains variables, \\(Y\\) called query variable, \\(E\\) called\nevidence variable, \\(H=X-Y-E\\) called hidden variable \nconfounding factor. get joint probabilities summing \nhidden variable. ! Usually \\(p(|B) \\neq p(B|)\\)! Priors often forgotten: E.g. \\(P(\\text{\"COVID-19\"})\\) \nconfused \\(P(\\text{\"COVID-19\"}|\\text{\"Person getting tested\"})\\)\n(people symptoms go testing station).! Base rate neglect: -representing prior probability. E.g.\ntest 5% false positive rate incidence disease\n2% population. tested positive population\nscreening probability disease 29%.\nConditional distributions Gaussian distributions Gaussian\ndistributions .","code":""},{"path":"probability-theory-linear-algebra.html","id":"independence","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.4.2 Independence","text":"independent variables holds: \\(P(|B)=P()\\) \\(P(B|)=P(B)\\)","code":""},{"path":"probability-theory-linear-algebra.html","id":"conditional-independence","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.4.3 Conditional independence","text":"Two events \\(\\) \\(B\\) independent, given \\(C\\): \\(P(|B,C)=P(|C)\\).\n\\(\\) \\(B\\) must information , given \ninformation \\(C\\). E.g. school children:\n\\(P(\\text{\"vocabulary\"}|\\text{\"height\"}, \\text{\"age\"})= P(\\text{\"vocabulary\"}|\\text{\"age\"})\\).","code":""},{"path":"probability-theory-linear-algebra.html","id":"bayes-rule","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.4.4 Bayes Rule","text":"Bayes rule structured approach update prior beliefs /\nprobabilities new information (data). conditional\nprobability (\\(P(,B)=P(|B)P(B)=P(B|)P()\\)) get Bayes\nrule transforming right-side equation\n:\\[P(\\text{hypothesis}|\\text{evidence}) =\\dfrac{P(\\text{evidence}|\\text{hypothesis})P(\\text{hypothesis})}{P(\\text{evidence})}\\]\noften used :\n\\[P(\\text{model}|\\text{data}) =\\dfrac{P(\\text{data}|\\text{model})P(\\text{model})}{P(\\text{data})}\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"terminology","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.4.4.1 Terminology:","text":"\\(P(\\text{hypothesis}|\\text{evidence})\\) = Posterior (probable\nhypothesis incorporating new evidence)\\(P(\\text{hypothesis}|\\text{evidence})\\) = Posterior (probable\nhypothesis incorporating new evidence)\\(P(\\text{evidence}|\\text{hypothesis})\\) = Likelihood (probable\nevidence , hypothesis true)\\(P(\\text{evidence}|\\text{hypothesis})\\) = Likelihood (probable\nevidence , hypothesis true)\\(P(\\text{hypothesis})\\) = Prior (probable hypothesis \nseeing evidence)\\(P(\\text{hypothesis})\\) = Prior (probable hypothesis \nseeing evidence)\\(P(\\text{evidence})\\) = Marginal (probable evidence \npossible hypotheses)\\(P(\\text{evidence})\\) = Marginal (probable evidence \npossible hypotheses)\\(\\dfrac{P(\\text{evidence}|\\text{hypothesis})}{P(\\text{evidence})}\\) =\nSupport \\(B\\) provides \\(\\)\\(\\dfrac{P(\\text{evidence}|\\text{hypothesis})}{P(\\text{evidence})}\\) =\nSupport \\(B\\) provides \\(\\)\\(P(\\text{data}|\\text{model})P(\\text{model})\\) = joint probability\n(\\(P(,B)\\))\\(P(\\text{data}|\\text{model})P(\\text{model})\\) = joint probability\n(\\(P(,B)\\))","code":""},{"path":"probability-theory-linear-algebra.html","id":"example-for-bayes-rule-using-covid-19-diagnostics","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.4.4.2 Example for Bayes Rule using COVID-19 Diagnostics","text":"\\[P(\\text{COVID-19}|\\text{cough}) =\\dfrac{P(\\text{cough}|\\text{COVID-19})P(\\text{COVID-19})}{P(\\text{cough})} = \\frac{0.7*0.01}{0.1}=0.07\\]\nEstimating \\(P(\\text{COVID-19}|\\text{cough})\\) difficult, \ncan outbreak number changes. However,\n\\(P(\\text{cough}|\\text{COVID-19})\\) stays stable, \\(P(\\text{COVID-19})\\) \n\\(P(\\text{cough})\\) can easily determined.","code":""},{"path":"probability-theory-linear-algebra.html","id":"further-concepts","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.5 Further Concepts","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"convergence-in-probability-of-random-variables","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.5.1 Convergence in Probability of Random Variables","text":"expect random variables (\\(X_i\\)) converge expected\nrandom variable \\(X\\). .e. looking infinite samples, \nprobability random variable \\(X_n\\) differs \nthreshold \\(\\epsilon\\) target \\(X\\) zero.\n\\[\\lim_{n \\rightarrow \\infty} P(|X_n - X| > \\epsilon) = 0\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"bernoullis-theorem-weak-law-of-large-numbers","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.5.2 Bernoulli’s Theorem / Weak Law of Large Numbers","text":"\\[\\lim_{n \\rightarrow \\infty} P(|\\frac{\\sum_{=1}^n X_i}{n} - \\mu| > \\epsilon) = 0,\\]\n\\(X_1,...,X_n\\) independent & identically distributed (..d.)\nRVs. \\(\\Rightarrow\\) enough samples, sample mean approach\ntrue mean. strong law large numbers states \n\\(|\\frac{\\sum_{=1}^n X_i}{n} - \\mu| < \\epsilon\\) \\(\\epsilon > 0\\).","code":""},{"path":"probability-theory-linear-algebra.html","id":"statistical-tests","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.6 Statistical tests","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"t-test","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.6.1 T-Test","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"z-test","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.6.2 Z-Test","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"chi-squared-test","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.6.3 Chi-Squared test","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"statistical-tests-in-bayesian-statistics","chapter":"1 Probability Theory & Linear Algebra","heading":"1.1.6.4 Statistical tests in Bayesian statistics","text":"","code":""},{"path":"probability-theory-linear-algebra.html","id":"linear-algebra","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2 Linear Algebra","text":"section meant give intuitive understanding \nunderlying mechanisms many algorithms. mainly summary \ncourse \n3Blue1Brown\n\ndeepai.org.details calculations see wikipedia.org.","code":""},{"path":"probability-theory-linear-algebra.html","id":"vectors","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1 Vectors","text":"two relevant perspectives us:Mathematical: Generally quantities expressed \nsingle number. objects vector space. objects\ncan also e.g. functions.Mathematical: Generally quantities expressed \nsingle number. objects vector space. objects\ncan also e.g. functions.Programmatical / Data: Vectors ordered lists numbers. \nmodel sample ordered list numbers numbers\nrepresent feature-value feature.Programmatical / Data: Vectors ordered lists numbers. \nmodel sample ordered list numbers numbers\nrepresent feature-value feature.vectors organized coordinate system commonly rooted\norigin (point \\([0,0]\\).","code":""},{"path":"probability-theory-linear-algebra.html","id":"linear-combinations","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1.1 Linear combinations","text":"create linear combinations vectors adding components\n(entries coordinate). Th points can reach linear\ncombinations called span vectors. vector lies \nspan another vector, linearly dependent.\ncan scale (stretch squish) vectors multiply vectors \nscalars (.e. numbers). vector length \\(1\\) called unit\nvector. unit vectors direction coordinate system \nbasis vectors. basis vectors stacked together form \nidentity matrix: matrix 1s diagonal. Since \nvalues diagonal also diagonal matrix.\\[\n= \\begin{bmatrix}\n1 \\quad 0 \\quad 0 \\\\\n0 \\quad 1 \\quad 0 \\\\\n0 \\quad 0 \\quad 1\n\\end{bmatrix}\n\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"linear-transformations","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1.2 Linear transformations","text":"Linear transformations functions move points around \nvector space, preserving linear relationships \npoints (straight lines stay straight, origin stays origin). \ninclude rotations reflections. can understand calculation \nlinear transformation point follows: give basis\nvectors new location. scale new location basis vectors \ncomponents respective dimension vector want \ntransform. take linear combination scaled, transformed\nbasis vectors:\\[\\begin{bmatrix} \\quad b \\\\ c \\quad d \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n  = x \\begin{bmatrix} \\\\ c \\end{bmatrix} + y \\begin{bmatrix} b \\\\ d \\end{bmatrix} = \\begin{bmatrix} x  + y b \\\\ x c + y d \\end{bmatrix}\n\\]likewise, can view matrix vector multiplication transformation\nspace. Full explanation: youtube.com -\n3Blue1Brown Multiplying two matrices\nrepresents sequential combination two linear transformations \nvector space.transpose \\(^T\\) matrix \\(\\) achieved mirroring matrix\ndiagonal therefore swapping rows columns. \ncommonly makes sense evaluating elements two matrices line \nregard scale. can also check matrices \northogonal.orthogonal/orthonormal matrix matrix holds\n\\(^TA=AA^T=\\), \\(\\) identity matrix. columns \northogonal matrices linearly independent .inverse matrix \\(^{-1}\\) matrix \\(\\) matrix \nyield transformation , multiplied \\(\\).","code":""},{"path":"probability-theory-linear-algebra.html","id":"determinants","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1.3 Determinants","text":"Determinants can used measure much linear combination\ncompresses stretches space. transformation inverts \nspace, determinant negative. determinant 0 means\ntransformation maps space onto lower dimension.dimensions come transformation/matrix rank.\npossible outputs matrix (span constructed \ncolumns) column space. vectors mapped 0 (onto\norigin) null space kernel matrix.Determinants can calculated square matrices. e.g.\n\\(3 \\times 2\\) matrix can viewed transformation mapping 2-D\n3-D space.dot product two vectors calculated like linear\ntransformation \\(1 \\times 2\\) matrix \\(2 \\times 1\\) matrix.\ntherefor maps onto 1-D Space can used measure \ncollinearity.cross product two vectors perpendicular vector \ndescribes parallelogram two vectors span. magnitude can\nseen area parallelogram. Beware: order \nvectors operation matters. cross product can expressed \ndeterminant. two vectors collinear perpendicular, cross\nproduct zero.","code":""},{"path":"probability-theory-linear-algebra.html","id":"system-of-equations","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1.4 System of equations","text":"Linear algebra can help solve systems equations.\\[\n\\begin{array}{ll} 1x+2y+3z=4 \\\\ 4x+5y+6z=-7 \\\\8x + 9y +0z = 1 \\end{array}\n\\quad  \\rightarrow  \\quad\n\\begin{bmatrix} 1 \\quad 2 \\quad 3 \\\\ 4 \\quad 5 \\quad 6 \\\\ 8 \\quad \\ 9 \\quad 0 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ z\\end{bmatrix} =\n\\begin{bmatrix} 4 \\\\ -7 \\\\ 1 \\end{bmatrix}\n\\quad \\rightarrow \\quad\n\\vec{x} = \\vec{v}\n\\]can imagine searching vector \\(\\vec{x}\\) land\n\\(\\vec{v}\\) transformation \\(\\).find \\(\\vec{x}\\) need inverse \\(\\):\\[\n^{-1}= \\begin{bmatrix} 1 \\quad 0 \\\\ 0 \\quad 1 \\end{bmatrix}\n\\]now multiply matrix equation \\(^{-1}\\) get:\\[\n^{-1} \\vec{x} = ^{-1} \\vec{v}\n\\quad \\rightarrow \\quad\n\\vec{x} = ^{-1} \\vec{v}\n\\]","code":""},{"path":"probability-theory-linear-algebra.html","id":"eigenvalues-and-eigenvectors","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1.5 Eigenvalues and Eigenvectors","text":"linear transformation \\(\\), eigenvectors \\(\\vec{v}\\) represent\nvectors stay span (keep orientation) \neigenvalues \\(\\lambda\\) scalars eigenvectors get\nscaled.\\[\n\\vec{v} = \\lambda \\vec{v}\n\\]Transforming \\(\\lambda\\) scaled identity matrix \\(\\) factoring \n\\(\\vec{v}\\), get: \\[\n(- \\lambda ) \\vec{v} =  \\vec{0}\n\\] tells us, transformation \\((- \\lambda )\\) needs map\nvector \\(\\vec{v}\\) onto lower dimension.eigenbasis \\(\\lambda \\) basis basis vectors \neigenvectors. sit diagonal basis matrix\n(\\(\\rightarrow\\) diagonal matrix).","code":""},{"path":"probability-theory-linear-algebra.html","id":"EV_Dec","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1.6 Eigenvalue decomposition","text":"eigen(value)decomposition decomposition matrix \nmatrix eigenvalues eigenvectors.\\[\nAU = U \\Lambda  \\quad \\rightarrow \\quad = U \\Lambda U^{-1}\n\\]\\(U\\) matrix eigenvectors \\(\\) \\(\\Lambda\\) \neigenbasis. Thus matrix operations can computed easily, since\n\\(\\Lambda\\) diagonal matrix.","code":""},{"path":"probability-theory-linear-algebra.html","id":"SVD1","chapter":"1 Probability Theory & Linear Algebra","heading":"1.2.1.7 Singular value decomposition","text":"Singular Value decomposition also applicable non-square\n\\(m \\times n\\)-matrix (\\(m\\) rows \\(n\\) columns). \nmatrix rank \\(r\\), can decompose \\[\n= U \\Sigma V^T\n\\] \\(U\\) orthogonal \\(m \\times r\\) matrix, \\(\\Sigma\\) \ndiagonal \\(r \\times r\\) matrix \\(V^T\\) orthogonal \\(r \\times n\\)\nmatrix. \\(U\\) contains left singular vectors, \\(V\\) right\nsingular vectors \\(\\Sigma\\) \\(Singular Values\\).\ndecomposition technique can used approximate original\nmatrix \\(\\) largest singular values. Thereby can save\ncomputation time matrix operations without loosing lot \ninformation.\napplications, please see SVD lower dimensional mapping.","code":""},{"path":"data-representation-analysis-processing.html","id":"data-representation-analysis-processing","chapter":"2 Data: Representation, Analysis & Processing","heading":"2 Data: Representation, Analysis & Processing","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"similarity-and-distance-measures","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1 Similarity and Distance Measures","text":"Choosing right distance measures important achieving good\nresults statistics, predictions clusterings.","code":""},{"path":"data-representation-analysis-processing.html","id":"metrics","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.1 Metrics","text":"distance measure called metric \\(d\\), following criteria\nneed fulfilled:Positivity: \\(d(x_1,x_2)≥0\\)Positivity: \\(d(x_1,x_2)≥0\\)\\(d(x_1,x_2)=0 \\text{ } x_1 = x_2\\)\\(d(x_1,x_2)=0 \\text{ } x_1 = x_2\\)Symmetry: \\(d(x_1, x_2) = d(x_2, x_1)\\)Symmetry: \\(d(x_1, x_2) = d(x_2, x_1)\\)Triangle inequality: \\(d(x_1, x_3) ≤ d(x_1, x_2) + d(x_2, x_3)\\)Triangle inequality: \\(d(x_1, x_3) ≤ d(x_1, x_2) + d(x_2, x_3)\\)may distance measures fulfill criteria, \nmetrics.","code":""},{"path":"data-representation-analysis-processing.html","id":"similarity-measures-on-vectors","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.2 Similarity measures on vectors","text":"measures used many objective functions compare data\npoints.available metrics sklearn : ‘cityblock’, ‘cosine’,\n‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’, scipy: ‘braycurtis’,\n‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’,\n‘kulsinski’, ‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,\n‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’\ninfo:\nscikit-learn.org","code":"from sklearn.metrics import pairwise_distances\nX1 = np.array([[2,3]])\nX2 = np.array([[2,4]])\npairwise_distances(X1,X2, metric=\"manhattan\")"},{"path":"data-representation-analysis-processing.html","id":"manhattan-distance","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.2.1 Manhattan distance","text":"distance sum absolute differences components\n(single coordinates) two points:\n\\[d(, B) = \\sum_{=1}^d | A_i - B_i |\\]info \nwikipedia.org.","code":""},{"path":"data-representation-analysis-processing.html","id":"hamming-distance","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.2.2 Hamming distance","text":"metric used pairs strings works equivalently \nManhattan distance. number positions different\nstrings.\ninfo \nwikipedia.org.","code":""},{"path":"data-representation-analysis-processing.html","id":"euclidian-distance","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.2.3 Euclidian distance","text":"\\[d(, B) = | - B | = \\sqrt{\\sum_{=1}^d (A_i-B_i)^2} \\]info euclidian distance \nwikipedia.org.\nusefulness metric can deteriorate high dimensional\nspaces. See curse \ndimensionality","code":""},{"path":"data-representation-analysis-processing.html","id":"chebyshev-distance","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.2.4 Chebyshev distance","text":"Chebyshev distance largest difference along \ncomponents two vectors.\\[d(, B) = \\max_i(|A_i-B_i|) \\]info \nwikipedia.org.","code":""},{"path":"data-representation-analysis-processing.html","id":"minkowski-distance","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.2.5 Minkowski Distance","text":"\\[d(, B) = (\\sum_{=1}^d |A_i-B_i|^p)^\\frac{1}{p} \\]\\(p=2\\) Minkowski distance equal Euclidian distance, \n\\(p=1\\) corresponds Manhattan distance converges \nChebyshev distance \\(p \\\\infty\\).   info \nwikipedia.org.","code":""},{"path":"data-representation-analysis-processing.html","id":"kernels","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3 Kernels","text":"Kernels functions output relationship points data. correspond mapping data high-dimensional space allow implicitly draw nonlinear decision boundaries linear models.","code":""},{"path":"data-representation-analysis-processing.html","id":"closure-properties-of-kernels","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.1 Closure properties of kernels","text":"\\(k_1\\) \\(k_2\\) kernels, \\(k_1 + k_2\\) kernel well.\\(k_1\\) \\(k_2\\) kernels, \\(k_1 + k_2\\) kernel well.\\(k_1\\) \\(k_2\\) kernels, product kernel well.\\(k_1\\) \\(k_2\\) kernels, product kernel well.\\(k\\) kernel \\(\\alpha\\) kernel, \\(\\alpha k\\) kernel well.\\(k\\) kernel \\(\\alpha\\) kernel, \\(\\alpha k\\) kernel well.define \\(k\\) set \\(D\\), points \\(D\\) value \\(k_0=0\\) still valid kernel.define \\(k\\) set \\(D\\), points \\(D\\) value \\(k_0=0\\) still valid kernel.","code":""},{"path":"data-representation-analysis-processing.html","id":"linear-kernel","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.2 Linear kernel","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"polynomial-kernel","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.3 polynomial kernel","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"gaussian-radial-basis-function-rbf-kernel","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.4 Gaussian Radial Basis Function (RBF) kernel","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"constant-kernel","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.5 constant kernel","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"delta-dirac-kernel","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.6 delta dirac kernel","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"r-convolution-kernels","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.7 R convolution kernels","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"string-kernels","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.1.3.8 String kernels","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"data-analysis","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2 Data Analysis","text":"Data analysis conducted iteratively get hold data, cleaned , processed analyse outputs model.","code":""},{"path":"data-representation-analysis-processing.html","id":"exploratory-data-analysis","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.1 Exploratory data analysis","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"initial-analysis","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.1.1 Initial analysis","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"after-preprocessing","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.1.2 After preprocessing","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"univariate-analysis","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.1.2.1 Univariate Analysis","text":"Analyse one attribute.","code":""},{},{},{"path":"data-representation-analysis-processing.html","id":"multivariate-analysis","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.1.2.2 Multivariate Analysis","text":"","code":""},{},{},{},{"path":"data-representation-analysis-processing.html","id":"output-analysis","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.2 Output Analysis","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"ethical-ai","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.2.1 Ethical AI","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"explanation-methods","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.2.2 Explanation methods","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"performance","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.2.3 Performance","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"monitoring","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.2.3 Monitoring","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"preprocessing-data","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3 Preprocessing data","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"handling-missing-values","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.1 Handling missing values","text":"algorithms assume features samples numerical\nvalues. cases missing values imputed (.e. inferred)\n(affordable) samples missing feature values can \ndeleted data set.","code":""},{"path":"data-representation-analysis-processing.html","id":"iterative-imputor-by-sklearn","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.1.1 Iterative imputor by sklearn","text":"features missing values, imputor imputes missing\nvalues modelling feature using existing values \nfeatures. uses several iterations results converge.! method scales \\(O(nd^3)\\), \\(n\\) number \nsamples \\(d\\) number features.info:\nscikit-learn.org","code":"from sklearn.experimental import enable_iterative_imputer # necessary since the imputor is still experimental\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor \nrf_estimator = RamdomForestRegressor(n_estimators = 8, max_depth = 6, bootstrap = true)\nimputor = IterativeImputer(random_state=0, estimator = rf_estimator, max_iter = 25)\nimputor.fit_transform(X)"},{"path":"data-representation-analysis-processing.html","id":"deleting-missing-values","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.1.2 Deleting missing values","text":"info:\npandas.pydata.org","code":"import pandas as pd\ndf.dropna(how=\"any\") # how=\"all\" would delete a sample if all values were missing"},{"path":"data-representation-analysis-processing.html","id":"date-and-time","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.2 Date and time","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"converting-to-datetime-format","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.2.1 Converting to datetime format","text":"","code":"import pandas as pd\npd.to_datetime(df.date_col, infer_datetime_format=True)"},{"path":"data-representation-analysis-processing.html","id":"create-columns-for-year-month-day","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.2.2 Create columns for year, month, day","text":"","code":"import pandas as pd\ndf['year'] = df.Date.dt.year\ndf['month'] = df.Date.dt.month\ndf['day'] = df.Date.dt.day"},{"path":"data-representation-analysis-processing.html","id":"replacing-data","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.3 Replacing data","text":"","code":"import pandas as pd\ndf.Col.apply(lambda x: 0 if x=='zero' else 1)"},{"path":"data-representation-analysis-processing.html","id":"encoding-data","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4 Encoding data","text":"multiple ways encode data, especially non-vectorized data, make suitable machine learning algorithms.","code":""},{"path":"data-representation-analysis-processing.html","id":"encoding-categorical-features","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.1 Encoding categorical features","text":"string values (e.g. “male”, “female”) features \nconverted integers. can done two methods:","code":""},{"path":"data-representation-analysis-processing.html","id":"ordinal-encoding","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.1.1 Ordinal Encoding","text":"integer assigned category (e.g. “male”=0, “female”=1)info:\nscikit-learn.org\nmethod useful categories ordered relationship\n(e.g. “bad”, “medium”, “good”). case (e.g. “dog”,\n“cat”, “bunny”) avoided since algorithm might deduct\nordered relationship none. cases\none-hot-encoding used.","code":"from sklearn.preprocessing import OrdinalEncoder\nord_enc = preprocessing.OrdinalEncoder()\nord_enc.fit(X)\nord_enc.transform(X)"},{"path":"data-representation-analysis-processing.html","id":"one-hot-encoding","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.1.2 One-Hot Encoding","text":"One-hot encoding assigns separate feature-column category \nencodes binarily (e.g. sample dog, 1 \ndog-column 0 cat bunny column).info:\nscikit-learn.org","code":"from sklearn.preprocessing import OneHotEncoder\nonehot_enc = OneHotEncoder(handle_unknown='ignore')\nonehot_enc.fit(X)\nonehot_enc.transform(X)"},{"path":"data-representation-analysis-processing.html","id":"graph-representation-of-data","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.2 Graph representation of data","text":"similarity/distance points can represented graphs. data points represented nodes, distances/similarities edges.","code":""},{"path":"data-representation-analysis-processing.html","id":"encoding-text-data","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.3 Encoding Text data","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"bag-of-words","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.3.1 Bag of words","text":"","code":""},{},{},{},{"path":"data-representation-analysis-processing.html","id":"encoding-image-data","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.4 Encoding image data","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"patches","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.4.4.1 Patches","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"standardization","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.5 Standardization","text":"Many machine learning models assume features centered\naround 0 similar variance. Therefore data \ncentered scaled unit variance training prediction.info:\nscikit-learn.orgAnother option scaling normalization. used, \nvalues fall strictly max min value.\ninfo:\nscikit-learn.org","code":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(input_df)"},{"path":"data-representation-analysis-processing.html","id":"splitting-in-training--and-test-data","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.6 Splitting in training- and test-data","text":"need split training set test- training-samples. \nalgorithm uses training samples known label/target value\nfitting parameters. test-set used determine \ntrained algorithm performs well new samples well. need give\nspecial considerations following points:Avoiding data information leak training set \ntest-setAvoiding data information leak training set \ntest-setValidating predictive performance deteriorates time\n(.e. algorithm perform worse new samples). \nespecially important models make predictions future\nevents.Validating predictive performance deteriorates time\n(.e. algorithm perform worse new samples). \nespecially important models make predictions future\nevents.Conversely, sampling test- training-sets randomly avoid\nintroducing bias two sets.Conversely, sampling test- training-sets randomly avoid\nintroducing bias two sets.info:\nscikit-learn.org","code":"# assuming you already imported the data and separated the label column:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"},{"path":"data-representation-analysis-processing.html","id":"feature-selection","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.7 Feature selection","text":"Usually label depend available features. detect\ncausal features, remove noisy ones reduce running training\ncosts algorithm, reduce amount features relevant\nones. can done priori (training) using wrapper\nmethods (integrated prediction algorithm used).! methods feature selection already built-,\ndecision trees.","code":""},{"path":"data-representation-analysis-processing.html","id":"a-priori-feature-selection","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.7.1 A priori feature selection","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"low-variance-threshold","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.7.1.1 Low variance threshold","text":"cheap method remove features variance \nthreshold.info:\nscikit-learn.org","code":"from sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(threshold=0.1)\nselector.fit_transform(X)"},{"path":"data-representation-analysis-processing.html","id":"mutual_info","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.7.1.2 Mutual information","text":"method works choosing features highest\ndependency features label.\\[ (X, Y) =D_{KL} \\left( P(X=x, Y=y), P(X=x) \\otimes P(Y=y) \\right) =\\sum_{y \\Y} \\sum_{x \\X}\n    { P(X=x, Y=y) \\log\\left(\\frac{P(X=x, Y=y)}{P(X=x)P(Y=y)}\\right) }\\], \\(D_{KL}\\) Kullback–Leibler\ndivergence\n(measure similarity distributions). \\(\\log\\)-Term \nquantifying different joint distribution product \nmarginal distributions.[info:\\\\](info:\\){.uri}\nscikit-learn.orgwikipedia.org/wiki/Mutual_information","code":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif # for regression use mutual_info_regression\nX_new = SelectKBest(mutual_info_classif, k=8).fit_transform(X, y)"},{"path":"data-representation-analysis-processing.html","id":"wrapper-methods","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.7.2 wrapper methods","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"greedy-feature-selection","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.7.2.1 Greedy feature selection","text":"Using greedy feature selection wrapper method, one commonly starts\n0 features adds feature returns highest score \nused classifier.info:\nscikit-learn.org","code":"from sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier()\nselector = SequentialFeatureSelector(classifier, n_features_to_select=8)\nselector.fit_transform(X, y)"},{"path":"data-representation-analysis-processing.html","id":"advice-pitfalls","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.7.3 Advice & Pitfalls","text":"Selected advice paper Guyon \nElisseeff:domain knowledge: Use .domain knowledge: Use .features commensurate (proportion): Normalize .features commensurate (proportion): Normalize .suspect interdependent features: Construct conjunctive\nfeatures products features.suspect interdependent features: Construct conjunctive\nfeatures products features.advice:Features useless , can useful combination\nfeatures.Features useless , can useful combination\nfeatures.Using multiple redundant variables can useful reduce noise.Using multiple redundant variables can useful reduce noise.also models (e.g. lasso regression, decision trees) \nfeature selection built model (.e. allowing\ncertain number features used penalizing use \nadditional features).also models (e.g. lasso regression, decision trees) \nfeature selection built model (.e. allowing\ncertain number features used penalizing use \nadditional features).","code":""},{"path":"data-representation-analysis-processing.html","id":"hyper-parameter-tuning","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.8 Hyper-parameter tuning","text":"hyper-parameters (e.g. kernel, gamma, number nodes tree) \ntrained algorithm . outer loop hyper-parameter tuning\nneeded find optimal hyper parameters.! strongly recommended separate another validation set \ntraining set hyper-parameter tuning (’ll end \ntraining-, validation- test-set). See Cross Validation\nbest practice.","code":""},{"path":"data-representation-analysis-processing.html","id":"grid-search","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.8.1 Grid search","text":"classic approach exhaustive grid search: create grid \nhyperparameters iterate combinations. combination \nbest score used end. approach causes big\ncomputational costs due combinatorial explosion.","code":""},{"path":"data-representation-analysis-processing.html","id":"randomized-search","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.8.2 Randomized search","text":"approach used, many combinations \nhyper-parameters tuning. allocate budget iterations \ncombinations parameters sampled randomly according \ndistributions provide.want evaluate large set hyperparameters, can use \nhalving strategy: tune large combination parameters \nresources (e.g. samples, trees). best performing half candidates\nre-evaluated twice many resources. continues \nbest-performing candidate evaluated full amount resources.info:\nscikit-learn.org","code":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.experimental import enable_halving_search_cv  # since this method is still experimental\nfrom sklearn.model_selection import HalvingRandomSearchCV\nfrom sklearn.utils.fixes import loguniform\n\nrf_clf = RandomForestClassifier()\n\nparam_distributions = {\"max_depth\": [3, None],\n                       \"min_samples_split\": loguniform(1, 10)}\nhypa_search = HalvingRandomSearchCV(rf_clf, param_distributions,\n                               resource='n_estimators',\n                               max_resources=10,\n                               n_jobs=-1, # important since hyper-parameter tuning is very costly\n                               scoring = 'balanced_accuracy',\n                               random_state=0).fit(X, y)"},{"path":"data-representation-analysis-processing.html","id":"model-selection","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.9 Model selection","text":"candidates hyper-parameters must evaluated \ndata trained (-fitting risk). Thus, separate\nanother data-set training data: validation set. \nreduces amount training data drastically. Therefore use \napproaches Cross Validation Bootstrapping.","code":""},{"path":"data-representation-analysis-processing.html","id":"crossval","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.9.1 Cross Validation","text":"k-fold Cross Validation, split training set k sub-sets.\ntrain samples k-1 sub-sets validate using data \nremaining sub-set. iterate validated \nsub-set . average k scores obtain.Schema process 5-fold Cross Validation. data first\nsplit training- test-data. training data split 5\nsub-sets. algorithm trained 4 sub-sets evaluated \nremaining sub-set. sub-set used validation . Source:\nscikit-learn.org.info:\nscikit-learn.org! time-series data (clearly ..d.) data,\nuse special cross-validation\nstrategies.\n\nstrategies\nworth considering.","code":"from sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nSVM_clf = svm.SVC (kernel='polynomial')\ncv_scores = cross_val_score(SVM_clf, X, y, cv = 7)\ncv_score = cv_scores.mean()"},{"path":"data-representation-analysis-processing.html","id":"bootstrapping","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.3.9.2 Bootstrapping","text":"Instead splitting data k subsets, can also just sample\ndata training validation sets.\ninfo:\nwikipedia.org.","code":""},{"path":"data-representation-analysis-processing.html","id":"errors-regularization","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4 Errors & regularization","text":"irreducible errors reducible errors. Irreducible errors\nstem unknown variables variables data . Reducible\nerrors deviations model desired behavior can \nreduced. Bias variance reducible errors.\\[\\text{Error} = \\text{Bias} + \\text{Var} + \\text{irr. Error}\\]","code":""},{"path":"data-representation-analysis-processing.html","id":"bias-and-variance","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4.1 Bias and Variance","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"bias-of-an-estimator","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4.1.1 Bias of an estimator","text":"Bias tells model oversimplifies true relationship \ndata (underfitting).\nmodel parameter \\(\\hat{\\theta}\\) estimator\ntrue \\(\\theta\\). want know whether model - \nunderestimates true \\(\\theta\\) systematically.\\[\\text{Bias}[\\hat{\\theta}]=\\text{E}_{X|\\mathcal{D}}[\\hat{\\theta}]- \\theta\\]E.g. parameter captures polynomial model / relationship\ndata , high value means model \nunderfitting.info:\nwikipedia.org","code":""},{"path":"data-representation-analysis-processing.html","id":"variance-of-an-estimator","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4.1.2 Variance of an estimator","text":"Variance tells model learns noise instead true\nrelationship data (overfitting).\\[\\text{Var}[\\hat{\\theta}]=\\text{E}_{X|\\mathcal{D}}[(\\text{E}_{X|\\mathcal{D}}[\\hat{\\theta}]- \\hat{\\theta})^2]\\]\n.e. bootstrap data, show much \nparameter jump around mean, learns different\nsampled sets.goal now find sweet spot biased (simple\nmodel) model high variance (complex model).Relationship bias, variance total error. minimum\ntotal error lies best compromise bias \nvariance. Source: User Bigbossfarin \nwikimedia.org..info:\nwikipedia.org","code":""},{"path":"data-representation-analysis-processing.html","id":"regularization","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4.2 Regularization","text":"combat overfitting, can introduce term loss-function\npenalizes complex models. linear regression, regularized\nloss function :\\[\\min L(\\hat{y},y)= \\min_{W,b} f(WX+b,y)+\\lambda R(W)\\] \\(f\\) \nunregularized loss function, \\(W\\) weight matrix, \\(X\\) \nsample matrix \\(b\\) bias offset term model (bias term\n\\(\\neq\\) bias estimator!). \\(R\\) regularization function \n\\(\\lambda\\) parameter controlling strength.\n.e. regularized loss function punishes large weights \\(W\\) leads\nflatter/smoother functions.info:\nwikipedia.org","code":""},{"path":"data-representation-analysis-processing.html","id":"bagging","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4.3 Bagging","text":"Train several instances complex estimator (aka. strong learner,\nlike large decision trees KNN small radius) subset \ndata. use majority vote average scores classifying \nget final prediction. training different subsets averaging\nresults, chances overfitting greatly reduced.info:\nscikit-learn.orgA classic example bagging classifier Random Forest\nClassifier\nvariant Extremely Randomized\nTrees\nreduces variance increases bias.","code":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nbagging = BaggingClassifier(KNeighborsClassifier(), max_features=0.5, n_estimators=20)"},{"path":"data-representation-analysis-processing.html","id":"boosting","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4.4 Boosting","text":"Compared bagging, use weak learners trained\nindependently . start single weak learner (e.g. \nsmall decision tree) repeat following steps:Add additional model train .Increase weights training samples falsely classified,\ndecrease weights correctly classified samples. (used \nnext added model.)Reweight results models combined model reduce \ntraining error.final model weighted ensemble weak classifiers.\npopular ones gradient boosted decision\ntree\nalgorithms.","code":""},{"path":"data-representation-analysis-processing.html","id":"stacking","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.4.5 Stacking","text":"Stacking closely resembles bagging: ensemble separately trained\nbase models used create ensemble model. However, continuous\n(instead discrete) outputs commonly fewer heterogeneous models\n(instead type models) used. continuous outputs \nfed final estimator (commonly logistic regression\nclassifier).info:\nscikit-learn.org","code":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import StackingClassifier\n\nclassifiers = [\n    ('svc', SVC()),\n    ('knn', KNeighborsClassifier()),\n    ('dtc', DecisionTreeClassifier())\n    ]\n    \nclf = StackingClassifier(\n    classifiers=estimators, final_estimator=LogisticRegression()\n    )\n\nclf.fit(X, y)"},{"path":"data-representation-analysis-processing.html","id":"tips-for-machine-learning-projects","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.5 Tips for machine learning projects","text":"","code":""},{"path":"data-representation-analysis-processing.html","id":"general-advice","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.5.1 General advice","text":"General advice machine learning Pedro\nDomingos:Let knowledge problem help choose candidate\nalgorithms. E.g. know rules comparing samples makes\nsense \\(\\rightarrow\\) Choose instance based learners. know\nstatistical dependencies relevant \\(\\rightarrow\\) choose\nGraph based models.Let knowledge problem help choose candidate\nalgorithms. E.g. know rules comparing samples makes\nsense \\(\\rightarrow\\) Choose instance based learners. know\nstatistical dependencies relevant \\(\\rightarrow\\) choose\nGraph based models.Don’t underestimate impact feature engineering: Many domain\nspecific features can boost accuracy.Don’t underestimate impact feature engineering: Many domain\nspecific features can boost accuracy.Get samples candidate features (instead focussing \nalgorithm)Get samples candidate features (instead focussing \nalgorithm)Don’t confuse correlation causation. Just model\ncan predict something, mean features cause \ntarget thus easily deduct clear action .Don’t confuse correlation causation. Just model\ncan predict something, mean features cause \ntarget thus easily deduct clear action .","code":""},{"path":"data-representation-analysis-processing.html","id":"common-mistakes","chapter":"2 Data: Representation, Analysis & Processing","heading":"2.5.2 Common mistakes","text":"aware: list never capture everything can go wrong. ;-)Data\nLeakage:\nInformation Samples test data leaked \ntraining data.\ndeleted duplicates beforehand\nfalsely assumed samples drawn independently\nsampled training set randomly. (E.g. multiple\nsamples patient, time series data)\nclass label encoded training features \nway find “Nature”.\njust used wrong training / test set programming.\nfeature engineering like finding n-grams Max, Min \ndata using test-set data.\nRemedy: Careful preliminary data analysis, deduplication,\nData\nLeakage:\nInformation Samples test data leaked \ntraining data.deleted duplicates beforehandYou falsely assumed samples drawn independently\nsampled training set randomly. (E.g. multiple\nsamples patient, time series data)class label encoded training features \nway find “Nature”.just used wrong training / test set programming.feature engineering like finding n-grams Max, Min \ndata using test-set data.Remedy: Careful preliminary data analysis, deduplication,Using wrong quality measures unbalanced data: E.g. Accuracy \nunbalanced data reasonable quality measure.Using wrong quality measures unbalanced data: E.g. Accuracy \nunbalanced data reasonable quality measure.Inconsistent\npreprocessing:\npreprocess training data certain way, \ntest- prediction-data.\nRemedy: Use one preprocessing pipeline can use \ntraining, testing prediction.\nInconsistent\npreprocessing:\npreprocess training data certain way, \ntest- prediction-data.Remedy: Use one preprocessing pipeline can use \ntraining, testing prediction.Curse dimensionality:\nuse many features amount samples \n\ndistance measure suitable high-dimensional space\n(e.g. Hamming distance, Euclidean distance)\nRemedy: Use lower-dimensional mapping, feature selection.\nCurse dimensionality:use many features amount samples \nhaveYour distance measure suitable high-dimensional space\n(e.g. Hamming distance, Euclidean distance)Remedy: Use lower-dimensional mapping, feature selection.Overfitting:\nuse complex algorithm (many degrees freedom)\namount data \nmany features\nRemedy: Get samples, reduce dimensionanlity,\nfeature selection, regularization, bagging, boosting, stacking.\nOverfitting:use complex algorithm (many degrees freedom)\namount data haveYou many featuresRemedy: Get samples, reduce dimensionanlity,\nfeature selection, regularization, bagging, boosting, stacking.Bad Data:\ndata representative find \n“real world”. (skewed population, old data, specific\nsensors, locations…)\nmany missing values among features.\ndata remotely linked target\nwant predict.\nerroneous entries data.\nRemedy: Clean data source, impute data, clean data \npreprocessing, get representative data, limit scope \napplication.\nBad Data:data representative find \n“real world”. (skewed population, old data, specific\nsensors, locations…)many missing values among features.data remotely linked target\nwant predict.erroneous entries data.Remedy: Clean data source, impute data, clean data \npreprocessing, get representative data, limit scope \napplication.","code":""},{"path":"classification-methods.html","id":"classification-methods","chapter":"3 Classification Methods","heading":"3 Classification Methods","text":"Classification assignment objects (data points) categories\n(classes). requires data set (.e. training set) points \nknown class labels. class labels known can instead\ngroup data using clustering algorithms (chapter\n3).","code":""},{"path":"classification-methods.html","id":"evaluation-of-classifiers","chapter":"3 Classification Methods","heading":"3.1 Evaluation of Classifiers","text":"","code":""},{"path":"classification-methods.html","id":"confusion-matrix","chapter":"3 Classification Methods","heading":"3.1.1 Confusion matrix","text":"gives quick overview distribution true positives\n(\\(TP\\)), false positives (\\(FP\\)) , \\(TN\\) true negatives, \\(FN\\) false\nnegatives.","code":""},{"path":"classification-methods.html","id":"basic-quality-measures","chapter":"3 Classification Methods","heading":"3.1.2 Basic Quality Measures","text":"\\(\\text{Accuracy / Success Rate} = \\frac{ \\text{correct predictions}}{\\text{total predictions}} = \\frac{ \\text{TP}+\\text{TN}}{\\text{TP} + \\text{TN}+\\text{FP}+\\text{FN}}\\)\nmetric used pure form, number\npositive negative samples balanced.\\(\\text{Accuracy / Success Rate} = \\frac{ \\text{correct predictions}}{\\text{total predictions}} = \\frac{ \\text{TP}+\\text{TN}}{\\text{TP} + \\text{TN}+\\text{FP}+\\text{FN}}\\)\nmetric used pure form, number\npositive negative samples balanced.\\(\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\\)\n.e. many positive predictions actually positive?\\(\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\\)\n.e. many positive predictions actually positive?\\(\\text{True positive rate / Recall / Sensitivity} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\\)\n.e. many positive samples catch?\\(\\text{True positive rate / Recall / Sensitivity} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\\)\n.e. many positive samples catch?\\(\\text{True negative rate / Specificity / Selectivity} = \\frac{\\text{TN}}{\\text{TN}+\\text{FP}}\\)\n.e. many negative samples catch negative\n(.e. truly negative)?\\(\\text{True negative rate / Specificity / Selectivity} = \\frac{\\text{TN}}{\\text{TN}+\\text{FP}}\\)\n.e. many negative samples catch negative\n(.e. truly negative)?\\(\\text{F-score} = 2 \\frac{\\text{precision} * \\text{recall}}{\\text{precision}+\\text{recall}}\\)\nuseful cases unbalanced classes balance \ntrade-precision recall.\\(\\text{F-score} = 2 \\frac{\\text{precision} * \\text{recall}}{\\text{precision}+\\text{recall}}\\)\nuseful cases unbalanced classes balance \ntrade-precision recall.","code":""},{"path":"classification-methods.html","id":"area-under-the-curve","chapter":"3 Classification Methods","heading":"3.1.3 Area under the Curve","text":"class measures represents quality classifier \ndifferent threshold values \\(\\theta\\) calculating area \ncurve spanned different quality measures.","code":""},{"path":"classification-methods.html","id":"area-under-the-receiver-operating-characteristics-curve-auroc-or-auc","chapter":"3 Classification Methods","heading":"3.1.3.1 Area under the Receiver Operating Characteristics Curve (AUROC or AUC)","text":"AUC can interpreted follows: classifier gets \npositive negative point, AUC shows probability \nclassifier give higher score positive point. perfect\nclassifier AUC 1, AUC 0.5 represents random guessing.! measure sensitive class imbalance!Area precision recall curve. Source: user cmglee \nwikipedia.org","code":""},{"path":"classification-methods.html","id":"area-under-the-precision-recall-curve-auprc-average-precision-avep","chapter":"3 Classification Methods","heading":"3.1.3.2 Area under the Precision-Recall Curve (AUPRC) / Average Precision (AveP)","text":"measure can used unbalanced data sets. represents \naverage precision function recall. value 1 represents\nperfect classifier.precision-recall curve. Source:\nscikit-learn.org","code":""},{"path":"classification-methods.html","id":"handling-unbalanced-data","chapter":"3 Classification Methods","heading":"3.1.4 Handling Unbalanced Data","text":"many samples one class others training\ncan lead high accuracy values event though classifier performs\npoorly smaller classes. can handle unbalance :-sampling smaller data set (creating artificial samples\nclass)-sampling smaller data set (creating artificial samples\nclass)giving weight samples smaller data setgiving weight samples smaller data setusing quality measure sensitive class imbalanceusing quality measure sensitive class imbalanceOversampling using imbalanced-learn (see: )Sensitivity, specificity, precision, recall, support F-score","code":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nfeatures_resampled, labels_resampled = ros.fit_resample(df[feature_cols], df[label_col])y_true = df[label_col]\ny_pred = classifier.predict(df[feature_cols])\n\nfrom imblearn.metrics import sensitivity_specificity_support\nsensitivity, specificity, support = sensitivity_specificity_support(y_true, y_pred) \n\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred) "},{"path":"classification-methods.html","id":"classification-algorithms","chapter":"3 Classification Methods","heading":"3.2 Classification Algorithms","text":"","code":""},{"path":"classification-methods.html","id":"nearest-neighbors-classifier","chapter":"3 Classification Methods","heading":"3.2.1 Nearest Neighbors Classifier","text":"classifier predicts class label using common class\nlabel \\(k\\) nearest neighbors training data set.Pros:Classifier take time training.Can learn complex decision boundaries.Cons:prediction time consuming scales n.scikit-learn.org","code":"from sklearn import KNeighborsClassifier\nkn_model = KNeighborsClassifier(n_neighbors=5)\nkn_model.fit(X, y)\nkn_model.predict([[5,1]])"},{"path":"classification-methods.html","id":"naive-bayes-classifier","chapter":"3 Classification Methods","heading":"3.2.2 Naive Bayes Classifier","text":"Naive Bayes classifies works assumption features \nconditionally independent given class label. every point \nsimplified version Bayes rule used:\\[P(Y=y_i|X=x) \\propto P(X=x|Y=y_i) * P(Y=y_i) \\] \\(Y\\) RV\nclass label \\(X\\) RV contains feature values.\nholds since \\(P(X=x)\\) classes. Since \ndifferent features \\(X_j\\) assumed independent can \nmultiplied . label \\(y_i\\) highest probability \npredicted class label:\n\\[\\arg \\max_{y_i} P(Y=y_i|X=x) \\propto P(Y=y_i) \\prod_{j=1}^{d} P(X_j=x_j|Y=y_i)  \\]\nOne usually estimates value \\(P(Y)\\) ferquency \ndifferent classes training data assumes classes \nequally likely.\nestimate \\(P(X_j=x_j|Y=y_i)\\) following distributions \ncommonly used: - binary features: Bernoulli distribution - \ndiscrete features: Multinomial distribution - continuous features:\nNormal / Gaussian distributionFor discrete features, need use smoothing\nprior\n(add 1 every feature count) avoid 0 probabilities samples \nfeatures 0 training data.Pros:Naive Bayes training fast.Naive Bayes training fast.Combine descrete continuous features since different\ndistribution can used feature.Combine descrete continuous features since different\ndistribution can used feature.can straight decision boundaries (classes \nvariance), circular decision boundaries (different variance, \nmean) parabolic decision boundaries (different mean, different\nvariance).can straight decision boundaries (classes \nvariance), circular decision boundaries (different variance, \nmean) parabolic decision boundaries (different mean, different\nvariance).Cons:probability estimates Naive Bayes usually\nbad.probability estimates Naive Bayes usually\nbad.independence assumption features usually \ngiven real life.independence assumption features usually \ngiven real life.info:\nscikit-learn.org","code":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X, y)"},{"path":"classification-methods.html","id":"linear-discriminant-analysis-lda","chapter":"3 Classification Methods","heading":"3.2.3 Linear discriminant analysis (LDA)","text":"Contrary Naive Bayes, features LDA assumed \nindependently distributed. Bayes rule distribution \nclass calculated according Bayes rule. \\(P(X=x|Y=y_i)\\) modeled\nmultivariate Gaussian distribution. Gaussians class\nassumed . log-posterior can simplified :\\[log(P(y=y_i|x) = - \\frac{1}{2} (x-\\mu_i)^t \\Sigma^{-1}(x-\\mu_i)+\\log P(y=y_i)\\]\n\\(\\mu_i\\) mean class \\(\\), \\((x-\\mu_i)^t \\Sigma^{-1}(x-\\mu_i)\\)\ncorresponds Mahalanobis\ndistance. Thus, \nassign point class whose distribution closest .\nLDA can also thought projecting data space \\(k-1\\)\ndimensions (\\(k\\) number classes). info:\nwikipedia.org.\ncan also used dimensionality reduction method.info:\nscikit-learn.org","code":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nclf = LinearDiscriminantAnalysis()\nclf.fit(X, y)"},{"path":"classification-methods.html","id":"support-vector-classifier-svc","chapter":"3 Classification Methods","heading":"3.2.4 Support Vector Classifier (SVC)","text":"SVCs use hyperplanes separate data points according class\nlabel maximum margin (\\(M\\)) separating hyperplane\n(\\(x^T\\beta + \\beta_0=0\\)) points. points perfectly\nseparated decision boundary, soft margin SVM used \nslack variable \\(\\xi\\) punishes points margin wrong\nside hyperplane. optimization problem given \n[@Hastie2009] : \\[\\begin{split}\n            \\max_{\\beta, \\beta_0, \\beta=1} M, \\\\\n            \\text{subject } y_i(x_i^T \\beta + \\beta_0) \\ge 1 - \\xi_i, \\quad \\forall , \\\\\\xi_i \\ge 0, \\quad \\sum \\xi_i \\le constant, \\quad = 1, ..., N,\n        \\end{split}\\] \\(\\beta\\) coefficients \\(x\\) \n\\(N\\) data points. support vectors points determine \norientation hyperplane (.e. closest points). \nclassification function given :\n\\[G(x) = \\text{sign}[x^T\\beta + \\beta_0]\\] calculate \ninner part function can get distance point \nhyperplane (SKlearn need divide norm vector \\(w\\) \nhyperplane get true distance). get probability point\nclass, can use Platt’s algorithm [@Platt1999]. SVMs \nsensitive scaling features. Therefore, data \nnormalized classification.","code":"from sklearn import svm\n# train the model\nsvc_model = svm.SVC()\nsvc_model.fit(train_df[feautre_cols], train_df[label_col])\n# test the model\ny_predict = svc_model.predict(test_df[feature_cols])"},{"path":"classification-methods.html","id":"decision-trees","chapter":"3 Classification Methods","heading":"3.2.5 Decision Trees","text":"decision tree uses binary rules recursively split data \nregions contain single class.Decision trees work like flowcharts root (top node),\nbranches (possible outcomes test), nodes (tests one attribute),\nleafs (class labels). one shows decision tree \nclassifier predicting patient survives sinking Titanic.\nSource: user Gilgoldm \nwikipedia.orgPros:Interpretable results, trees big.Interpretable results, trees big.Cheap train predict.Cheap train predict.Can handle categorical continuous data time.Can handle categorical continuous data time.Can used multi-output\nproblems\n(e.g. color shape object).Can used multi-output\nproblems\n(e.g. color shape object).Cons:Overfitting risksSome concepts hard learn (X-relationships, Decision\nboundaries smooth)Unstable predictions: Small changes data can lead vastly\ndifferent decision trees.Tips: - PCA helps tree find separating features. -\nVisualize produced tree. - Setting lower boundary \nsplit-sizes data, reduces chance overfitting. - Balance\ndataset weight samples according class sizes avoid\nconstructing biased trees.info:\nscikit-learn.orgDue overfitting risk instability, ensembles decision\ntrees commonly used.","code":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=10, min_samples_split=0.01, class_weight=\"balanced\")\nclf = clf.fit(X, Y)"},{"path":"classification-methods.html","id":"random-forests","chapter":"3 Classification Methods","heading":"3.2.5.1 Random forests","text":"Random forests version bagging classifier\nemploying decision trees. reduce variance, separate trees can\nassigned limited number features well.info:\nscikit-learn.org","code":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=10, max_features=\"sqrt\", class_weight=\"balanced\")\nclf.fit(X, y)"},{"path":"classification-methods.html","id":"gradient-boosted-decision-trees","chapter":"3 Classification Methods","heading":"3.2.5.2 Gradient boosted decision trees","text":"Gradient boosted decision tree models form \nboosting employing decision trees.info: lightgbm\ndocumentation,\nParameter\ntuning,Parameter\ntuning\nSimilar model:\nscikit-learn.org","code":"import lightgbm as lgbm\nclf = lgbm.LGBMClassifier(class_weight= \"balanced\")\nclf.fit(X, y)"},{"path":"unsupervised-learning.html","id":"unsupervised-learning","chapter":"4 Unsupervised Learning","heading":"4 Unsupervised Learning","text":"","code":""},{"path":"unsupervised-learning.html","id":"custering-methods","chapter":"4 Unsupervised Learning","heading":"4.1 Custering Methods","text":"Clustering methods used group data class labels \npresent. thereby want learn intrinsic structure data.","code":""},{"path":"unsupervised-learning.html","id":"metrics-for-clustering-algorithms","chapter":"4 Unsupervised Learning","heading":"4.1.1 metrics for Clustering algorithms","text":"","code":""},{"path":"unsupervised-learning.html","id":"silhouette-coefficient","chapter":"4 Unsupervised Learning","heading":"4.1.1.1 Silhouette coefficient","text":"silhouette coefficient compares average distance point \npoints cluster \\(d(x,\\mu_{C})\\) average distance\npoint points second nearest Cluster\n\\(d(x,\\mu_{C'})\\).\\[s(x) = \\frac{d(x,\\mu_{C'})-d(x,\\mu_{C})}{\\max(d(x,\\mu_{C}), d(x,\\mu_{C'}))}\\]\n\\(C\\) cluster \\(C'\\) second nearest cluster. \npoint clearly cluster, \\(s(x)\\) close \\(1\\). \npoint two clusters, \\(s(x)\\) close 0. point \ncloser another cluster, \\(s(x)\\) negative.\nvarying number clusters, one can find number \nhighest silhouette coefficients.Pros:score high dense highly separated clusters.Cons:silhouette coefficient mainly suitable convex clusters,\nsince gives high values kind clusters.info:\nscikit-learn.orgA faster alternative Davies-Bouldin\nscore,\nvalues closer 0 indicate better clustering.","code":"from sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nclu = KMeans(n_clusters=4)\nclu.fit(X)\nlabels = clu.labels_\nsilhouette_score(X, labels, metric='manhattan')"},{"path":"unsupervised-learning.html","id":"adjusted-mutual-information-score","chapter":"4 Unsupervised Learning","heading":"4.1.1.2 Adjusted mutual information score","text":"labelled samples, can use mutual information\nscore test classes correspond clusters.\nadjusted mutual information score adjusts chance.","code":"from sklearn.metrics import adjusted_mutual_info_score\nadjusted_mutual_info_score(Y, clusters)"},{"path":"unsupervised-learning.html","id":"k-means-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.2 K-Means Clustering","text":"Goal: Divide data K clusters variance within \nclusters minimized. objective function:\\[V(D) = \\sum_{=1}^k \\sum{x_j \\C_i} (x_j - \\mu_i)^2,\\] \\(V\\) \nvariance, \\(C_i\\) cluster, \\(\\mu_i\\) cluster mean, \\(x_j\\) \ndatapoint. algorithm works follows:Assign data k initial clusters.Assign data k initial clusters.Calculate mean cluster.Calculate mean cluster.Assign data points closest cluster mean.Assign data points closest cluster mean.point changed cluster, repeat step 2.point changed cluster, repeat step 2.Data Iris flower data set clustered 3 clusters using\nk-Means. right data points assigned actual\nspecies. Figure user Chire \nwikimedia.org.info:\nscikit-learn.orgA faster alternative mini batch\nK-Means.","code":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\nkmeans.predict([[5, 1]])"},{"path":"unsupervised-learning.html","id":"graph-based-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.3 Graph-Based Clustering","text":"represent data set \\(D\\) graph \\(G=(V,E)\\) divide \nconnected sub-graphs represent clusters. edge \\(e_{ij}\\)\n(nodes \\(v_i\\) \\(v_j\\)) weight \\(w_{ij}\\) (commonly\nsimilarity distance measure).","code":""},{"path":"unsupervised-learning.html","id":"basic-graph-based-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.3.1 Basic Graph-Based Clustering","text":"basic algorithm works like :Define weight-threshold \\(\\theta\\).Define weight-threshold \\(\\theta\\).edges: \\(w_{ij}\\) > \\(\\theta\\): remove \\(e_{ij}\\).edges: \\(w_{ij}\\) > \\(\\theta\\): remove \\(e_{ij}\\).nodes connected path (found via depth first search):\nAssign cluster.nodes connected path (found via depth first search):\nAssign cluster.","code":""},{"path":"unsupervised-learning.html","id":"dbscan","chapter":"4 Unsupervised Learning","heading":"4.1.3.2 DBScan","text":"Density-Based Spatial Clustering Applications Noise \nnoise robust version basic graph-based clustering. create\nclusters based dense connected regions. works like :point core point least \\(\\text{minPts}\\) within \nradius \\(\\epsilon\\) point (including point ).point core point least \\(\\text{minPts}\\) within \nradius \\(\\epsilon\\) point (including point ).point directly reachable core point \nwithin \\(\\epsilon\\) \\(core point\\).point directly reachable core point \nwithin \\(\\epsilon\\) \\(core point\\).points part cluster (may part \ncluster).points part cluster (may part \ncluster).! points clusters, assignment cluster depends\norder point assignments.info:\nscikit-learn.orgThere newer version algorithm (Hierarchical\nDBSCAN),\nallows clusters varying density, robustness \ncluster assignment makes tuning \\(\\epsilon\\) unnecessary.","code":"from sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=3, min_samples=4)\ndbscan.fit(X)"},{"path":"unsupervised-learning.html","id":"cut-based-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.3.3 Cut-Based Clustering","text":"introduce adjacency/similarity matrix \\(W\\) (measures similarity\ndata points) define number clusters \\(k\\). now try\nminimize weight edges \\(\\kappa\\) clusters \\(C\\) (equal\ncutting edges nodes least similar):\\[\\begin{aligned}\n                \\begin{split}  \n                    \\min \\frac{1}{2} \\sum_{=1}^k \\sum_{b=1}^k \\kappa(C_a, C_b) \\\\\n                    \\text{} \\kappa(C_a, C_b) = \\sum_{v_i \\C_a , v_j \\C_b , \\neq b} W_{ij} \\\\\n                    \\text{ } \\kappa(C_a, C_a) = 0\n                \\end{split}\n            \\end{aligned}\\] \\(\\rightarrow\\) add \nsimilarities/edge-weights clusters (within \nclusters).\nconstructing similarity matrix, different kernels can used\n(commonly linear kernel Gaussian kernel).","code":""},{"path":"unsupervised-learning.html","id":"spectral-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.4 Spectral Clustering","text":"Spectral clustering works non-linearly mapping \nmatrix-representation graph onto lower-dimensional space based\nspectrum (set eigenvectors) group points . \nmapping preserves local distances, .e. close points stay close \nmapping. employs three steps: Preprocessing,\ndecomposition grouping.PreprocessingWe create Laplacian matrix \\(L\\) (Laplacian operator matrix form,\nmeasuring strongly vertex differs nearby vertices (\nedges similarity measures)): \\[L = D - W\\]\n\\[D_{ij} = \\begin{cases}\n        \\sum_{j=1}^N W_{ij} \\\\\n        0 \\text{ } \\neq j\n    \\end{cases}\\] \\(D\\) degree matrix ((weighted) degree\nnode diagonal) \\(W\\) adjacency/similarity\nmatrix (measures similarity data points).DecompositionYou first normalize Laplacian avoid big impacts highly\nconnected vertices/nodes. info calculation \nwikipedia.org.make eigenvalue decomposition:\\[L U = \\Lambda U \\quad \\rightarrow \\quad L = U \\Lambda U^{-1} \\]\\(U\\) matrix eigenvectors \\(\\Lambda\\) diagonal\nmatrix eigenvalues. can now find lower-dimensional embedding \nchoosing \\(k\\) smallest non-zero eigenvalues. final data now\nrepresented matrix k eigenvectors.GroupingYou multiple options:can cut graph using chosen eigenvectors splitting\n0 median value.can cut graph using chosen eigenvectors splitting\n0 median value.get final cluster assignments normalizing now\nk-dimensional data applying k-means clustering .get final cluster assignments normalizing now\nk-dimensional data applying k-means clustering .info:\nscikit-learn.org","code":"from sklearn.cluster import SpectralClustering\nscl = SpectralClustering(n_clusters=4,\n        affinity='rbf',\n        assign_labels='cluster_qr', # assigns labels directly from Eig vecs,\n        n-jobs = -1)\nscl.fit(X)"},{"path":"unsupervised-learning.html","id":"SSP","chapter":"4 Unsupervised Learning","heading":"4.1.5 Sparse Subspace Clustering (SSP)","text":"underlying assumption SSP different clusters reside\ndifferent subspaces data. Clusters therefore perpendicular\npoints cluster can reconstructed \ncombinations points cluster (\\(\\rightarrow\\)\nself-expressiveness, reconstruction vectors sparse). \npoint try find points can used recreate\npoint - form cluster. points\ngives data matrix \\(X\\) matrix reconstruction vectors \\(V\\):\\[X = X*V\\text{ s.t. diag}(V)=0.\\]now try minimize V-matrix according L1-norm (giving\nsparse matrix). matrix can used e.g. spectral\nclustering.details original paper \nSSC-OMP.info: github.com","code":"from cluster.selfrepresentation import SparseSubspaceClusteringOMP\nssc = SparseSubspaceClusteringOMP(n_clusters=3,affinity=\"symmetrize\")\nssc.fit(X)"},{"path":"unsupervised-learning.html","id":"soft-assignment-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.6 Soft-assignment Clustering","text":"Soft clustering assigns point probabilities belonging \nclusters instead assigning one cluster. \ngives measure certain algorithm clustering\npoint.","code":""},{"path":"unsupervised-learning.html","id":"gaussian-mixture-models","chapter":"4 Unsupervised Learning","heading":"4.1.6.1 Gaussian Mixture Models","text":"Gaussian mixture models try find ensemble gaussian\ndistributions best describe data. \ndistributions/components used clusters. points belong\ncluster certain probability. find distributions,\nuse expectation maximization algorithm:Assume centers Gaussians (e.g. k-means) calculate\npoint probability generated \ndistribution (\\(p(x_i \\C_k | \\phi_i, \\mu_k, \\sigma_k)\\)).Assume centers Gaussians (e.g. k-means) calculate\npoint probability generated \ndistribution (\\(p(x_i \\C_k | \\phi_i, \\mu_k, \\sigma_k)\\)).Change parameters maximize likelihood data, given\ncluster probabilities points.Change parameters maximize likelihood data, given\ncluster probabilities points.probability data point belonging cluster can calculated\nvia Bayes theorem.info theory:\nbrilliant.org.info:\nscikit-learn.org","code":"from sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=4, covariance_type='full')\ngm.fit(X)\ngm.predict_proba(X)"},{"path":"unsupervised-learning.html","id":"other-models","chapter":"4 Unsupervised Learning","heading":"4.1.6.2 Other models","text":"models also means calculate cluster probabilities \npoints. HDBSCAN-algorithm see\n.","code":""},{"path":"unsupervised-learning.html","id":"hierarchical-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.6.3 Hierarchical Clustering","text":"Instead clustering point one cluster, assign hierarchy.Pros:Hierarchies clusters reflects data set therefore relationship points better.Cons:difficult make clear statements cluster membership \\(\\rightarrow\\) define limit hierarchical depth","code":""},{"path":"unsupervised-learning.html","id":"artificial-neural-networks-for-clustering","chapter":"4 Unsupervised Learning","heading":"4.1.7 Artificial Neural Networks for Clustering","text":"See chapter Neural Networks\n(5)","code":""},{"path":"unsupervised-learning.html","id":"mapping-to-lower-dimensions","chapter":"4 Unsupervised Learning","heading":"4.2 Mapping to lower dimensions","text":"","code":""},{"path":"unsupervised-learning.html","id":"manifold-learning","chapter":"4 Unsupervised Learning","heading":"4.2.1 Manifold learning","text":"","code":""},{"path":"unsupervised-learning.html","id":"isomap","chapter":"4 Unsupervised Learning","heading":"4.2.1.1 Isomap","text":"","code":""},{"path":"unsupervised-learning.html","id":"local-linear-embedding","chapter":"4 Unsupervised Learning","heading":"4.2.1.2 Local linear embedding","text":"","code":""},{"path":"unsupervised-learning.html","id":"multi-dimensional-scaling","chapter":"4 Unsupervised Learning","heading":"4.2.1.3 Multi dimensional scaling","text":"","code":""},{"path":"unsupervised-learning.html","id":"decomposition-techniques","chapter":"4 Unsupervised Learning","heading":"4.2.2 Decomposition techniques","text":"","code":""},{"path":"unsupervised-learning.html","id":"SVD2","chapter":"4 Unsupervised Learning","heading":"4.2.2.1 Singular value decomposition","text":"Singular value decomposition used compress large matrices \ndata smaller ones, much less data, without loosing lot\ninformation. Please visit mathematical explanation \nunderlying mechanisms.","code":""},{"path":"unsupervised-learning.html","id":"principle-component-analysis-pca","chapter":"4 Unsupervised Learning","heading":"4.2.2.2 Principle Component analysis (PCA)","text":"","code":""},{"path":"unsupervised-learning.html","id":"outlier-detection","chapter":"4 Unsupervised Learning","heading":"4.3 Outlier detection","text":"","code":""},{"path":"unsupervised-learning.html","id":"local-outlier-factor","chapter":"4 Unsupervised Learning","heading":"4.3.1 Local outlier factor","text":"","code":""},{"path":"unsupervised-learning.html","id":"isolation-forest","chapter":"4 Unsupervised Learning","heading":"4.3.2 Isolation forest","text":"","code":""},{"path":"generative-models.html","id":"generative-models","chapter":"5 Generative models","heading":"5 Generative models","text":"","code":""},{"path":"generative-models.html","id":"generative-models-for-discrete-data","chapter":"5 Generative models","heading":"5.1 Generative Models for Discrete Data","text":"","code":""},{"path":"generative-models.html","id":"bayesian-concept-learning","chapter":"5 Generative models","heading":"5.1.1 Bayesian Concept Learning","text":"can learn concept \\(c \\C\\) positive examples alone. \ndefine posterior: \\(p(c|\\mathcal{D})\\). get learn concept \nneed hypothesis space \\(\\mathcal{H}\\) version space (subset \n\\(\\mathcal{H}\\)) consistent \\(\\mathcal{D}\\). choose \nhypothesis \\(h\\) assuming samples randomly chosen \ntrue concept calculate\n\\(p(\\mathcal{D}|h)=\\lbrack \\frac{1}{|h|}\\rbrack^N\\) (sampling \\(N\\) data\npoints \\(h\\)). choose hypothesis highest\nprobability (thereby choose suspicious coincidences broad\nmodels). priors can chosen e.g. giving lower priority \nconcepts complex rules (e.g. “powers 2 100 \n64.”). subjective, however often beneficial rapid learning.\nUsing Bayes rule, can calculate posterior:\n\\[p(h|\\mathcal{D}) =\\dfrac{p(\\mathcal{D}|h)p(h)}{p(\\mathcal{D})} =  \\dfrac{p(\\mathcal{D}|h)p(h)}{\\sum_{h' \\\\mathcal{H}}p(\\mathcal{D}|h')p(h')}=\\dfrac{\\mathbb{}(\\mathcal{D} \\h)p(h)}{\\sum_{h' \\\\mathcal{H}}\\mathbb{}(\\mathcal{D} \\h')p(h')},\\]\n\\(\\mathbb{}(\\mathcal{D} \\h)p(h)\\) = 1 data adhere \n\\(h\\). maximum \\(p(h|\\mathcal{D})\\) MAP estimate.data MAP-estimate converges MLE. true\nhypothesis \\(\\mathcal{H}\\) MLE MAP converge \n(\\(\\rightarrow\\) consistent estimators). take entire\ndistribution hypotheses get distribution estimate\n(point prediction) \\(\\rightarrow\\) posterior predictive\ndistribution.\n\\[p(\\tilde{x}|\\mathcal{D}) = \\sum_h p(\\tilde{x}|h)p(h|\\mathcal{D})\\]\nweighting hypotheses called Bayes model averaging. \nsmall data sets get vague posterior broad predictive\ndistribution. can replace posteriors delta-function:\n\\[p(\\tilde{x}|\\mathcal{D}) = \\sum_h p(\\tilde{x}|h)\\delta_{\\hat{h}_{\\text{MAP}}}(h)\\]\n\\(\\rightarrow\\) plug-approximation (-represents uncertainty).","code":""},{"path":"generative-models.html","id":"beta-binomial-model","chapter":"5 Generative models","heading":"5.1.2 Beta-binomial model","text":"distribution uses binomial distribution \nlikelihood beta-distribution ’s \\(\\theta\\) parameter \nprior.","code":""},{"path":"generative-models.html","id":"likelihood","chapter":"5 Generative models","heading":"5.1.2.1 Likelihood","text":"\\[p(\\mathcal{D}|\\theta) = \\text{Bin}(k|n, \\theta) \\propto \\theta^k (1-\\theta)^{n-k},\\]\n\\(k\\) successful trials, \\(n\\) total trials \n\\(\\theta\\) success-probabilities single experiments. \\(k\\)\n\\(n-k\\) sufficient statistics data:\n\\(p(\\theta|\\mathcal{D} = p(\\theta|k, n-k))\\).","code":""},{"path":"generative-models.html","id":"prior","chapter":"5 Generative models","heading":"5.1.2.2 Prior","text":"\\[\\text{Beta}(\\theta|\\alpha, \\beta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\]\nparameters \\(\\alpha\\) \\(\\beta\\) used hyper parameters. \nprior form likelihood \\(\\rightarrow\\) conjugate\nprior.","code":""},{"path":"generative-models.html","id":"posterior","chapter":"5 Generative models","heading":"5.1.2.3 Posterior","text":"\\[p(\\theta|\\mathcal{D}) \\propto \\text{Bin}(k|n,\\theta)\\text{Beta}(\\theta|\\alpha, \\beta)=\\text{Beta}(\\theta|k+\\alpha,n-k+\\beta)\\]\nadd pseudo-counts (\\(\\alpha, \\beta\\)) empirical counts (\\(N, k\\)). \nposterior predictive distribution : \\[\\begin{aligned}\n                p(\\tilde{x}=1|\\mathcal{D})& =\\int_0^1 p(\\tilde{x}=1|\\theta)p(\\theta|\\mathcal{D})\\mathrm{d}\\theta\n                        =\\int_0^1 \\theta\\text{Beta}(\\theta|N-k+\\alpha,k+\\beta)\\mathrm{d}\\theta \\\\\n                        & =\\text{E}[\\theta|\\mathcal{D}]=\\dfrac{N-k+\\alpha}{N\\cancel{-k+k}+\\alpha+\\beta} \\\\\n            \\end{aligned}\\] instead used MLE \\(\\theta\\)\ninstead (\\(p(\\theta|\\mathcal{D}) = p(\\theta|\\theta_{\\text{MLE}})\\)) \nlittle data failures (e.g. 3 coin flips \ntails). MLE estimate \\(\\theta_{\\text{MLE}}) = 0/3 = 0\\).\ncalled zero count estimate problem black swan paradox\n(.e. don’t attribute possibilities something never seen\n). Solution: use uniform prior (\\(\\alpha = \\beta = 1\\)):\n\\(p(\\tilde{x}=1|\\mathcal{D}) = \\dfrac{N-k+1}{N+2}\\).","code":""},{"path":"generative-models.html","id":"dirichlet-multinomial-model","chapter":"5 Generative models","heading":"5.1.3 Dirichlet-multinomial model","text":": model \\(k\\) , now: \\(k\\) times (e.g. four pips die roll)\n\\(n\\) experiments.","code":""},{"path":"generative-models.html","id":"prior-1","chapter":"5 Generative models","heading":"5.1.3.1 Prior:","text":"Dirichlet distribution:\n\\[\\text{Dir}(\\theta|\\alpha) = \\dfrac{1}{B({\\alpha})}\\prod\\limits_{k=1}^K \\theta_k^{\\alpha_k-1}\\]","code":""},{"path":"generative-models.html","id":"posterior-1","chapter":"5 Generative models","heading":"5.1.3.2 Posterior:","text":"\\[\\begin{aligned}\n            p({\\theta}|\\mathcal{D})& \\propto p(\\mathcal{D}|{\\theta})p({\\theta|\\alpha}) \\\\\n                & \\propto \\prod\\limits_{k=1}^K\\theta_k^{N_k}\\theta_k^{\\alpha_k-1} = \\prod\\limits_{k=1}^K\\theta_k^{N_k+\\alpha_k-1}\\\\\n                & =\\text{Dir}({\\theta}|\\alpha_1+N_1,\\cdots,\\alpha_K+N_K)\n            \\end{aligned}\\] posterior predictive distribution :\n\\[\\begin{aligned}\n                p(\\tilde{\\tilde{X}}=j|\\mathcal{D})& =\\int p(\\tilde{X}=j|{\\theta})p({\\theta}|\\mathcal{D})\\mathrm{d}{\\theta} \\\\\n                    & =\\int p(\\tilde{X}=j|\\theta_j)\\left[\\int p({\\theta}_{-j}, \\theta_j|\\mathcal{D})\\mathrm{d}{\\theta}_{-j}\\right]\\mathrm{d}\\theta_j \\\\\n                    & =\\int \\theta_jp(\\theta_j|\\mathcal{D})\\mathrm{d}\\theta_j=\\text{E}[\\theta_j|\\mathcal{D}]=\\dfrac{N_j+\\alpha_j}{\\sum_k N_k+\\alpha_k}\n                \\end{aligned}\\] (, avoid zero-count\nproblem via pseudo counts.)","code":""},{"path":"regression.html","id":"regression","chapter":"6 Regression","heading":"6 Regression","text":"","code":""},{"path":"regression.html","id":"evaluation-of-regression-models","chapter":"6 Regression","heading":"6.1 Evaluation of regression models","text":"","code":""},{"path":"regression.html","id":"mean-squared-error","chapter":"6 Regression","heading":"6.1.1 Mean squared error","text":"measure shows deviation predicted value \\(\\hat{y}\\) target value \\(y\\). squaring penalized large deviations avoids respective cancellation positive negative errors.\\[ MSE = 1/n \\sum_i (y_i - \\hat{y}_i)^2\\]info: scikit-learn.org","code":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_true, y_pred)"},{"path":"regression.html","id":"r2-score-coefficient-of-determination","chapter":"6 Regression","heading":"6.1.2 R^2 score / coefficient of determination","text":"measure shows much variance \ntarget/dependent variable \\(y\\) can explained model/independent\nvariable \\(\\hat{y}\\).\\[\nR^2 = 1 - \\frac{\\text{Unexplained Variance}}{\\text{Total Variance}} = \\frac{SS_{res}}{SS_{tot}} \\\\\nSS_{res} = \\sum_i(y_i-\\hat{y}_i)^2 \\\\\nSS_{tot} = \\sum_i(y_i - \\bar{y}_i)^2\n\\] \\(\\bar{y}\\) mean target \\(y\\).value commonly reaches 0 (model always predicts mean \\(y\\)) 1 (perfect fit model data). can however negative (e.g. wrong model, heavy overfitting, …). adjusted R^2 compensates size model (variables), favoring simpler models.\ninfo: wikipedia.orgMore info: scikit-learn.org","code":"from sklearn.metrics import r2_score\nr2 = r2_score(y_true, y_pred)"},{"path":"regression.html","id":"visual-tools","chapter":"6 Regression","heading":"6.1.3 Visual tools","text":"","code":""},{"path":"regression.html","id":"linear-models","chapter":"6 Regression","heading":"6.2 Linear Models","text":"","code":""},{"path":"regression.html","id":"ordinary-least-squares","chapter":"6 Regression","heading":"6.2.1 Ordinary Least Squares","text":"","code":""},{"path":"regression.html","id":"lasso-regression","chapter":"6 Regression","heading":"6.2.2 Lasso regression","text":"","code":""},{"path":"regression.html","id":"ridge-regression","chapter":"6 Regression","heading":"6.2.3 Ridge regression","text":"","code":""},{"path":"regression.html","id":"kernel-ridge-regression","chapter":"6 Regression","heading":"6.2.3.1 Kernel ridge regression","text":"","code":""},{"path":"regression.html","id":"bayesian-regression","chapter":"6 Regression","heading":"6.2.4 Bayesian regression","text":"","code":""},{"path":"regression.html","id":"anova","chapter":"6 Regression","heading":"6.2.5 ANOVA","text":"","code":""},{"path":"regression.html","id":"generalized-linear-models","chapter":"6 Regression","heading":"6.2.6 Generalized linear models","text":"","code":""},{"path":"regression.html","id":"gaussian-process-regression","chapter":"6 Regression","heading":"6.3 Gaussian process regression","text":"Gaussian process regression based Bayesian Probability: \ngenerate many models calculate probability models given\nsamples. make predictions based probabilities \nmodels.get non-linear functions data using non-linear kernels:\nassume input data points similar, similar\ntarget values. concept similarity (e.g. hour day) \nencoded kernels use.Schema training process Gaussian process regression. \nleft graph shows prior samples functions . functions\nconditioned data (graph middle). right graph\nshows predictions credible intervals gray. Source:\nuser Cdipaolo96 \nwikimedia.org\n.Pros:model reports predictions certain probability.model reports predictions certain probability.Cons:Training scales \\(O(n^3)\\).Training scales \\(O(n^3)\\).need design choose kernel.need design choose kernel.info:\nscikit-learn.org","code":"from sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, ExpSineSquared\nkernel = DotProduct() + WhiteKernel() + RBF() + ExpSineSquared() # The kernel hyperparameters are tuned by the model\ngpr = GaussianProcessRegressor(kernel=kernel)\ngpr.fit(X, y)\ngpr.predict(X, return_std=True)"},{"path":"regression.html","id":"gradient-boosted-tree-regression","chapter":"6 Regression","heading":"6.4 Gradient boosted tree regression","text":"Apart classification, gradient boosted trees also allow \nregression. works like gradient boosted trees classification: \niteratively add decision tree regressors minimize regression\nloss already fitted ensemble. decision tree\nregressor\ndecision tree trained continuous data instead \ndiscrete classification data, output still\ndiscrete.info:\nscikit-learn.org","code":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(n_estimators = 500, min_samples_split =5, max_depth = 4, max_features=\"sqrt\", n_iter_no_change=15)\ngbr.fit(X, y)"},{"path":"regression.html","id":"time-series-forecasting","chapter":"6 Regression","heading":"6.5 Time Series Forecasting","text":"“normal” settings order samples play role\n(e.g. blood sugar level one sample independent others). \ntime series however, samples need represented ordered\nvector matrix (e.g. temperature Jan 2nd independent \ntemperature Jan 1st).","code":"import pandas as pd\ndf = pd.read_csv(\"data.csv\", header=0, index_col=0, names=[\"date\", \"sales\"])\nsales_series = df[\"sales\"] # pandas series make working with time series easier "},{"path":"regression.html","id":"arimax-model","chapter":"6 Regression","heading":"6.5.1 ARIMA(X) Model","text":"univariate time series model exogenous regressor.","code":""},{"path":"regression.html","id":"varmmax-model","chapter":"6 Regression","heading":"6.5.2 VARMMA(X) Model","text":"Multivariate time series model, variables can influence \ntarget can influence variables vice versa.","code":""},{"path":"regression.html","id":"prophet-model","chapter":"6 Regression","heading":"6.5.3 Prophet-Model","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"neural-networks-neural-networks","chapter":"7 Neural Networks {#Neural Networks}","heading":"7 Neural Networks {#Neural Networks}","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"introduction","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.1 Introduction","text":"basic level, neural networks consist many simple models\n(e.g. linear logistic models) chained together \ndirected network. models sit neurons (nodes) network.\nimportant components neurons :Activation: \\(= Wx+b\\) (\\(W\\) = weights \\(b\\) = bias)Activation: \\(= Wx+b\\) (\\(W\\) = weights \\(b\\) = bias)Non-linearity: \\(f(x, \\theta)=\\sigma()\\) (e.g. sigmoid function\nlogistic regression, giving probability output. \\(\\theta\\)\nthreshold)Non-linearity: \\(f(x, \\theta)=\\sigma()\\) (e.g. sigmoid function\nlogistic regression, giving probability output. \\(\\theta\\)\nthreshold)neurons (nodes) first layer uses input sample\nvalues feeds output activation function next\nnodes next layer, .s.o. later layers thereby learn\ncomplicated concepts structures.Model artificial neural network one hidden layer. Figure\nuser LearnDataSci \nwikimedia.org.","code":""},{"path":"neural-networks-neural-networks.html","id":"non-linearities","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.1.1 Non-Linearities","text":"Different non-linear functions can used generate output \nneurons.","code":""},{"path":"neural-networks-neural-networks.html","id":"sigmoidlogistic-functions","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.1.1.1 Sigmoid/Logistic Functions","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"tanh-functions","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.1.1.2 Tanh Functions","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"rectifiersrelu","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.1.1.3 Rectifiers/ReLU","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"terminology-1","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.1.1.4 Terminology","text":"Input layer/visible layer: Input variablesInput layer/visible layer: Input variablesHidden layer: Layers nodes input output layerHidden layer: Layers nodes input output layerOutput layer: Layer nodes produce output variablesOutput layer: Layer nodes produce output variablesSize: Number nodes networkSize: Number nodes networkWidth: Number nodes layerWidth: Number nodes layerDepth: Number layersDepth: Number layersCapacity: type functions can learned \nnetworkCapacity: type functions can learned \nnetworkArchitecture: arrangement layers nodes networkArchitecture: arrangement layers nodes network","code":""},{"path":"neural-networks-neural-networks.html","id":"feedforward-neural-network-multi-layer-perceptron","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.2 Feedforward Neural Network / Multi-Layer Perceptron","text":"simplest type proper neural networks. neuron \nlayer connected neuron next layer \ncycles. outputs previous layer corresponds \\(x\\) \nactivation function. output (\\(x_i\\)) previous layer gets ’s\nweight (\\(w_i\\)) node bias (\\(b\\)) added node.\nNeurons high output “active” neurons, \nnegative outputs “inactive”. result mapped probability\nrange (commonly) sigmoid function. output given\nnext layer.\ninput layer 6400 features (80*80 image), network 2\nhidden layers 16 nodes \n\\(6400*16+16*16+16*10+16+16+10 = 102'858\\) parameters. high\nnumber degrees freedom requires lot training samples.","code":"        from torch import nn\n\n        class CustomNet(nn.Module):\n            def __init__(self):\n                super(CustomNet, self).__init__()\n                self.lin_layer_1 = nn.Linear(in_features=10, out_features=10)\n                self.relu = nn.ReLU()\n                self.lin_layer_2 = nn.Linear(in_features=10, out_features=10)\n\n            def forward(self, x):\n                x = self.lin_layer_1(x)\n                x = self.relu\n                x = self.lin_layer_2(x)\n                return x\n\n            def num_flat_features(self, x):\n                size = x.size()[1:] # Use all but the batch dimension\n                num = 1\n                for i in size:\n                    num *= i\n                return num\n\n        new_net = CustomNet()"},{"path":"neural-networks-neural-networks.html","id":"backpropagation","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.2.1 Backpropagation","text":"method neural networks learn optimal weights\nbiases nodes. components cost function \ngradient descent method.\ncost function analyses difference designated\nactivation output layer (according label data) \nactual activation layer. Commonly residual sum squares\nused.\nget direction next best parameter-combination using \nstochastic gradient descent algorithm using gradient cost\nfunction:use “mini-batch” samples round/step gradient\ndescent.use “mini-batch” samples round/step gradient\ndescent.calculate squared residual feature output layer\nsample.calculate squared residual feature output layer\nsample.calculate bias weights output\nlayer activation last hidden layer must \nget result. average images \nmini-batch.calculate bias weights output\nlayer activation last hidden layer must \nget result. average images \nmini-batch.calculate weights, biases activations \nupstream layers \\(\\rightarrow\\) backpropagate.calculate weights, biases activations \nupstream layers \\(\\rightarrow\\) backpropagate.","code":""},{"path":"neural-networks-neural-networks.html","id":"convolutional-neural-networks","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.3 Convolutional Neural Networks","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"autoencoders","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.4 Autoencoders","text":"Contrary architectures, autoencoders used \nunsupervised learning. goal compress decompress data \nlearn important structures data. layers therefore\nbecome smaller encoding step later layers get bigger\n, original representation data. optimization\nproblem now:\n\\[\\min_{W,b} \\frac{1}{N}*\\sum_{=1}^N ||x_i - \\hat{x}_i||^2\\] \\(x_i\\)\noriginal datapoint \\(\\hat{x}_i\\) reconstructed\ndatapoint.Model autoencoder. encoder layers compress data towards\ncode layer, decoder layers decompress data . Figure\nMichela Massi \nwikimedia.org.","code":""},{"path":"neural-networks-neural-networks.html","id":"autoencoders-for-clustering","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.4.1 Autoencoders for clustering","text":"can look layers NN ways represent data different\nform complexity compactness. code layers autoencoders \ncompact way represent data. can use \ncompressed representation code layer clustering \ndata. code layer however optimized task XXXX\ncombined cost function autoencoder k-means\nclustering:\n\\[\\min_{W,b} \\frac{1}{N}*\\sum_{=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda \\sum_{=1}^N ||f(x_i) - c_i||^2\\]\n\\(f(x_i)\\) non-linearity code layer \\(\\lambda\\) \nweight constant.\nXXXX adapted spectral clustering (section\n3.3) using autoencoders replacing \n(linear) eigen-decomposition (non-linear) decomposition \nencoder. spectral clustering Laplacian matrix used \ninput decomposition step (encoder) compressed\nrepresentation (code-layer) fed k-means clustering.Deep subspace clustering XXXX employs autoencoders combined \nsparse subspace clustering 3.3.1. used autoencoders optimized compact\nrepresentation code layer: \\[\\begin{split}\n                \\min_{W,b} \\frac{1}{N}*\\sum_{=1}^N ||x_i - \\hat{x}_i||^2 - \\lambda ||V||_1 \\\\\n                \\text{s.t.} F(X) = F(X)*V \\text{ diag}(V)=0\n            \\end{split}\\] V sparse representation \ncode layer (\\(F(X)\\)) .","code":""},{"path":"neural-networks-neural-networks.html","id":"generative-adversarial-networks","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.5 Generative adversarial networks","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"recurrent-neural-networks","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.6 Recurrent neural networks","text":"","code":""},{"path":"neural-networks-neural-networks.html","id":"long-short-term-memory-networks","chapter":"7 Neural Networks {#Neural Networks}","heading":"7.7 Long short-term memory networks","text":"","code":""},{"path":"explanation-and-inspection-methods.html","id":"explanation-and-inspection-methods","chapter":"8 Explanation and inspection methods","heading":"8 Explanation and inspection methods","text":"","code":""},{"path":"explanation-and-inspection-methods.html","id":"global-explainability-methods","chapter":"8 Explanation and inspection methods","heading":"8.1 Global explainability methods","text":"","code":""},{"path":"explanation-and-inspection-methods.html","id":"inherently-explainable-algorithms","chapter":"8 Explanation and inspection methods","heading":"8.1.1 Inherently explainable algorithms","text":"","code":""},{"path":"explanation-and-inspection-methods.html","id":"local-explainability-methods","chapter":"8 Explanation and inspection methods","heading":"8.2 Local explainability methods","text":"","code":""},{"path":"explanation-and-inspection-methods.html","id":"shapely-method","chapter":"8 Explanation and inspection methods","heading":"8.2.1 Shapely method","text":"","code":""},{"path":"explanation-and-inspection-methods.html","id":"lime","chapter":"8 Explanation and inspection methods","heading":"8.2.2 LIME","text":"","code":""},{"path":"explanation-and-inspection-methods.html","id":"anchor-methods","chapter":"8 Explanation and inspection methods","heading":"8.2.3 Anchor methods","text":"","code":""},{"path":"machine-learning-automation-productivity.html","id":"machine-learning-automation-productivity","chapter":"9 Machine learning Automation & Productivity","heading":"9 Machine learning Automation & Productivity","text":"quickly get confirmation target variable can actually predicted certain accuracy, automated machine learning approaches huge time saver.\nuseful practice, resulting models & pipelines need highly configurable upon investigation initial performance shortcomings. Many auto-ML libraries tested tend overfitting. Using uncommonly high number cross validation folds usually helps.\n\\ auto-ML packages:","code":""},{"path":"machine-learning-automation-productivity.html","id":"holistic-approach","chapter":"9 Machine learning Automation & Productivity","heading":"9.1 Holistic approach","text":"","code":""},{"path":"machine-learning-automation-productivity.html","id":"autosklearn","chapter":"9 Machine learning Automation & Productivity","heading":"9.1.1 Autosklearn","text":"Auto-Sklearn preprocessing, model selection, training hyperparameter tuning. \\\ninfo: neurips.ccMore info: automl.github.io","code":"from autosklearn.classification import AutoSklearnClassifier\nclf = AutoSklearnClassifier(time_left_for_this_task=120, per_run_time_limit=30, resampling_strategy_arguments={\"folds\":10}, include={'data_preprocessor': ['NoPreprocessing']})\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)"},{"path":"machine-learning-automation-productivity.html","id":"flaml","chapter":"9 Machine learning Automation & Productivity","heading":"9.1.2 FLAML","text":"","code":""},{"path":"machine-learning-automation-productivity.html","id":"h2o.ai","chapter":"9 Machine learning Automation & Productivity","heading":"9.1.3 H2O.ai","text":"H2O.ai’s library includes auto-ML library well. now configurable desired. library however contains modules admissible AI (ML discriminated minarities etc.) model explainability. \\\ninfo: h2o.ai","code":""},{"path":"machine-learning-automation-productivity.html","id":"autokeras","chapter":"9 Machine learning Automation & Productivity","heading":"9.1.4 AutoKeras","text":"","code":""},{"path":"machine-learning-automation-productivity.html","id":"model-selection-automation","chapter":"9 Machine learning Automation & Productivity","heading":"9.2 Model selection automation","text":"","code":""},{"path":"machine-learning-automation-productivity.html","id":"feature-engineering-automation","chapter":"9 Machine learning Automation & Productivity","heading":"9.3 Feature engineering automation","text":"","code":""},{"path":"machine-learning-automation-productivity.html","id":"cookie-cutter-visualizations","chapter":"9 Machine learning Automation & Productivity","heading":"9.4 cookie-cutter Visualizations","text":"","code":""},{"path":"machine-learning-automation-productivity.html","id":"comprehensive-validation-reports","chapter":"9 Machine learning Automation & Productivity","heading":"9.5 Comprehensive validation reports","text":"","code":""},{"path":"experimental-design.html","id":"experimental-design","chapter":"10 Experimental Design","heading":"10 Experimental Design","text":"","code":""},{"path":"experimental-design.html","id":"deadly-sins-of-experimental-design","chapter":"10 Experimental Design","heading":"10.1 7 Deadly sins of experimental design","text":"","code":""},{"path":"experimental-design.html","id":"power-analysis","chapter":"10 Experimental Design","heading":"10.2 Power analysis","text":"","code":""},{"path":"machine-learning-project-management.html","id":"machine-learning-project-management","chapter":"11 Machine Learning Project Management","heading":"11 Machine Learning Project Management","text":"","code":""},{"path":"machine-learning-project-management.html","id":"basis-for-machine-learning-in-companies","chapter":"11 Machine Learning Project Management","heading":"11.1 Basis for Machine Learning in Companies","text":"Cover Topics:Data StrategyData StrategyData ManagementData ManagementArchitecture carrying machine learning workloadsArchitecture carrying machine learning workloads","code":""},{"path":"machine-learning-project-management.html","id":"how-to-automate-process-with-machine-learning","chapter":"11 Machine Learning Project Management","heading":"11.2 How to automate process with Machine Learning","text":"","code":""},{"path":"machine-learning-project-management.html","id":"the-stages-from-manual-to-ml","chapter":"11 Machine Learning Project Management","heading":"11.2.1 The stages from ManuaL to ML","text":"good business process entails feedback loop client \noptimize process:Input \\(\\rightarrow\\) Process \\(\\rightarrow\\) Output \\(\\rightarrow\\)\nFeedback \\(\\rightarrow\\) Optimization \\(\\rightarrow\\) new Process\n\\(\\rightarrow\\) …Business processes evolve along phases, whose steps \nskipped:Individual employee works tasks, commonly informal rules \nheuristics used.Individual employee works tasks, commonly informal rules \nheuristics used.Team works tasks. process formalized \nstandardized ensure quality effective collaboration. Don’t\nstay long, since scaleable.Team works tasks. process formalized \nstandardized ensure quality effective collaboration. Don’t\nstay long, since scaleable.Digitization used automate (parts ) process. \nstep done ML, since need data \narchitecture ML part anyway. flexible \ncan fail adapt quicker. Don’t stay long, since \nassess quaility process well.Digitization used automate (parts ) process. \nstep done ML, since need data \narchitecture ML part anyway. flexible \ncan fail adapt quicker. Don’t stay long, since \nassess quaility process well.Analytics used measure performance process \noptimizations successful. Don’t skip since need\nindicators ML optimizationa monitoring anyway.\nDon’t stay long miss automation scalability.Analytics used measure performance process \noptimizations successful. Don’t skip since need\nindicators ML optimizationa monitoring anyway.\nDon’t stay long miss automation scalability.Machine Learning used automate optimize analysis,\ninsights decision making data. still need people\nstep 2 analyse outcomes, failures react \n(monitoring).Machine Learning used automate optimize analysis,\ninsights decision making data. still need people\nstep 2 analyse outcomes, failures react \n(monitoring).info: ","code":""},{"path":"machine-learning-project-management.html","id":"phases-of-the-ml-project","chapter":"11 Machine Learning Project Management","heading":"11.2.2 Phases of the ML project","text":"info: Framing problemFraming problemData collection & managementData collection & managementBuilding infrastructure (data pipeline, databases, training &\ndeployment pipelines least work way \ndesignated product unless quick’n’dirty PoC)Building infrastructure (data pipeline, databases, training &\ndeployment pipelines least work way \ndesignated product unless quick’n’dirty PoC)Data ingestion, transformation & feature engineeringData ingestion, transformation & feature engineeringModel selection, training, testing & evaluationModel selection, training, testing & evaluationDeployment & integrationDeployment & integrationMonitoringMonitoring","code":""},{"path":"machine-learning-project-management.html","id":"how-to-frame-ml-problems","chapter":"11 Machine Learning Project Management","heading":"11.2.3 How to frame ML problems","text":"ML-view:\npredicted?\ndata need target input?\nML-view:predicted?predicted?data need target input?data need target input?Software development view:\ninfo need users make decision? (\ndefines API)\nuse service? many people ?\nprocess conducted today?\nSoftware development view:info need users make decision? (\ndefines API)info need users make decision? (\ndefines API)use service? many people ?use service? many people ?process conducted today?process conducted today?Data view:\ndata needs collected? ?\nneed transform data analyze & make\ndecisions ? (Feature engineering)\nreact outputs algorithm? (e.g. kick \nautomatic process, inform stakeholders…)\nData view:data needs collected? ?data needs collected? ?need transform data analyze & make\ndecisions ? (Feature engineering)need transform data analyze & make\ndecisions ? (Feature engineering)react outputs algorithm? (e.g. kick \nautomatic process, inform stakeholders…)react outputs algorithm? (e.g. kick \nautomatic process, inform stakeholders…)","code":""},{"path":"machine-learning-project-management.html","id":"common-pitfalls-in-machine-learning","chapter":"11 Machine Learning Project Management","heading":"11.3 Common Pitfalls in Machine Learning","text":"Underestimate effort data collection, engineering,\ntransformation & ingestionUnderestimate effort data collection, engineering,\ntransformation & ingestionFocus much effort optimizing machine learning algorithm\ninstead getting dataFocus much effort optimizing machine learning algorithm\ninstead getting dataHaving samples diversity (independent attributes) \noutdated/unrepresentative dataHaving samples diversity (independent attributes) \noutdated/unrepresentative dataData properly maintained available needed \nprojectData properly maintained available needed \nprojectAssume oversight necessary data-,\ntarget-selection, feature engineering monitoring subject\nmatter expertsAssume oversight necessary data-,\ntarget-selection, feature engineering monitoring subject\nmatter expertsOptimize skewed indicator causes unwanted side-effects \nmodel decisionsOptimize skewed indicator causes unwanted side-effects \nmodel decisionsBuilding models scratch, pre-trained / --shelf\nmodels job (especially text, image, audio video tasks\nemploying neural networks)Building models scratch, pre-trained / --shelf\nmodels job (especially text, image, audio video tasks\nemploying neural networks)process monitoring retrainingHaving process monitoring retraining","code":""}]
