<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Classification Methods | Machine Learning Reference</title>
  <meta name="description" content="Chapter 3 Classification Methods | Machine Learning Reference" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Classification Methods | Machine Learning Reference" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="MoritzGuck/Machine-Learning-Reference" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Classification Methods | Machine Learning Reference" />
  
  
  

<meta name="author" content="Moritz GÃ¼ck" />


<meta name="date" content="2023-03-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-representation-analysis-processing.html"/>
<link rel="next" href="unsupervised-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>Machine Learning Reference</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html"><i class="fa fa-check"></i><b>1</b> Probability Theory &amp; Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-theory"><i class="fa fa-check"></i><b>1.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-basics"><i class="fa fa-check"></i><b>1.1.1</b> Probability Basics</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-distributions"><i class="fa fa-check"></i><b>1.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#CLT"><i class="fa fa-check"></i><b>1.1.3</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#bayesian-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian probability</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#further-concepts"><i class="fa fa-check"></i><b>1.1.5</b> Further Concepts</a></li>
<li class="chapter" data-level="1.1.6" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#statistical-tests"><i class="fa fa-check"></i><b>1.1.6</b> Statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#vectors"><i class="fa fa-check"></i><b>1.2.1</b> Vectors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html"><i class="fa fa-check"></i><b>2</b> Data: Representation, Analysis &amp; Processing</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-and-distance-measures"><i class="fa fa-check"></i><b>2.1</b> Similarity and Distance Measures</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#metrics"><i class="fa fa-check"></i><b>2.1.1</b> Metrics</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-measures-on-vectors"><i class="fa fa-check"></i><b>2.1.2</b> Similarity measures on vectors</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#kernels"><i class="fa fa-check"></i><b>2.1.3</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#data-analysis"><i class="fa fa-check"></i><b>2.2</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>2.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#output-analysis"><i class="fa fa-check"></i><b>2.2.2</b> Output Analysis</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#monitoring"><i class="fa fa-check"></i><b>2.2.3</b> Monitoring</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#preprocessing-data"><i class="fa fa-check"></i><b>2.3</b> Preprocessing data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#handling-missing-values"><i class="fa fa-check"></i><b>2.3.1</b> Handling missing values</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#date-and-time"><i class="fa fa-check"></i><b>2.3.2</b> Date and time</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#replacing-data"><i class="fa fa-check"></i><b>2.3.3</b> Replacing data</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#encoding-data"><i class="fa fa-check"></i><b>2.3.4</b> Encoding data</a></li>
<li class="chapter" data-level="2.3.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#standardization"><i class="fa fa-check"></i><b>2.3.5</b> Standardization</a></li>
<li class="chapter" data-level="2.3.6" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#splitting-in-training--and-test-data"><i class="fa fa-check"></i><b>2.3.6</b> Splitting in training- and test-data</a></li>
<li class="chapter" data-level="2.3.7" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#feature-selection"><i class="fa fa-check"></i><b>2.3.7</b> Feature selection</a></li>
<li class="chapter" data-level="2.3.8" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>2.3.8</b> Hyper-parameter tuning</a></li>
<li class="chapter" data-level="2.3.9" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#model-selection"><i class="fa fa-check"></i><b>2.3.9</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#errors-regularization"><i class="fa fa-check"></i><b>2.4</b> Errors &amp; regularization</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bias-and-variance"><i class="fa fa-check"></i><b>2.4.1</b> Bias and Variance</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#regularization"><i class="fa fa-check"></i><b>2.4.2</b> Regularization</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bagging"><i class="fa fa-check"></i><b>2.4.3</b> Bagging</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#boosting"><i class="fa fa-check"></i><b>2.4.4</b> Boosting</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#stacking"><i class="fa fa-check"></i><b>2.4.5</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#tips-for-machine-learning-projects"><i class="fa fa-check"></i><b>2.5</b> Tips for machine learning projects</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#general-advice"><i class="fa fa-check"></i><b>2.5.1</b> General advice</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#common-mistakes"><i class="fa fa-check"></i><b>2.5.2</b> Common mistakes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-methods.html"><a href="classification-methods.html"><i class="fa fa-check"></i><b>3</b> Classification Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-methods.html"><a href="classification-methods.html#evaluation-of-classifiers"><i class="fa fa-check"></i><b>3.1</b> Evaluation of Classifiers</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-methods.html"><a href="classification-methods.html#confusion-matrix"><i class="fa fa-check"></i><b>3.1.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-methods.html"><a href="classification-methods.html#basic-quality-measures"><i class="fa fa-check"></i><b>3.1.2</b> Basic Quality Measures</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-methods.html"><a href="classification-methods.html#area-under-the-curve"><i class="fa fa-check"></i><b>3.1.3</b> Area under the Curve</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-methods.html"><a href="classification-methods.html#handling-unbalanced-data"><i class="fa fa-check"></i><b>3.1.4</b> Handling Unbalanced Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-methods.html"><a href="classification-methods.html#classification-algorithms"><i class="fa fa-check"></i><b>3.2</b> Classification Algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-methods.html"><a href="classification-methods.html#nearest-neighbors-classifier"><i class="fa fa-check"></i><b>3.2.1</b> Nearest Neighbors Classifier</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-methods.html"><a href="classification-methods.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>3.2.2</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-methods.html"><a href="classification-methods.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>3.2.3</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-methods.html"><a href="classification-methods.html#support-vector-classifier-svc"><i class="fa fa-check"></i><b>3.2.4</b> Support Vector Classifier (SVC)</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-methods.html"><a href="classification-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.2.5</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#custering-methods"><i class="fa fa-check"></i><b>4.1</b> Custering Methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#metrics-for-clustering-algorithms"><i class="fa fa-check"></i><b>4.1.1</b> metrics for Clustering algorithms</a></li>
<li class="chapter" data-level="4.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#graph-based-clustering"><i class="fa fa-check"></i><b>4.1.3</b> Graph-Based Clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#spectral-clustering"><i class="fa fa-check"></i><b>4.1.4</b> Spectral Clustering</a></li>
<li class="chapter" data-level="4.1.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#SSP"><i class="fa fa-check"></i><b>4.1.5</b> Sparse Subspace Clustering (SSP)</a></li>
<li class="chapter" data-level="4.1.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#soft-assignment-clustering"><i class="fa fa-check"></i><b>4.1.6</b> Soft-assignment Clustering</a></li>
<li class="chapter" data-level="4.1.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#artificial-neural-networks-for-clustering"><i class="fa fa-check"></i><b>4.1.7</b> Artificial Neural Networks for Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#mapping-to-lower-dimensions"><i class="fa fa-check"></i><b>4.2</b> Mapping to lower dimensions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#manifold-learning"><i class="fa fa-check"></i><b>4.2.1</b> Manifold learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#decomposition-techniques"><i class="fa fa-check"></i><b>4.2.2</b> Decomposition techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#outlier-detection"><i class="fa fa-check"></i><b>4.3</b> Outlier detection</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#local-outlier-factor"><i class="fa fa-check"></i><b>4.3.1</b> Local outlier factor</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#isolation-forest"><i class="fa fa-check"></i><b>4.3.2</b> Isolation forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generative-models.html"><a href="generative-models.html"><i class="fa fa-check"></i><b>5</b> Generative models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="generative-models.html"><a href="generative-models.html#generative-models-for-discrete-data"><i class="fa fa-check"></i><b>5.1</b> Generative Models for Discrete Data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="generative-models.html"><a href="generative-models.html#bayesian-concept-learning"><i class="fa fa-check"></i><b>5.1.1</b> Bayesian Concept Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="generative-models.html"><a href="generative-models.html#beta-binomial-model"><i class="fa fa-check"></i><b>5.1.2</b> Beta-binomial model</a></li>
<li class="chapter" data-level="5.1.3" data-path="generative-models.html"><a href="generative-models.html#dirichlet-multinomial-model"><i class="fa fa-check"></i><b>5.1.3</b> Dirichlet-multinomial model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#evaluation-of-regression-models"><i class="fa fa-check"></i><b>6.1</b> Evaluation of regression models</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#mean-squared-error"><i class="fa fa-check"></i><b>6.1.1</b> Mean squared error</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#r2-score-coefficient-of-determination"><i class="fa fa-check"></i><b>6.1.2</b> R^2 score / coefficient of determination</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#visual-tools"><i class="fa fa-check"></i><b>6.1.3</b> Visual tools</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#linear-models"><i class="fa fa-check"></i><b>6.2</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>6.2.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#lasso-regression"><i class="fa fa-check"></i><b>6.2.2</b> Lasso regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="regression.html"><a href="regression.html#ridge-regression"><i class="fa fa-check"></i><b>6.2.3</b> Ridge regression</a></li>
<li class="chapter" data-level="6.2.4" data-path="regression.html"><a href="regression.html#bayesian-regression"><i class="fa fa-check"></i><b>6.2.4</b> Bayesian regression</a></li>
<li class="chapter" data-level="6.2.5" data-path="regression.html"><a href="regression.html#anova"><i class="fa fa-check"></i><b>6.2.5</b> ANOVA</a></li>
<li class="chapter" data-level="6.2.6" data-path="regression.html"><a href="regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>6.2.6</b> Generalized linear models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.3</b> Gaussian process regression</a></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#gradient-boosted-tree-regression"><i class="fa fa-check"></i><b>6.4</b> Gradient boosted tree regression</a></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#time-series-forecasting"><i class="fa fa-check"></i><b>6.5</b> Time Series Forecasting</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#arimax-model"><i class="fa fa-check"></i><b>6.5.1</b> ARIMA(X) Model</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#varmmax-model"><i class="fa fa-check"></i><b>6.5.2</b> VARMMA(X) Model</a></li>
<li class="chapter" data-level="6.5.3" data-path="regression.html"><a href="regression.html#prophet-model"><i class="fa fa-check"></i><b>6.5.3</b> Prophet-Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Networks {#Neural Networks}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#non-linearities"><i class="fa fa-check"></i><b>7.1.1</b> Non-Linearities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#feedforward-neural-network-multi-layer-perceptron"><i class="fa fa-check"></i><b>7.2</b> Feedforward Neural Network / Multi-Layer Perceptron</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>7.2.1</b> Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>7.3</b> Convolutional Neural Networks</a></li>
<li class="chapter" data-level="7.4" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>7.4</b> Autoencoders</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders-for-clustering"><i class="fa fa-check"></i><b>7.4.1</b> Autoencoders for clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>7.5</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="7.6" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>7.6</b> Recurrent neural networks</a></li>
<li class="chapter" data-level="7.7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#long-short-term-memory-networks"><i class="fa fa-check"></i><b>7.7</b> Long short-term memory networks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html"><i class="fa fa-check"></i><b>8</b> Machine learning Automation &amp; Productivity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#holistic-approach"><i class="fa fa-check"></i><b>8.1</b> Holistic approach</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autosklearn"><i class="fa fa-check"></i><b>8.1.1</b> Autosklearn</a></li>
<li class="chapter" data-level="8.1.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#flaml"><i class="fa fa-check"></i><b>8.1.2</b> FLAML</a></li>
<li class="chapter" data-level="8.1.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#h2o.ai"><i class="fa fa-check"></i><b>8.1.3</b> H2O.ai</a></li>
<li class="chapter" data-level="8.1.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autokeras"><i class="fa fa-check"></i><b>8.1.4</b> AutoKeras</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#model-selection-automation"><i class="fa fa-check"></i><b>8.2</b> Model selection automation</a></li>
<li class="chapter" data-level="8.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#feature-engineering-automation"><i class="fa fa-check"></i><b>8.3</b> Feature engineering automation</a></li>
<li class="chapter" data-level="8.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#cookie-cutter-visualizations"><i class="fa fa-check"></i><b>8.4</b> cookie-cutter Visualizations</a></li>
<li class="chapter" data-level="8.5" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#comprehensive-validation-reports"><i class="fa fa-check"></i><b>8.5</b> Comprehensive validation reports</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html"><i class="fa fa-check"></i><b>9</b> Machine Learning Project Management</a>
<ul>
<li class="chapter" data-level="9.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#basis-for-machine-learning-in-companies"><i class="fa fa-check"></i><b>9.1</b> Basis for Machine Learning in Companies</a></li>
<li class="chapter" data-level="9.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-automate-business-processes-with-machine-learning"><i class="fa fa-check"></i><b>9.2</b> How to automate business processes with machine learning</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#the-stages-from-manual-to-ml"><i class="fa fa-check"></i><b>9.2.1</b> The stages from ManuaL to ML</a></li>
<li class="chapter" data-level="9.2.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#phases-of-the-ml-project"><i class="fa fa-check"></i><b>9.2.2</b> Phases of the ML project</a></li>
<li class="chapter" data-level="9.2.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-frame-ml-problems"><i class="fa fa-check"></i><b>9.2.3</b> How to frame ML problems</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#common-pitfalls-in-machine-learning"><i class="fa fa-check"></i><b>9.3</b> Common pitfalls in machine learning</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Reference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-methods" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Classification Methods<a href="classification-methods.html#classification-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Classification is the assignment of objects (data points) to categories
(classes). It requires a data set (i.e.Â training set) of points with
known class labels. If the class labels are not known you can instead
group the data using clustering algorithms (chapter
<a href="#Clustering%20Methods" reference-type="ref" reference="Clustering Methods">3</a>).</p>
<div id="evaluation-of-classifiers" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Evaluation of Classifiers<a href="classification-methods.html#evaluation-of-classifiers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="confusion-matrix" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Confusion matrix<a href="classification-methods.html#confusion-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This gives a quick overview on the distribution of true positives
(<span class="math inline">\(TP\)</span>), false positives (<span class="math inline">\(FP\)</span>) , <span class="math inline">\(TN\)</span> true negatives, <span class="math inline">\(FN\)</span> false
negatives.</p>
</div>
<div id="basic-quality-measures" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Basic Quality Measures<a href="classification-methods.html#basic-quality-measures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><span class="math inline">\(\text{Accuracy / Success Rate} = \frac{ \text{correct predictions}}{\text{total predictions}} = \frac{ \text{TP}+\text{TN}}{\text{TP} + \text{TN}+\text{FP}+\text{FN}}\)</span><br />
This metric should only be used in this pure form, when the number
of positive and negative samples are balanced.</p></li>
<li><p><span class="math inline">\(\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}\)</span><br />
i.e.Â How many of your positive predictions are actually positive?</p></li>
<li><p><span class="math inline">\(\text{True positive rate / Recall / Sensitivity} = \frac{\text{TP}}{\text{TP}+\text{FN}}\)</span><br />
i.e.Â How many of the positive samples did you catch?</p></li>
<li><p><span class="math inline">\(\text{True negative rate / Specificity / Selectivity} = \frac{\text{TN}}{\text{TN}+\text{FP}}\)</span><br />
i.e.Â How many of the negative samples did you catch as negative
(i.e.Â are truly negative)?</p></li>
<li><p><span class="math inline">\(\text{F-score} = 2 \frac{\text{precision} * \text{recall}}{\text{precision}+\text{recall}}\)</span><br />
This is useful in cases of unbalanced classes to balance the
trade-off between precision and recall.</p></li>
</ul>
</div>
<div id="area-under-the-curve" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Area under the Curve<a href="classification-methods.html#area-under-the-curve" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This class of measures represents the quality of the classifier for
different threshold values <span class="math inline">\(\theta\)</span> by calculating the area under the
curve spanned by different quality measures.</p>
<div id="area-under-the-receiver-operating-characteristics-curve-auroc-or-auc" class="section level4 hasAnchor" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> Area under the Receiver Operating Characteristics Curve (AUROC or AUC)<a href="classification-methods.html#area-under-the-receiver-operating-characteristics-curve-auroc-or-auc" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The AUC can be interpreted as follows: When the classifier gets a
positive and a negative point, the AUC shows the probability that the
classifier will give a higher score to the positive point. A perfect
classifier has an AUC of 1, and AUC of 0.5 represents random guessing.</p>
<p><strong>!</strong> This measure is not sensitive to class imbalance!</p>
<div class="figure">
<img src="figures/Roc_curve.png" style="width:44.0%" alt="" />
<p class="caption">Area under the precision recall curve. Source: <a href="https://commons.wikimedia.org/wiki/File:Roc_curve.svg">user cmglee on
wikipedia.org</a></p>
</div>
</div>
<div id="area-under-the-precision-recall-curve-auprc-average-precision-avep" class="section level4 hasAnchor" number="3.1.3.2">
<h4><span class="header-section-number">3.1.3.2</span> Area under the Precision-Recall Curve (AUPRC) / Average Precision (AveP)<a href="classification-methods.html#area-under-the-precision-recall-curve-auprc-average-precision-avep" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This measure can be used for unbalanced data sets. It represents the
average precision as a function of the recall. The value of 1 represents
a perfect classifier.</p>
<div class="figure">
<img src="figures/pr_curve.png" style="width:65.0%" alt="" />
<p class="caption">The precision-recall curve. Source:
<a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#:~:text=The%20precision%2Drecall%20curve%20shows,a%20low%20false%20negative%20rate.">scikit-learn.org</a></p>
</div>
</div>
</div>
<div id="handling-unbalanced-data" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Handling Unbalanced Data<a href="classification-methods.html#handling-unbalanced-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having many more samples in one class than the others during training
can lead to high accuracy values event though the classifier performs
poorly on the smaller classes. You can handle the unbalance by:</p>
<ul>
<li><p>up-sampling the smaller data set (creating more artificial samples
for that class)</p></li>
<li><p>giving more weight to the samples in the smaller data set</p></li>
<li><p>using a quality measure that is sensitive to class imbalance</p></li>
</ul>
<p>Oversampling using imbalanced-learn (see: )</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="classification-methods.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> RandomOverSampler</span>
<span id="cb28-2"><a href="classification-methods.html#cb28-2" aria-hidden="true" tabindex="-1"></a>ros <span class="op">=</span> RandomOverSampler(random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-3"><a href="classification-methods.html#cb28-3" aria-hidden="true" tabindex="-1"></a>features_resampled, labels_resampled <span class="op">=</span> ros.fit_resample(df[feature_cols], df[label_col])</span></code></pre></div>
<p>Sensitivity, specificity, precision, recall, support and F-score</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="classification-methods.html#cb29-1" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> df[label_col]</span>
<span id="cb29-2"><a href="classification-methods.html#cb29-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> classifier.predict(df[feature_cols])</span>
<span id="cb29-3"><a href="classification-methods.html#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="classification-methods.html#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.metrics <span class="im">import</span> sensitivity_specificity_support</span>
<span id="cb29-5"><a href="classification-methods.html#cb29-5" aria-hidden="true" tabindex="-1"></a>sensitivity, specificity, support <span class="op">=</span> sensitivity_specificity_support(y_true, y_pred) </span>
<span id="cb29-6"><a href="classification-methods.html#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="classification-methods.html#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_fscore_support</span>
<span id="cb29-8"><a href="classification-methods.html#cb29-8" aria-hidden="true" tabindex="-1"></a>precision, recall, fscore, support <span class="op">=</span> precision_recall_fscore_support(y_true, y_pred) </span></code></pre></div>
</div>
</div>
<div id="classification-algorithms" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Classification Algorithms<a href="classification-methods.html#classification-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="nearest-neighbors-classifier" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Nearest Neighbors Classifier<a href="classification-methods.html#nearest-neighbors-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This classifier predicts the class label using the most common class
label of its <span class="math inline">\(k\)</span> nearest neighbors in the training data set.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Classifier does not take time for training.</li>
<li>Can learn complex decision boundaries.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>The prediction is time consuming and scales with n.</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="classification-methods.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb30-2"><a href="classification-methods.html#cb30-2" aria-hidden="true" tabindex="-1"></a>kn_model <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb30-3"><a href="classification-methods.html#cb30-3" aria-hidden="true" tabindex="-1"></a>kn_model.fit(X, y)</span>
<span id="cb30-4"><a href="classification-methods.html#cb30-4" aria-hidden="true" tabindex="-1"></a>kn_model.predict([[<span class="dv">5</span>,<span class="dv">1</span>]])</span></code></pre></div>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">scikit-learn.org</a><br />
</p>
</div>
<div id="naive-bayes-classifier" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Naive Bayes Classifier<a href="classification-methods.html#naive-bayes-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Naive Bayes classifies works on the assumption that the features are
conditionally independent given the class label. For every point a
simplified version of Bayes rule is used:</p>
<p><span class="math display">\[P(Y=y_i|X=x) \propto P(X=x|Y=y_i) * P(Y=y_i) \]</span> where <span class="math inline">\(Y\)</span> is the RV
for the class label and <span class="math inline">\(X\)</span> is the RV that contains the feature values.
This holds since <span class="math inline">\(P(X=x)\)</span> is the same for all classes. Since the
different features <span class="math inline">\(X_j\)</span> are assumed to be independent they can be
multiplied out. The label <span class="math inline">\(y_i\)</span> with the highest probability is the
predicted class label:
<span class="math display">\[\arg \max_{y_i} P(Y=y_i|X=x) \propto P(Y=y_i) \prod_{j=1}^{d} P(X_j=x_j|Y=y_i)  \]</span>
One usually estimates the value of <span class="math inline">\(P(Y)\)</span> as the ferquency of the
different classes in the training data or assumes that all classes are
equally likely.<br />
To estimate the <span class="math inline">\(P(X_j=x_j|Y=y_i)\)</span> the following distributions are
commonly used: - For binary features: Bernoulli distribution - For
discrete features: Multinomial distribution - For continuous features:
Normal / Gaussian distribution</p>
<p>For discrete features, you need to use a <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes">smoothing
prior</a>
(add 1 to every feature count) to avoid 0 probabilities for samples with
features being 0 in the training data.</p>
<p><strong>Pros:</strong></p>
<ul>
<li><p>Naive Bayes training is fast.</p></li>
<li><p>Combine descrete and continuous features since a different
distribution can be used for each feature.</p></li>
<li><p>You can have straight decision boundaries (when classes have same
variance), circular decision boundaries (different variance, same
mean) and parabolic decision boundaries (different mean, different
variance).</p></li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><p>The probability estimates from Naive Bayes are <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes">usually
bad</a>.</p></li>
<li><p>The independence assumption between the features is usually not
given in real life.</p></li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="classification-methods.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb31-2"><a href="classification-methods.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb31-3"><a href="classification-methods.html#cb31-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GaussianNB()</span>
<span id="cb31-4"><a href="classification-methods.html#cb31-4" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">scikit-learn.org</a><br />
</p>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Linear discriminant analysis (LDA)<a href="classification-methods.html#linear-discriminant-analysis-lda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Contrary to Naive Bayes, the features in LDA are not assumed to be
independently distributed. As with Bayes rule a distribution for each
class is calculated according to Bayes rule. <span class="math inline">\(P(X=x|Y=y_i)\)</span> is modeled
as a multivariate Gaussian distribution. The Gaussians for each class
are assumed to be the same. The log-posterior can be simplified to:</p>
<p><span class="math display">\[log(P(y=y_i|x) = - \frac{1}{2} (x-\mu_i)^t \Sigma^{-1}(x-\mu_i)+\log P(y=y_i)\]</span>
<span class="math inline">\(\mu_i\)</span> is the mean of class <span class="math inline">\(i\)</span>, <span class="math inline">\((x-\mu_i)^t \Sigma^{-1}(x-\mu_i)\)</span>
corresponds to the <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis
distance</a>. Thus, we
assign the point to the class whose distribution it is closest to.<br />
LDA can also be thought of projecting the data into a space with <span class="math inline">\(k-1\)</span>
dimensions (<span class="math inline">\(k\)</span> being number of classes). More info:
<a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">wikipedia.org</a>.
It can also be used as a dimensionality reduction method.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="classification-methods.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.discriminant_analysis <span class="im">import</span> LinearDiscriminantAnalysis</span>
<span id="cb32-2"><a href="classification-methods.html#cb32-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LinearDiscriminantAnalysis()</span>
<span id="cb32-3"><a href="classification-methods.html#cb32-3" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html">scikit-learn.org</a><br />
</p>
</div>
<div id="support-vector-classifier-svc" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Support Vector Classifier (SVC)<a href="classification-methods.html#support-vector-classifier-svc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SVCs use hyperplanes to separate data points according to their class
label with a maximum margin (<span class="math inline">\(M\)</span>) between the separating hyperplane
(<span class="math inline">\(x^T\beta + \beta_0=0\)</span>) and the points. If points cannot be perfectly
separated by the decision boundary, a soft margin SVM is used with a
slack variable <span class="math inline">\(\xi\)</span> that punishes points in the margin or on the wrong
side of the hyperplane. The optimization problem is given by
<span class="citation">[@Hastie2009]</span> : <span class="math display">\[\begin{split}
            \max_{\beta, \beta_0, \beta=1} M, \\
            \text{subject to } y_i(x_i^T \beta + \beta_0) \ge 1 - \xi_i, \quad \forall i, \\\xi_i \ge 0, \quad \sum \xi_i \le constant, \quad i= 1, ..., N,
        \end{split}\]</span> where <span class="math inline">\(\beta\)</span> are the coefficients and <span class="math inline">\(x\)</span> are the
<span class="math inline">\(N\)</span> data points. The support vectors are the points that determine the
orientation of the hyperplane (i.e.Â the closest points). The
classification function is given by:
<span class="math display">\[G(x) = \text{sign}[x^T\beta + \beta_0]\]</span> If you only calculate the
inner part of the function you can get the distance of a point to your
hyperplane (in SKlearn you need to divide by the norm vector <span class="math inline">\(w\)</span> of your
hyperplane to get the true distance). To get the probability of a point
being in a class, you can use Plattâs algorithm <span class="citation">[@Platt1999]</span>. SVMs are
sensitive to the scaling of the features. Therefore, the data should be
normalized before classification.<br />
</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="classification-methods.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb33-2"><a href="classification-methods.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb33-3"><a href="classification-methods.html#cb33-3" aria-hidden="true" tabindex="-1"></a>svc_model <span class="op">=</span> svm.SVC()</span>
<span id="cb33-4"><a href="classification-methods.html#cb33-4" aria-hidden="true" tabindex="-1"></a>svc_model.fit(train_df[feautre_cols], train_df[label_col])</span>
<span id="cb33-5"><a href="classification-methods.html#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># test the model</span></span>
<span id="cb33-6"><a href="classification-methods.html#cb33-6" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> svc_model.predict(test_df[feature_cols])</span></code></pre></div>
<!-- ### Stochastic Gradient Descent -->
</div>
<div id="decision-trees" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Decision Trees<a href="classification-methods.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A decision tree uses binary rules to recursively split the data into
regions that contain only a single class.</p>
<div class="figure">
<img src="figures/Decision_Tree.jpg" style="width:60.0%" alt="" />
<p class="caption">Decision trees work like flowcharts and have a root (the top node),
branches (possible outcomes of a test), nodes (tests on one attribute),
leafs (the class labels). This one shows the decision tree for the
classifier predicting if a patient survives the sinking of the Titanic.
Source: <a href="https://commons.wikimedia.org/wiki/File:Decision_Tree.jpg">user Gilgoldm on
wikipedia.org</a></p>
</div>
<p><strong>Pros:</strong></p>
<ul>
<li><p>Interpretable results, if trees are not too big.</p></li>
<li><p>Cheap to train and predict.</p></li>
<li><p>Can handle categorical and continuous data at the same time.</p></li>
<li><p>Can be used for <a href="https://scikit-learn.org/stable/modules/tree.html#multi-output-problems">multi-output
problems</a>
(e.g.Â color and shape of object).</p></li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Overfitting risks</li>
<li>Some concepts are hard to learn (X-OR relationships, Decision
boundaries are not smooth)</li>
<li>Unstable predictions: Small changes in data can lead to vastly
different decision trees.</li>
</ul>
<p><strong>Tips:</strong> - Doing PCA helps the tree find separating features. -
Visualize the produced tree. - Setting a lower boundary on the
split-sizes of the data, reduces the chance of overfitting. - Balance
the dataset or weight the samples according to class sizes to avoid
constructing biased trees.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="classification-methods.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb34-2"><a href="classification-methods.html#cb34-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">10</span>, min_samples_split<span class="op">=</span><span class="fl">0.01</span>, class_weight<span class="op">=</span><span class="st">&quot;balanced&quot;</span>)</span>
<span id="cb34-3"><a href="classification-methods.html#cb34-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> clf.fit(X, Y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">scikit-learn.org</a><br />
</p>
<p>Due to the overfitting risk and the instability, ensembles of decision
trees are commonly used.</p>
<div id="random-forests" class="section level4 hasAnchor" number="3.2.5.1">
<h4><span class="header-section-number">3.2.5.1</span> Random forests<a href="classification-methods.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Random forests are a version of a <a href="data-representation-analysis-processing.html#bagging">bagging</a> classifier
employing decision trees. To reduce the variance, the separate trees can
be assigned a limited number of features as well.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="classification-methods.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb35-2"><a href="classification-methods.html#cb35-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(max_depth<span class="op">=</span><span class="dv">10</span>, max_features<span class="op">=</span><span class="st">&quot;sqrt&quot;</span>, class_weight<span class="op">=</span><span class="st">&quot;balanced&quot;</span>)</span>
<span id="cb35-3"><a href="classification-methods.html#cb35-3" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">scikit-learn.org</a><br />
</p>
</div>
<div id="gradient-boosted-decision-trees" class="section level4 hasAnchor" number="3.2.5.2">
<h4><span class="header-section-number">3.2.5.2</span> Gradient boosted decision trees<a href="classification-methods.html#gradient-boosted-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Gradient boosted decision tree models are a form of
<a href="data-representation-analysis-processing.html#boosting">boosting</a> employing decision trees.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="classification-methods.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightgbm <span class="im">as</span> lgbm</span>
<span id="cb36-2"><a href="classification-methods.html#cb36-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> lgbm.LGBMClassifier(class_weight<span class="op">=</span> <span class="st">&quot;balanced&quot;</span>)</span>
<span id="cb36-3"><a href="classification-methods.html#cb36-3" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span></code></pre></div>
<p>More info: <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm-lgbmclassifier">lightgbm
documentation</a>,
<a href="https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html">Parameter
tuning</a>,<br />
<a href="https://neptune.ai/blog/lightgbm-parameters-guide">Further Parameter
tuning</a><br />
Similar model:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">scikit-learn.org</a><br />
</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-representation-analysis-processing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/MoritzGuck/Machine-Learning-Reference/edit/master/03-Classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
