<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Probability Theory &amp; Linear Algebra | Machine Learning Reference</title>
  <meta name="description" content="Chapter 1 Probability Theory &amp; Linear Algebra | Machine Learning Reference" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Probability Theory &amp; Linear Algebra | Machine Learning Reference" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="MoritzGuck/Machine-Learning-Reference" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Probability Theory &amp; Linear Algebra | Machine Learning Reference" />
  
  
  

<meta name="author" content="Moritz Gück" />


<meta name="date" content="2023-04-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="data-representation-analysis-processing.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>Machine Learning Reference</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html"><i class="fa fa-check"></i><b>1</b> Probability Theory &amp; Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-theory"><i class="fa fa-check"></i><b>1.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-basics"><i class="fa fa-check"></i><b>1.1.1</b> Probability Basics</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-distributions"><i class="fa fa-check"></i><b>1.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#CLT"><i class="fa fa-check"></i><b>1.1.3</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#bayesian-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian probability</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#further-concepts"><i class="fa fa-check"></i><b>1.1.5</b> Further Concepts</a></li>
<li class="chapter" data-level="1.1.6" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#statistical-tests"><i class="fa fa-check"></i><b>1.1.6</b> Statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#vectors"><i class="fa fa-check"></i><b>1.2.1</b> Vectors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html"><i class="fa fa-check"></i><b>2</b> Data: Representation, Analysis &amp; Processing</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-and-distance-measures"><i class="fa fa-check"></i><b>2.1</b> Similarity and Distance Measures</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#metrics"><i class="fa fa-check"></i><b>2.1.1</b> Metrics</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-measures-on-vectors"><i class="fa fa-check"></i><b>2.1.2</b> Similarity measures on vectors</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#kernels"><i class="fa fa-check"></i><b>2.1.3</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#data-analysis"><i class="fa fa-check"></i><b>2.2</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>2.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#output-analysis"><i class="fa fa-check"></i><b>2.2.2</b> Output Analysis</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#monitoring"><i class="fa fa-check"></i><b>2.2.3</b> Monitoring</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#preprocessing-data"><i class="fa fa-check"></i><b>2.3</b> Preprocessing data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#handling-missing-wrong-data"><i class="fa fa-check"></i><b>2.3.1</b> Handling missing &amp; wrong data</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#working-with-date-and-time"><i class="fa fa-check"></i><b>2.3.2</b> Working with Date and Time</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#encoding-discretizing-data"><i class="fa fa-check"></i><b>2.3.3</b> Encoding &amp; discretizing data</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#standardization"><i class="fa fa-check"></i><b>2.3.4</b> Standardization</a></li>
<li class="chapter" data-level="2.3.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#splitting-in-training--and-test-data"><i class="fa fa-check"></i><b>2.3.5</b> Splitting in training- and test-data</a></li>
<li class="chapter" data-level="2.3.6" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#feature-selection"><i class="fa fa-check"></i><b>2.3.6</b> Feature selection</a></li>
<li class="chapter" data-level="2.3.7" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>2.3.7</b> Hyper-parameter tuning</a></li>
<li class="chapter" data-level="2.3.8" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#model-selection"><i class="fa fa-check"></i><b>2.3.8</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#errors-regularization"><i class="fa fa-check"></i><b>2.4</b> Errors &amp; regularization</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bias-and-variance"><i class="fa fa-check"></i><b>2.4.1</b> Bias and Variance</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#regularization"><i class="fa fa-check"></i><b>2.4.2</b> Regularization</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bagging"><i class="fa fa-check"></i><b>2.4.3</b> Bagging</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#boosting"><i class="fa fa-check"></i><b>2.4.4</b> Boosting</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#stacking"><i class="fa fa-check"></i><b>2.4.5</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#tips-for-machine-learning-projects"><i class="fa fa-check"></i><b>2.5</b> Tips for machine learning projects</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#general-advice"><i class="fa fa-check"></i><b>2.5.1</b> General advice</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#common-mistakes"><i class="fa fa-check"></i><b>2.5.2</b> Common mistakes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-methods.html"><a href="classification-methods.html"><i class="fa fa-check"></i><b>3</b> Classification Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-methods.html"><a href="classification-methods.html#evaluation-of-classifiers"><i class="fa fa-check"></i><b>3.1</b> Evaluation of Classifiers</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-methods.html"><a href="classification-methods.html#confusion-matrix"><i class="fa fa-check"></i><b>3.1.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-methods.html"><a href="classification-methods.html#basic-quality-measures"><i class="fa fa-check"></i><b>3.1.2</b> Basic Quality Measures</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-methods.html"><a href="classification-methods.html#area-under-the-curve"><i class="fa fa-check"></i><b>3.1.3</b> Area under the Curve</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-methods.html"><a href="classification-methods.html#handling-unbalanced-data"><i class="fa fa-check"></i><b>3.1.4</b> Handling Unbalanced Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-methods.html"><a href="classification-methods.html#classification-algorithms"><i class="fa fa-check"></i><b>3.2</b> Classification Algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-methods.html"><a href="classification-methods.html#nearest-neighbors-classifier"><i class="fa fa-check"></i><b>3.2.1</b> Nearest Neighbors Classifier</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-methods.html"><a href="classification-methods.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>3.2.2</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-methods.html"><a href="classification-methods.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>3.2.3</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-methods.html"><a href="classification-methods.html#support-vector-classifier-svc"><i class="fa fa-check"></i><b>3.2.4</b> Support Vector Classifier (SVC)</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-methods.html"><a href="classification-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.2.5</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#custering-methods"><i class="fa fa-check"></i><b>4.1</b> Custering Methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#metrics-for-clustering-algorithms"><i class="fa fa-check"></i><b>4.1.1</b> metrics for Clustering algorithms</a></li>
<li class="chapter" data-level="4.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#graph-based-clustering"><i class="fa fa-check"></i><b>4.1.3</b> Graph-Based Clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#spectral-clustering"><i class="fa fa-check"></i><b>4.1.4</b> Spectral Clustering</a></li>
<li class="chapter" data-level="4.1.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#SSP"><i class="fa fa-check"></i><b>4.1.5</b> Sparse Subspace Clustering (SSP)</a></li>
<li class="chapter" data-level="4.1.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#soft-assignment-clustering"><i class="fa fa-check"></i><b>4.1.6</b> Soft-assignment Clustering</a></li>
<li class="chapter" data-level="4.1.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#artificial-neural-networks-for-clustering"><i class="fa fa-check"></i><b>4.1.7</b> Artificial Neural Networks for Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#mapping-to-lower-dimensions"><i class="fa fa-check"></i><b>4.2</b> Mapping to lower dimensions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#manifold-learning"><i class="fa fa-check"></i><b>4.2.1</b> Manifold learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#decomposition-techniques"><i class="fa fa-check"></i><b>4.2.2</b> Decomposition techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#outlier-detection"><i class="fa fa-check"></i><b>4.3</b> Outlier detection</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#local-outlier-factor"><i class="fa fa-check"></i><b>4.3.1</b> Local outlier factor</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#isolation-forest"><i class="fa fa-check"></i><b>4.3.2</b> Isolation forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generative-models.html"><a href="generative-models.html"><i class="fa fa-check"></i><b>5</b> Generative models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="generative-models.html"><a href="generative-models.html#generative-models-for-discrete-data"><i class="fa fa-check"></i><b>5.1</b> Generative Models for Discrete Data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="generative-models.html"><a href="generative-models.html#bayesian-concept-learning"><i class="fa fa-check"></i><b>5.1.1</b> Bayesian Concept Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="generative-models.html"><a href="generative-models.html#beta-binomial-model"><i class="fa fa-check"></i><b>5.1.2</b> Beta-binomial model</a></li>
<li class="chapter" data-level="5.1.3" data-path="generative-models.html"><a href="generative-models.html#dirichlet-multinomial-model"><i class="fa fa-check"></i><b>5.1.3</b> Dirichlet-multinomial model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#evaluation-of-regression-models"><i class="fa fa-check"></i><b>6.1</b> Evaluation of regression models</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#mean-squared-error"><i class="fa fa-check"></i><b>6.1.1</b> Mean squared error</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#r2-score-coefficient-of-determination"><i class="fa fa-check"></i><b>6.1.2</b> R^2 score / coefficient of determination</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#visual-tools"><i class="fa fa-check"></i><b>6.1.3</b> Visual tools</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#linear-models"><i class="fa fa-check"></i><b>6.2</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>6.2.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#lasso-regression"><i class="fa fa-check"></i><b>6.2.2</b> Lasso regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="regression.html"><a href="regression.html#ridge-regression"><i class="fa fa-check"></i><b>6.2.3</b> Ridge regression</a></li>
<li class="chapter" data-level="6.2.4" data-path="regression.html"><a href="regression.html#bayesian-regression"><i class="fa fa-check"></i><b>6.2.4</b> Bayesian regression</a></li>
<li class="chapter" data-level="6.2.5" data-path="regression.html"><a href="regression.html#anova"><i class="fa fa-check"></i><b>6.2.5</b> ANOVA</a></li>
<li class="chapter" data-level="6.2.6" data-path="regression.html"><a href="regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>6.2.6</b> Generalized linear models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.3</b> Gaussian process regression</a></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#gradient-boosted-tree-regression"><i class="fa fa-check"></i><b>6.4</b> Gradient boosted tree regression</a></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#time-series-forecasting"><i class="fa fa-check"></i><b>6.5</b> Time Series Forecasting</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#arimax-model"><i class="fa fa-check"></i><b>6.5.1</b> ARIMA(X) Model</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#varmmax-model"><i class="fa fa-check"></i><b>6.5.2</b> VARMMA(X) Model</a></li>
<li class="chapter" data-level="6.5.3" data-path="regression.html"><a href="regression.html#prophet-model"><i class="fa fa-check"></i><b>6.5.3</b> Prophet-Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Networks {#Neural Networks}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#non-linearities"><i class="fa fa-check"></i><b>7.1.1</b> Non-Linearities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#feedforward-neural-network-multi-layer-perceptron"><i class="fa fa-check"></i><b>7.2</b> Feedforward Neural Network / Multi-Layer Perceptron</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>7.2.1</b> Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>7.3</b> Convolutional Neural Networks</a></li>
<li class="chapter" data-level="7.4" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>7.4</b> Autoencoders</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders-for-clustering"><i class="fa fa-check"></i><b>7.4.1</b> Autoencoders for clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>7.5</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="7.6" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>7.6</b> Recurrent neural networks</a></li>
<li class="chapter" data-level="7.7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#long-short-term-memory-networks"><i class="fa fa-check"></i><b>7.7</b> Long short-term memory networks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html"><i class="fa fa-check"></i><b>8</b> Machine learning Automation &amp; Productivity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#holistic-approach"><i class="fa fa-check"></i><b>8.1</b> Holistic approach</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autosklearn"><i class="fa fa-check"></i><b>8.1.1</b> Autosklearn</a></li>
<li class="chapter" data-level="8.1.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#flaml"><i class="fa fa-check"></i><b>8.1.2</b> FLAML</a></li>
<li class="chapter" data-level="8.1.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#h2o.ai"><i class="fa fa-check"></i><b>8.1.3</b> H2O.ai</a></li>
<li class="chapter" data-level="8.1.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autokeras"><i class="fa fa-check"></i><b>8.1.4</b> AutoKeras</a></li>
<li class="chapter" data-level="8.1.5" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#fastai"><i class="fa fa-check"></i><b>8.1.5</b> Fastai</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#model-selection-automation"><i class="fa fa-check"></i><b>8.2</b> Model selection automation</a></li>
<li class="chapter" data-level="8.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#feature-engineering-automation"><i class="fa fa-check"></i><b>8.3</b> Feature engineering automation</a></li>
<li class="chapter" data-level="8.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#cookie-cutter-visualizations"><i class="fa fa-check"></i><b>8.4</b> cookie-cutter Visualizations</a></li>
<li class="chapter" data-level="8.5" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#comprehensive-validation-reports"><i class="fa fa-check"></i><b>8.5</b> Comprehensive validation reports</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html"><i class="fa fa-check"></i><b>9</b> Machine Learning Project Management</a>
<ul>
<li class="chapter" data-level="9.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#basis-for-machine-learning-in-companies"><i class="fa fa-check"></i><b>9.1</b> Basis for Machine Learning in Companies</a></li>
<li class="chapter" data-level="9.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-automate-business-processes-with-machine-learning"><i class="fa fa-check"></i><b>9.2</b> How to automate business processes with machine learning</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#the-stages-from-manual-to-ml"><i class="fa fa-check"></i><b>9.2.1</b> The stages from ManuaL to ML</a></li>
<li class="chapter" data-level="9.2.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#phases-of-the-ml-project"><i class="fa fa-check"></i><b>9.2.2</b> Phases of the ML project</a></li>
<li class="chapter" data-level="9.2.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-frame-ml-problems"><i class="fa fa-check"></i><b>9.2.3</b> How to frame ML problems</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#common-pitfalls-in-machine-learning"><i class="fa fa-check"></i><b>9.3</b> Common pitfalls in machine learning</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Reference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-theory-linear-algebra" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Probability Theory &amp; Linear Algebra<a href="probability-theory-linear-algebra.html#probability-theory-linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="probability-theory" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Probability Theory<a href="probability-theory-linear-algebra.html#probability-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A probability is a measure of how frequent or likely an event will take
place.</p>
<div id="probability-basics" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Probability Basics<a href="probability-theory-linear-algebra.html#probability-basics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="probability-interpretations" class="section level4 hasAnchor" number="1.1.1.1">
<h4><span class="header-section-number">1.1.1.1</span> Probability interpretations<a href="probability-theory-linear-algebra.html#probability-interpretations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Frequentist:</strong> Fraction of positive samples, if we measured
infinitely many samples.</p></li>
<li><p><strong>Objectivist:</strong> Probabilities are due to inherent uncertainty
properties. Probabilities are calculated by putting outcomes of
interest into relation with all possible outcomes.</p></li>
<li><p><strong>Subjectivist:</strong> An agent’s <em>rational</em> degree of belief (not
external). The belief needs to be coherent (i.e. if you make bets
using your probabilities you should not be guaranteed lose money)
and therefore need to follow the rules of probability.</p></li>
<li><p><strong>Bayesian:</strong> (Building on subjectivism) A reasonable expectation /
degree of belief based on the information available to the
statistician / system. It allows to give certainties to events,
where we don’t have samples on (e.g. disappearance of the south pole
until 2030).</p></li>
</ul>
<p>Also the frequentist view is not free of subjectivity since you need to
compare events on otherwise similar objects. Usually there are no
completely similar objects, so you need to define them.</p>
</div>
<div id="probability-space" class="section level4 hasAnchor" number="1.1.1.2">
<h4><span class="header-section-number">1.1.1.2</span> Probability Space<a href="probability-theory-linear-algebra.html#probability-space" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The probability space is a triplet space containing a sample/outcome
space <span class="math inline">\(\Omega\)</span> (containing all possible atomic events), a collection of
events <span class="math inline">\(S\)</span> (containing a subset of <span class="math inline">\(\Omega\)</span> to which we want to assign
probabilities) and the mapping <span class="math inline">\(P\)</span> between <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(S\)</span>.</p>
</div>
<div id="axioms-of-probability" class="section level4 hasAnchor" number="1.1.1.3">
<h4><span class="header-section-number">1.1.1.3</span> Axioms of Probability<a href="probability-theory-linear-algebra.html#axioms-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The mapping <span class="math inline">\(P\)</span> must fulfill the axioms of probability:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P(a) \geq 0\)</span></p></li>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span></p></li>
<li><p><span class="math inline">\(a,b \in S\)</span> and <span class="math inline">\(a \cap b = \{\}\)</span>
<span class="math inline">\(\Rightarrow P(a \cup b) = P(a) + P(b)\)</span></p></li>
</ol>
<p><span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> are events.</p>
</div>
<div id="random-variable-rv" class="section level4 hasAnchor" number="1.1.1.4">
<h4><span class="header-section-number">1.1.1.4</span> Random Variable (RV)<a href="probability-theory-linear-algebra.html#random-variable-rv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A RV is a <strong>function</strong> that maps points from the sample space <span class="math inline">\(\Omega\)</span>
to some range (e.g. Real numbers or booleans). They are characterized by
their distribution function. E.g. for a coin toss:
<span class="math display">\[X(\omega) = \begin{cases}
                0, \text{ if } \omega = heads\\
                1, \text{ if } \omega = tails.
            \end{cases}\]</span></p>
</div>
<div id="proposition" class="section level4 hasAnchor" number="1.1.1.5">
<h4><span class="header-section-number">1.1.1.5</span> Proposition<a href="probability-theory-linear-algebra.html#proposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A Proposition is a conclusion of a statistical inference/prediction that
can be true or false (e.g. a classification of a datapoint). More
formally: A disjunction of events where the logic model holds. An event
can be written as a <strong>propositional logic model</strong>:<br />
<span class="math inline">\(A = true, B = false \Rightarrow a \land \neg b\)</span>. Propositions can be
continuous, discrete or boolean.</p>
</div>
</div>
<div id="probability-distributions" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Probability distributions<a href="probability-theory-linear-algebra.html#probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Probability distributions assign probabilities to to all possible points
in <span class="math inline">\(\Omega\)</span> (e.g. <span class="math inline">\(P(Weather) = \langle 0.3, 0.4, 0.2, 0.1 \rangle\)</span>,
representing Rain, sunshine, clouds and snow). Joint probability
distributions give you a probability for each atomic event of the RVs
(e.g. <span class="math inline">\(P(weather, accident)\)</span> gives you a <span class="math inline">\(2\times 4\)</span> matrix.)</p>
<div id="cumulative-distribution-function-cdf" class="section level4 hasAnchor" number="1.1.2.1">
<h4><span class="header-section-number">1.1.2.1</span> Cumulative Distribution Function (CDF)<a href="probability-theory-linear-algebra.html#cumulative-distribution-function-cdf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The CDF is defined as <span class="math inline">\(F_X(x) = P(X \leq x)\)</span> (See figure
<a href="probability-theory-linear-algebra.html#CDF" reference-type="ref" reference="CDF"><span class="math inline">\(CDF\)</span></a>).</p>
<div class="figure">
<img src="figures/Normal_Distribution_CDF.png" id="CDF" style="width:60.0%" alt="" />
<p class="caption">Cumulative distribution function of a normal distribution for
different mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma\)</span>). <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Normal_Distribution_CDF.svg">user
Inductiveload on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="probability-density-function-pdf" class="section level4 hasAnchor" number="1.1.2.2">
<h4><span class="header-section-number">1.1.2.2</span> Probability Density Function (PDF)<a href="probability-theory-linear-algebra.html#probability-density-function-pdf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For continuous functions the PDF is defined by
<span class="math display">\[p(x) =  {d \over dx} p(X \leq x).\]</span> The probability of x being in a
finite interval is <span class="math display">\[P(a &lt; X \leq b) = \int_a^b p(x) dx\]</span> A PDF is shown
in the following figure.</p>
<div class="figure">
<img src="figures/Boxplot_vs_PDF.png" id="Boxplot" style="width:60.0%" alt="" />
<p class="caption">Probability density function of a normal distribution with variance
(<span class="math inline">\(\sigma\)</span>). In red a range from a Box-plot is shown with
<a href="probability-theory-linear-algebra.html#dist_prop">quartiles</a> (Q1, Q3) and interquartile range (IQR). For the
cutoffs (borders to darker blue regions) the IQR (on top) and <span class="math inline">\(\sigma\)</span>
are chosen. Another common cutoff is the confidence interval with light
blue regions having a probability mass of <span class="math inline">\(2 * \alpha / 2\)</span>. <em>Source:
<a href="https://commons.wikimedia.org/wiki/File:Boxplot_vs_PDF.svg">user Jhguch on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="dist_prop" class="section level4 hasAnchor" number="1.1.2.3">
<h4><span class="header-section-number">1.1.2.3</span> Properties of Distributions<a href="probability-theory-linear-algebra.html#dist_prop" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The <strong>expected value</strong> (<span class="math inline">\(E\)</span>) or <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) is given by
<span class="math inline">\(E[X] = \sum_{x \in X} x*p(x)\)</span> for discrete RVs and
<span class="math inline">\(E[X] = \int_X x*p(x) dx\)</span> for continuous RVs.</p></li>
<li><p>The <strong>variance</strong> measures the spread of a distribution:
<span class="math inline">\(var[X] = \sigma^2 = E[(X-\mu)^2] = E[X]^2 - \mu^2\)</span>.</p></li>
<li><p>The <strong>standard deviation</strong> is given by: <span class="math inline">\(\sqrt{var[X]} = \sigma\)</span>. It is interpreted as the deviation from the mean that needs to be expected.</p></li>
<li><p>The <strong>mode</strong> is the value with the highest probability (or the point
in the PDF with the highest value):</p></li>
<li><p>The <strong>median</strong> is the point at which all point less than the median
and all points greater than the median have the same probability
(<span class="math inline">\(0.5\)</span>).</p></li>
<li><p>The <strong>quantiles</strong> (<span class="math inline">\(Q\)</span>) divide the datapoints into sets of equal
number. The <span class="math inline">\(Q_1\)</span> qua<strong>r</strong>tile has 25% of the values below it. The
<strong>interquartile range</strong> (IQR) is a measure to show the variability
in the data (how distant the points from the first and last quartile
are)</p></li>
</ul>
</div>
<div id="diracdelta" class="section level4 hasAnchor" number="1.1.2.4">
<h4><span class="header-section-number">1.1.2.4</span> Dirac delta function<a href="probability-theory-linear-algebra.html#diracdelta" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>dirac delta</strong> is simply a function that is infinite at one point
and 0 everywhere else:
<span class="math display">\[\delta(x)=\begin{cases} \infty , \; \text{ if } x = 0 \\0, \quad \text{if } x \neq 0 \end{cases} \qquad \text{and } \int_{-\infty}^{\infty} \delta(x) dx = 1\]</span>
(Needed for distributions further on)</p>
</div>
<div id="uniform-distribution" class="section level4 hasAnchor" number="1.1.2.5">
<h4><span class="header-section-number">1.1.2.5</span> Uniform distribution<a href="probability-theory-linear-algebra.html#uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The uniform distribution has the same probability throughout a specific
interval:
<span class="math display">\[\text{Unif}(a,b) = \frac{1}{b-a} 1 \mkern-6mu 1 (a &lt; x \leq b) = \begin{cases}
                \frac{1}{b-a}, \quad \text{if } x \in [ a,b ] \\
                0, \qquad \text{else}
            \end{cases}\]</span> <span class="math inline">\(1 \mkern-6mu 1\)</span> is a vector of ones.</p>
<div class="figure">
<img src="figures/Uniform_Distribution.png" style="width:40.0%" alt="" />
<p class="caption">Uniform distribution. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Uniform_Distribution_PDF_SVG.svg">user IkamusumeFan on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="discrete-distributions" class="section level4 hasAnchor" number="1.1.2.6">
<h4><span class="header-section-number">1.1.2.6</span> Discrete distributions<a href="probability-theory-linear-algebra.html#discrete-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Used for random variables that have discrete states.</p>
<div id="binomial-distribution" class="section level5 hasAnchor" number="1.1.2.6.1">
<h5><span class="header-section-number">1.1.2.6.1</span> Binomial distribution<a href="probability-theory-linear-algebra.html#binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Used for series of experiments with two outcomes (success or miss. e.g.
a series of coin flips).
<span class="math display">\[X \sim \text{Bin}(n, \theta ), \quad \text{Bin}(k|n,\theta)={n \choose k} \theta^k (1-\theta)^{n-k} , \quad {n \choose k} = \frac{n!}{k!(n-k)!},\]</span>
where <span class="math inline">\(n\)</span> is the number of total experiments, <span class="math inline">\(k\)</span> is the number of
successful experiments and <span class="math inline">\(\theta\)</span> is the probability of success of an
experiment.</p>
<div class="figure">
<img src="figures/Pascals_triangle_binomial_distribution.png" alt="" />
<p class="caption">Binomial distribution of balls in <a href="https://en.wikipedia.org/wiki/Pascal%27s_triangle">Pascals
triangles</a> with
different numbers of layers (The top one has 0 layers). Example: For a
triangle with <span class="math inline">\(n=6\)</span> layers, the probability that a ball lands in the
middle box <span class="math inline">\(k=3\)</span> is <span class="math inline">\(20/64\)</span>. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Pascal%27s_triangle;_binomial_distribution.svg">user Watchduck on
wikimedia.org</a></em></p>
</div>
</div>
<div id="bernoulli-distribution" class="section level5 hasAnchor" number="1.1.2.6.2">
<h5><span class="header-section-number">1.1.2.6.2</span> Bernoulli distribution<a href="probability-theory-linear-algebra.html#bernoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Is a special case of the binomial distribution with <span class="math inline">\(n=1\)</span> (e.g. one coin
toss).
<span class="math display">\[X \sim \text{Ber}(\theta ), \quad \text{Ber}(x | \theta)=\theta^{1 \mkern-6mu 1 (x=1)} (1-\theta)^{1 \mkern-6mu 1(x=0)}= \begin{cases}
                    \theta, \qquad \text{if } x=1 \\
                    1 - \theta, \; \text{if } x=0
                \end{cases}\]</span></p>
</div>
<div id="multinomial-distribution" class="section level5 hasAnchor" number="1.1.2.6.3">
<h5><span class="header-section-number">1.1.2.6.3</span> Multinomial distribution<a href="probability-theory-linear-algebra.html#multinomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Used for experiments with k different outcomes (e.g. dice rolls:
Probability of different counts of the different sides).
<span class="math display">\[\text{Mu}(x|n,\theta) =  {n \choose x_1, ..., x_K}\prod_{i=1}^K\theta_j^{x_j} = \frac{n!}{x_1!, ..., x_k!}\prod_{i=1}^K\theta_j^{x_j},\]</span>
where <span class="math inline">\(k\)</span> is the number of outcomes, <span class="math inline">\(x_j\)</span> is the number times that
outcome <span class="math inline">\(j\)</span> happens. <span class="math inline">\(X = (X_1, ..., X_K)\)</span> is the <em>random vector</em>.</p>
</div>
<div id="multinoulli-distribution" class="section level5 hasAnchor" number="1.1.2.6.4">
<h5><span class="header-section-number">1.1.2.6.4</span> Multinoulli distribution<a href="probability-theory-linear-algebra.html#multinoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Is a special case of the multinomial distribution with <span class="math inline">\(n=1\)</span>. The random
vector is then represented in <em>dummy-</em> or <em>one-hot-encoding</em> (e.g.
<span class="math inline">\((0,0,1,0,0,0)\)</span> if outcome 3 takes place).
<span class="math display">\[\text{Mu}(x|1,\theta) = \prod_{j=0}^K \theta_j^{1 \mkern-6mu 1(x_j=1)}\]</span></p>
</div>
<div id="empirical-distribution" class="section level5 hasAnchor" number="1.1.2.6.5">
<h5><span class="header-section-number">1.1.2.6.5</span> Empirical distribution<a href="probability-theory-linear-algebra.html#empirical-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The empirical distribution follows the empirical measurements strictly.
The CDF jumps by 1/n every time a sample is “encountered” (see figure).</p>
<p><span class="math display">\[\text{p}_{\text{emp}}(A) = \frac{1}{N} \sum_{i=1}^N \delta_{x_i}(A), \quad \delta_{x_i}=\begin{cases}1, \quad \text{if } x \in A \\0, \quad \text{if } x \notin A \end{cases},\]</span>
w where <span class="math inline">\(x_1, ..., x_N\)</span> is a data set with N points. The points can also
be weighted: <span class="math display">\[p(x) =  \sum_{i=1}^N w_i \delta_{x_i}(x)\]</span></p>
<div class="figure">
<img src="figures/Empirical_distribution_function.png" style="width:50.0%" alt="" />
<p class="caption">Cumulative empirical distribution function (blue line) for samples
drawn from a standard normal distribution (green line). The values of
the drawn samples is shown as grey lines at the bottom. Source: <a href="https://commons.wikimedia.org/wiki/File:Empirical_distribution_function.png">user
nagualdesign on
wikimedia.org.</a></p>
</div>
</div>
</div>
<div id="continuous-distributions" class="section level4 hasAnchor" number="1.1.2.7">
<h4><span class="header-section-number">1.1.2.7</span> Continuous distributions<a href="probability-theory-linear-algebra.html#continuous-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Used for random variables that have continuous states.</p>
<div id="normalgaussian-distribution" class="section level5 hasAnchor" number="1.1.2.7.1">
<h5><span class="header-section-number">1.1.2.7.1</span> Normal/Gaussian distribution<a href="probability-theory-linear-algebra.html#normalgaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Often chosen for random noise because it is simple and needs few
assumptions (see sect. <a href="probability-theory-linear-algebra.html#CLT">CLT</a>). The PDF is given by:</p>
<p><span class="math display">\[p(x|\mu\sigma^2)= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right],\]</span>
where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance. The CDF is given
by:
<span class="math display">\[\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{\infty}^xe^{\frac{-t^2}{2}dt}\]</span></p>
</div>
<div id="multivariate-normalgaussian-distribution" class="section level5 hasAnchor" number="1.1.2.7.2">
<h5><span class="header-section-number">1.1.2.7.2</span> Multivariate normal/Gaussian distribution<a href="probability-theory-linear-algebra.html#multivariate-normalgaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>For T datapoints with k dimensions (features). The pdf is:
<span class="math display">\[p(x|\mu,\Sigma) = \dfrac{1}{\sqrt{(2\pi)^k|\Sigma|}}\exp\left[-\dfrac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu)\right],\]</span>
where x now has multiple dimension (<span class="math inline">\(x_1, x_2, ..., x_k\)</span>) and <span class="math inline">\(\Sigma\)</span>
is the <span class="math inline">\(k \times k\)</span> covariance matrix:
<span class="math inline">\(\Sigma = \text{E}[(X-\mu)(X-\mu)]\)</span>. The covariance between features is:
<span class="math inline">\(\text{Cov}[X_i, X_j] = \text{E}[(X_i-\mu_i)(X_j-\mu_j)]\)</span></p>
</div>
<div id="beta-distribution" class="section level5 hasAnchor" number="1.1.2.7.3">
<h5><span class="header-section-number">1.1.2.7.3</span> Beta distribution<a href="probability-theory-linear-algebra.html#beta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>defined for <span class="math inline">\(0 \leq x \leq 1\)</span> (see figure <a href="#Beta_distr">Beta
distribution</a>). The pdf is:
<span class="math display">\[f(x|\alpha, \beta) = \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\]</span>
The <a href="https://en.wikipedia.org/wiki/Beta_function">beta function</a> <span class="math inline">\(B\)</span> is
there to normalize and ensure that the total probability is 1 .</p>
<div class="figure">
<img src="figures/Beta_distribution_pdf.png" style="width:60.0%" label="Beta_distr" alt="" />
<p class="caption">Probability density function of a beta-distribution with different
parameter values. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Beta_distribution_pdf.png">user MarkSweep on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="dirichlet-distribution" class="section level5 hasAnchor" number="1.1.2.7.4">
<h5><span class="header-section-number">1.1.2.7.4</span> Dirichlet distribution<a href="probability-theory-linear-algebra.html#dirichlet-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The multivariate version of the Beta distribution. The PDF is:
<span class="math display">\[\text{Dir}({x}|{\alpha}) \triangleq \dfrac{1}{B({\alpha})}\prod\limits_{i=1}^K x_i^{\alpha_i-1},\quad \sum_{i=1}^K x_i =1, \quad x_i \geq 0 \text{ }\forall i\]</span></p>
<div class="figure">
<img src="figures/Dirichlet_distributions.png" style="width:60.0%" label="#Dirichlet_distr" alt="" />
<p class="caption">Probability density function of a Dirichlet-distribution on a
2-simplex (triangle) with different parameter values. Clockwise from top
left: <span class="math inline">\(\alpha\)</span> = (6,2,2), (3,7,5), (6,2,6), (2,3,4). <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Dirichlet_distributions.png">user ThG
on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="marginal-distributions" class="section level5 hasAnchor" number="1.1.2.7.5">
<h5><span class="header-section-number">1.1.2.7.5</span> Marginal distributions<a href="probability-theory-linear-algebra.html#marginal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Are the probability distributions of subsets of the original
distribution. Marginal distributions of normal distributions are also
normal distributions.</p>
<div class="figure">
<img src="figures/Multivariate_gaussian.png" style="width:60.0%" alt="" />
<p class="caption">Data following a 2D-Gaussian distribution. Marginal distributions are
shown on the sides in blue and orange. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Multivariate_Gaussian_inequality_demonstration.svg">user Auguel on
wikimedia.org</a>.</em></p>
</div>
</div>
</div>
</div>
<div id="CLT" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Central limit theorem<a href="probability-theory-linear-algebra.html#CLT" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In many cases the sum of random variables will follow a normal
distribution as n goes to infinity.</p>
</div>
<div id="bayesian-probability" class="section level3 hasAnchor" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Bayesian probability<a href="probability-theory-linear-algebra.html#bayesian-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Baeysian probability represents the plausibility of a proposition based
on the available information (i.e. the degree at which the information
supports the proposition). The use of this form of statistics is
especially useful if random variables cannot be assumed to be i.i.d.
(i.e. When an event is not independent of the event before it (e.g.
drawing balls without laying them back into the urn)).</p>
<div id="conditionalposterior-probability" class="section level4 hasAnchor" number="1.1.4.1">
<h4><span class="header-section-number">1.1.4.1</span> Conditional/Posterior Probability<a href="probability-theory-linear-algebra.html#conditionalposterior-probability" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Expresses the probability of one event (<span class="math inline">\(Y\)</span>) under the condition that
another event (<span class="math inline">\(E\)</span>) has occurred. (e.g. <span class="math inline">\(C\)</span> = “gets cancer”, <span class="math inline">\(S\)</span> = “is a
smoker” <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(p(C|S)=0.2\)</span>, meaning: “given the <em>sole
information</em> that someone is a smoker, their probability of getting
cancer is 20%.”)<br />
<br />
The conditional probability can be calculated like follows. By defining
the joined probability like so: <span class="math display">\[P(A \cap B) = P(A \mid B) P(B)\]</span> you
solve for <span class="math inline">\(P(A \mid B)\)</span>:
<span class="math display">\[P(A \mid B) = \frac{P(A \cap B)}{P(B)}=\frac{P(A, B)}{P(B)}=\alpha{P(A, B)},\]</span>
where <span class="math inline">\(\alpha\)</span> is used as a normalization constant. If you have hidden
variables (confounding factors) you need to sum them out like so:
<span class="math display">\[P(Y|E=e)=\alpha P(Y,E=e)=\alpha\sum_h P(Y,E=e,H=h)\]</span> where <span class="math inline">\(X\)</span>
contains all variables, <span class="math inline">\(Y\)</span> is called <em>query variable</em>, <span class="math inline">\(E\)</span> is called
<em>evidence variable</em>, <span class="math inline">\(H=X-Y-E\)</span> is called <em>hidden variable</em> or
<em>confounding factor</em>. You get the joint probabilities by summing out the
hidden variable. <!--# check if the formula is correct --></p>
<p><strong>!</strong> Usually <span class="math inline">\(p(A|B) \neq p(B|A)\)</span><br />
<strong>!</strong> Priors are often forgotten: E.g. <span class="math inline">\(P(\text{&quot;COVID-19&quot;})\)</span> is
confused with <span class="math inline">\(P(\text{&quot;COVID-19&quot;}|\text{&quot;Person is getting tested&quot;})\)</span>
(because only people with symptoms go to the testing station).<br />
<strong>!</strong> Base rate neglect: Under-representing the prior probability. E.g.
You have a test with a 5% false positive rate and a incidence of disease
of 2% in the population. If you are tested positive in a population
screening your probability of having the disease is only 29%.<br />
Conditional distributions of Gaussian distributions are Gaussian
distributions themselves.</p>
</div>
<div id="independence" class="section level4 hasAnchor" number="1.1.4.2">
<h4><span class="header-section-number">1.1.4.2</span> Independence<a href="probability-theory-linear-algebra.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For independent variables it holds: <span class="math inline">\(P(A|B)=P(A)\)</span> or <span class="math inline">\(P(B|A)=P(B)\)</span></p>
</div>
<div id="conditional-independence" class="section level4 hasAnchor" number="1.1.4.3">
<h4><span class="header-section-number">1.1.4.3</span> Conditional independence<a href="probability-theory-linear-algebra.html#conditional-independence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, given <span class="math inline">\(C\)</span>: <span class="math inline">\(P(A|B,C)=P(A|C)\)</span>.
<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> must not have any information on each other, given the
information on <span class="math inline">\(C\)</span>. E.g. for school children:
<span class="math inline">\(P(\text{&quot;vocabulary&quot;}|\text{&quot;height&quot;}, \text{&quot;age&quot;})= P(\text{&quot;vocabulary&quot;}|\text{&quot;age&quot;})\)</span>.</p>
</div>
<div id="bayes-rule" class="section level4 hasAnchor" number="1.1.4.4">
<h4><span class="header-section-number">1.1.4.4</span> Bayes Rule<a href="probability-theory-linear-algebra.html#bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Bayes rule is a structured approach to update prior beliefs /
probabilities with new information (data). With the conditional
probability from before (<span class="math inline">\(P(A,B)=P(A|B)P(B)=P(B|A)P(A)\)</span>) we get <strong>Bayes
rule</strong> by transforming the right-side equation
to:<span class="math display">\[P(\text{hypothesis}|\text{evidence}) =\dfrac{P(\text{evidence}|\text{hypothesis})P(\text{hypothesis})}{P(\text{evidence})}\]</span>
often used as:
<span class="math display">\[P(\text{model}|\text{data}) =\dfrac{P(\text{data}|\text{model})P(\text{model})}{P(\text{data})}\]</span></p>
<div id="terminology" class="section level5 hasAnchor" number="1.1.4.4.1">
<h5><span class="header-section-number">1.1.4.4.1</span> Terminology:<a href="probability-theory-linear-algebra.html#terminology" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><p><span class="math inline">\(P(\text{hypothesis}|\text{evidence})\)</span> = Posterior (How probable
hypothesis is after incorporating new evidence)</p></li>
<li><p><span class="math inline">\(P(\text{evidence}|\text{hypothesis})\)</span> = Likelihood (How probable
the evidence is, if the hypothesis is true)</p></li>
<li><p><span class="math inline">\(P(\text{hypothesis})\)</span> = Prior (How probable hypothesis was before
seeing evidence)</p></li>
<li><p><span class="math inline">\(P(\text{evidence})\)</span> = Marginal (How probable evidence is under all
possible hypotheses)</p></li>
<li><p><span class="math inline">\(\dfrac{P(\text{evidence}|\text{hypothesis})}{P(\text{evidence})}\)</span> =
Support <span class="math inline">\(B\)</span> provides for <span class="math inline">\(A\)</span></p></li>
<li><p><span class="math inline">\(P(\text{data}|\text{model})P(\text{model})\)</span> = joint probability
(<span class="math inline">\(P(A,B)\)</span>)</p></li>
</ul>
</div>
<div id="example-for-bayes-rule-using-covid-19-diagnostics" class="section level5 hasAnchor" number="1.1.4.4.2">
<h5><span class="header-section-number">1.1.4.4.2</span> Example for Bayes Rule using COVID-19 Diagnostics<a href="probability-theory-linear-algebra.html#example-for-bayes-rule-using-covid-19-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[P(\text{COVID-19}|\text{cough}) =\dfrac{P(\text{cough}|\text{COVID-19})P(\text{COVID-19})}{P(\text{cough})} = \frac{0.7*0.01}{0.1}=0.07\]</span>
Estimating <span class="math inline">\(P(\text{COVID-19}|\text{cough})\)</span> is difficult, because there
can be an outbreak and the number changes. However,
<span class="math inline">\(P(\text{cough}|\text{COVID-19})\)</span> stays stable, <span class="math inline">\(P(\text{COVID-19})\)</span> and
<span class="math inline">\(P(\text{cough})\)</span> can be easily determined.</p>
</div>
</div>
</div>
<div id="further-concepts" class="section level3 hasAnchor" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Further Concepts<a href="probability-theory-linear-algebra.html#further-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="convergence-in-probability-of-random-variables" class="section level4 hasAnchor" number="1.1.5.1">
<h4><span class="header-section-number">1.1.5.1</span> Convergence in Probability of Random Variables<a href="probability-theory-linear-algebra.html#convergence-in-probability-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>You expect your random variables (<span class="math inline">\(X_i\)</span>) to converge to an expected
random variable <span class="math inline">\(X\)</span>. I.e. after looking at infinite samples, the
probability that your random variable <span class="math inline">\(X_n\)</span> differs more than a
threshold <span class="math inline">\(\epsilon\)</span> from your target <span class="math inline">\(X\)</span> should be zero.
<span class="math display">\[\lim_{n \rightarrow \infty} P(|X_n - X| &gt; \epsilon) = 0\]</span></p>
</div>
<div id="bernoullis-theorem-weak-law-of-large-numbers" class="section level4 hasAnchor" number="1.1.5.2">
<h4><span class="header-section-number">1.1.5.2</span> Bernoulli’s Theorem / Weak Law of Large Numbers<a href="probability-theory-linear-algebra.html#bernoullis-theorem-weak-law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\lim_{n \rightarrow \infty} P(|\frac{\sum_{i=1}^n X_i}{n} - \mu| &gt; \epsilon) = 0,\]</span>
where <span class="math inline">\(X_1,...,X_n\)</span> are independent &amp; identically distributed (i.i.d.)
RVs. <span class="math inline">\(\Rightarrow\)</span> With enough samples, the sample mean will approach
the true mean. The <strong>strong law of large numbers</strong> states that
<span class="math inline">\(|\frac{\sum_{i=1}^n X_i}{n} - \mu| &lt; \epsilon\)</span> for any <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<p><font color="grey"></p>
</div>
</div>
<div id="statistical-tests" class="section level3 hasAnchor" number="1.1.6">
<h3><span class="header-section-number">1.1.6</span> Statistical tests<a href="probability-theory-linear-algebra.html#statistical-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="t-test" class="section level4 hasAnchor" number="1.1.6.1">
<h4><span class="header-section-number">1.1.6.1</span> T-Test<a href="probability-theory-linear-algebra.html#t-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="z-test" class="section level4 hasAnchor" number="1.1.6.2">
<h4><span class="header-section-number">1.1.6.2</span> Z-Test<a href="probability-theory-linear-algebra.html#z-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="chi-squared-test" class="section level4 hasAnchor" number="1.1.6.3">
<h4><span class="header-section-number">1.1.6.3</span> Chi-Squared test<a href="probability-theory-linear-algebra.html#chi-squared-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="statistical-tests-in-bayesian-statistics" class="section level4 hasAnchor" number="1.1.6.4">
<h4><span class="header-section-number">1.1.6.4</span> Statistical tests in Bayesian statistics<a href="probability-theory-linear-algebra.html#statistical-tests-in-bayesian-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p></font></p>
</div>
</div>
</div>
<div id="linear-algebra" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Linear Algebra<a href="probability-theory-linear-algebra.html#linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section is meant to give an intuitive understanding of the
underlying mechanisms of many algorithms. It is mainly a summary of the
course from
<a href="https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3Blue1Brown</a>
and
<a href="https://deepai.org/machine-learning-glossary-and-terms/vector">deepai.org</a>.</p>
<p>For details on the calculations see wikipedia.org.</p>
<div id="vectors" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Vectors<a href="probability-theory-linear-algebra.html#vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are two relevant perspectives for us:</p>
<ul>
<li><p><strong>Mathematical:</strong> Generally quantities that cannot be expressed by
single number. They are objects in a <em>vector space</em>. Such objects
can also be e.g. functions.</p></li>
<li><p><strong>Programmatical / Data:</strong> Vectors are ordered lists of numbers. You
model each sample as such an ordered list of numbers and the numbers
represent the feature-value of that feature.</p></li>
</ul>
<p>Your vectors are organized in a <em>coordinate system</em> and commonly rooted
in the <em>origin</em> (point <span class="math inline">\([0,0]\)</span>.<br />
</p>
<div id="linear-combinations" class="section level4 hasAnchor" number="1.2.1.1">
<h4><span class="header-section-number">1.2.1.1</span> Linear combinations<a href="probability-theory-linear-algebra.html#linear-combinations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>You create <em>linear combinations</em> of vectors by adding their components
(entries in a coordinate). Th All points that you can reach by linear
combinations are called the <em>span</em> of these vectors. If a vector lies in
the span of another vector, they are <em>linearly dependent</em>.<br />
You can <em>scale</em> (stretch or squish) vectors multiply vectors by
<em>scalars</em> (i.e. numbers). A vector with length <span class="math inline">\(1\)</span> is called <em>unit
vector</em>. The unit vectors in each direction of the coordinate system are
its <em>basis vectors</em>. The basis vectors stacked together form an
<em>identity matrix</em>: a matrix with 1s on its diagonal. Since there are
only values on its diagonal it is also a <em>diagonal matrix</em>.<br />
</p>
<p><span class="math display">\[
I = \begin{bmatrix}
1 \quad 0 \quad 0 \\
0 \quad 1 \quad 0 \\
0 \quad 0 \quad 1
\end{bmatrix}
\]</span></p>
</div>
<div id="linear-transformations" class="section level4 hasAnchor" number="1.2.1.2">
<h4><span class="header-section-number">1.2.1.2</span> Linear transformations<a href="probability-theory-linear-algebra.html#linear-transformations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Linear transformations</em> are functions that move points around in a
vector space, while preserving the linear relationships between the
points (straight lines stay straight, the origin stays the origin). They
include rotations and reflections. You can understand the calculation of
the linear transformation of a point as follows: You give the basis
vectors a new location. You scale the new location basis vectors with
the components of the respective dimension of the vector you want to
transform. You take the linear combination of the scaled, transformed
basis vectors:</p>
<p><span class="math display">\[\begin{bmatrix} a \quad b \\ c \quad d \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
  = x \begin{bmatrix} a \\ c \end{bmatrix} + y \begin{bmatrix} b \\ d \end{bmatrix} = \begin{bmatrix} x a  + y b \\ x c + y d \end{bmatrix}
\]</span></p>
<!-- Construct a graph visualization to show how the transformation impacts the original vector -->
<p>likewise, you can view matrix vector multiplication as a transformation
of your space. Full explanation: <a href="https://youtu.be/kYB8IZa5AuE">youtube.com -
3Blue1Brown</a> Multiplying two matrices
represents the sequential combination of two linear transformations in
your vector space.</p>
<p>A <em>transpose</em> <span class="math inline">\(A^T\)</span> of a matrix <span class="math inline">\(A\)</span> is achieved by mirroring the matrix
on its diagonal and therefore swapping its rows and columns. This
commonly makes sense when evaluating if elements of two matrices line up
in regard to their scale. You can also check if matrices are
<a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal</a>.</p>
<p>An <em>orthogonal/orthonormal matrix</em> is a matrix for which holds
<span class="math inline">\(A^TA=AA^T=I\)</span>, where <span class="math inline">\(I\)</span> is the identity matrix. The columns of
orthogonal matrices are linearly independent of each other.</p>
<p>An <em>inverse matrix</em> <span class="math inline">\(A^{-1}\)</span> of a matrix <span class="math inline">\(A\)</span> is the matrix that would
yield no transformation at all, if multiplied with <span class="math inline">\(A\)</span>.</p>
</div>
<div id="determinants" class="section level4 hasAnchor" number="1.2.1.3">
<h4><span class="header-section-number">1.2.1.3</span> Determinants<a href="probability-theory-linear-algebra.html#determinants" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Determinants</em> can be used to measure how much a linear combination
compresses or stretches the space. If a transformation inverts the
space, the determinant will be negative. If a determinant is 0 it means
that the transformation maps the space onto a lower dimension.</p>
<p>The dimensions that come out of a transformation/matrix are its <em>rank</em>.
All possible outputs of your matrix (the span constructed by its
columns) is the <em>column space</em>. All vectors that are mapped to 0 (onto
the origin) are the <em>null space</em> or <em>kernel</em> of the matrix.</p>
<p>Determinants can only be calculated for square matrices. An e.g.
<span class="math inline">\(3 \times 2\)</span> matrix can be viewed as a transformation mapping from 2-D
to 3-D space.<br />
</p>
<p>The <em>dot product</em> of two vectors is calculated like a linear
transformation between a <span class="math inline">\(1 \times 2\)</span> matrix and a <span class="math inline">\(2 \times 1\)</span> matrix.
It therefor maps onto the 1-D Space and can be used as a measure of
collinearity.</p>
<p>The <em>cross product</em> of two vectors is a perpendicular vector that
describes the parallelogram that the two vectors span. Its magnitude can
be seen as the area of the parallelogram. Beware: The order of the
vectors in the operation matters. The cross product can be expressed by
a determinant. If two vectors are collinear or perpendicular, the cross
product is zero.</p>
</div>
<div id="system-of-equations" class="section level4 hasAnchor" number="1.2.1.4">
<h4><span class="header-section-number">1.2.1.4</span> System of equations<a href="probability-theory-linear-algebra.html#system-of-equations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Linear algebra can help you solve systems of equations.</p>
<p><span class="math display">\[
\begin{array}{ll} 1x+2y+3z=4 \\ 4x+5y+6z=-7 \\8x + 9y +0z = 1 \end{array}
\quad  \rightarrow  \quad
\begin{bmatrix} 1 \quad 2 \quad 3 \\ 4 \quad 5 \quad 6 \\ 8 \quad \ 9 \quad 0 \end{bmatrix}
\begin{bmatrix} x \\ y \\ z\end{bmatrix} =
\begin{bmatrix} 4 \\ -7 \\ 1 \end{bmatrix}
\quad \rightarrow \quad
A \vec{x} = \vec{v}
\]</span></p>
<p>You can imagine this as as searching a vector <span class="math inline">\(\vec{x}\)</span> that will land
on <span class="math inline">\(\vec{v}\)</span> after the transformation <span class="math inline">\(A\)</span>.</p>
<p>To find <span class="math inline">\(\vec{x}\)</span> you need the <em>inverse</em> of <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
A^{-1}A = \begin{bmatrix} 1 \quad 0 \\ 0 \quad 1 \end{bmatrix}
\]</span></p>
<p>You now multiply the matrix equation with <span class="math inline">\(A^{-1}\)</span> and get:</p>
<p><span class="math display">\[
A^{-1} A \vec{x} = A^{-1} \vec{v}
\quad \rightarrow \quad
\vec{x} = A^{-1} \vec{v}
\]</span></p>
</div>
<div id="eigenvalues-and-eigenvectors" class="section level4 hasAnchor" number="1.2.1.5">
<h4><span class="header-section-number">1.2.1.5</span> Eigenvalues and Eigenvectors<a href="probability-theory-linear-algebra.html#eigenvalues-and-eigenvectors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a linear transformation <span class="math inline">\(A\)</span>, the eigenvectors <span class="math inline">\(\vec{v}\)</span> represent
the vectors that stay on their span (keep orientation) and the
eigenvalues <span class="math inline">\(\lambda\)</span> are the scalars by which the eigenvectors get
scaled.</p>
<p><span class="math display">\[
A \vec{v} = \lambda \vec{v}
\]</span></p>
<p>Transforming <span class="math inline">\(\lambda\)</span> to a scaled identity matrix <span class="math inline">\(I\)</span> and factoring out
<span class="math inline">\(\vec{v}\)</span>, we get: <span class="math display">\[
(A - \lambda I) \vec{v} =  \vec{0}
\]</span> This tells us, that the transformation <span class="math inline">\((A - \lambda I)\)</span> needs to map
the vector <span class="math inline">\(\vec{v}\)</span> onto a lower dimension.</p>
<p>An <em>eigenbasis</em> <span class="math inline">\(\lambda I\)</span> is a basis where the basis vectors are
eigenvectors. They will sit on the diagonal of your basis matrix
(<span class="math inline">\(\rightarrow\)</span> it will be a <em>diagonal matrix</em>).</p>
</div>
<div id="EV_Dec" class="section level4 hasAnchor" number="1.2.1.6">
<h4><span class="header-section-number">1.2.1.6</span> Eigenvalue decomposition<a href="probability-theory-linear-algebra.html#EV_Dec" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An <em>eigen(value)decomposition</em> is the decomposition of a matrix into the
matrix of eigenvalues and eigenvectors.</p>
<p><span class="math display">\[
AU = U \Lambda  \quad \rightarrow \quad A = U \Lambda U^{-1}
\]</span></p>
<p>where <span class="math inline">\(U\)</span> is the matrix of the eigenvectors of <span class="math inline">\(A\)</span> and <span class="math inline">\(\Lambda\)</span> is the
eigenbasis. Thus matrix operations can be computed more easily, since
<span class="math inline">\(\Lambda\)</span> is a diagonal matrix.</p>
</div>
<div id="SVD1" class="section level4 hasAnchor" number="1.2.1.7">
<h4><span class="header-section-number">1.2.1.7</span> Singular value decomposition<a href="probability-theory-linear-algebra.html#SVD1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Singular Value decomposition is also applicable to a non-square
<span class="math inline">\(m \times n\)</span>-matrix (with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns). If you have a
matrix with rank <span class="math inline">\(r\)</span>, you can decompose it into</p>
<p><span class="math display">\[
A = U \Sigma V^T
\]</span> where <span class="math inline">\(U\)</span> is an orthogonal <span class="math inline">\(m \times r\)</span> matrix, <span class="math inline">\(\Sigma\)</span> is a
diagonal <span class="math inline">\(r \times r\)</span> matrix and <span class="math inline">\(V^T\)</span> is an orthogonal <span class="math inline">\(r \times n\)</span>
matrix. <span class="math inline">\(U\)</span> contains the <em>left singular vectors</em>, <span class="math inline">\(V\)</span> the <em>right
singular vectors</em> and <span class="math inline">\(\Sigma\)</span> the <span class="math inline">\(Singular Values\)</span>.<br />
This decomposition technique can be used to approximate the original
matrix <span class="math inline">\(A\)</span> with only the <span class="math inline">\(k\)</span> largest singular values. This lets you work
in a space with only <span class="math inline">\(k\)</span> dimensions given by <span class="math inline">\(U_k \Sigma_k\)</span>. Thereby you
can save computation time and memory space without loosing a lot of
information.</p>
<div class="figure">
<img src="figures/SVD_lower_dims.png" style="width:50.0%" alt="" />
<p class="caption">SVD and truncated SVD. The upper schema shows the decomposition of the
matrix <span class="math inline">\(M\)</span> with m rows and n columns into <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(V\)</span>. The
lower schema shows the truncated SVD for lower dimensional mapping
<span class="math inline">\(U_t \Sigma_t\)</span> : The first <span class="math inline">\(t\)</span> eigenvalues from <span class="math inline">\(\Sigma\)</span> have been
chosen to reconstruct a compressed version <span class="math inline">\(\bar{M}\)</span>. This figure has
been adapted from <a href="https://commons.wikimedia.org/wiki/File:Reduced_Singular_Value_Decompositions.svg">user Cmglee on
wikimedia.org.</a></p>
</div>
<p>For more detailed explanation, see this <a href="https://stats.stackexchange.com/questions/342046/explaining-dimensionality-reduction-using-svd-without-reference-to-pca">Stack Exchange
Thread</a>.<br />
For applications, please see <a href="unsupervised-learning.html#SVD2">SVD for lower dimensional mapping</a>.</p>

<hr />
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-representation-analysis-processing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/MoritzGuck/Machine-Learning-Reference/edit/master/01-Probability-Theory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
