<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Data: Representation, Analysis &amp; Processing | Machine Learning Reference</title>
  <meta name="description" content="Chapter 2 Data: Representation, Analysis &amp; Processing | Machine Learning Reference" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Data: Representation, Analysis &amp; Processing | Machine Learning Reference" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="MoritzGuck/Machine-Learning-Reference" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Data: Representation, Analysis &amp; Processing | Machine Learning Reference" />
  
  
  

<meta name="author" content="Moritz GÃ¼ck" />


<meta name="date" content="2023-04-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability-theory-linear-algebra.html"/>
<link rel="next" href="classification-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>Machine Learning Reference</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html"><i class="fa fa-check"></i><b>1</b> Probability Theory &amp; Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-theory"><i class="fa fa-check"></i><b>1.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-basics"><i class="fa fa-check"></i><b>1.1.1</b> Probability Basics</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-distributions"><i class="fa fa-check"></i><b>1.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#CLT"><i class="fa fa-check"></i><b>1.1.3</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#bayesian-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian probability</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#further-concepts"><i class="fa fa-check"></i><b>1.1.5</b> Further Concepts</a></li>
<li class="chapter" data-level="1.1.6" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#statistical-tests"><i class="fa fa-check"></i><b>1.1.6</b> Statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#vectors"><i class="fa fa-check"></i><b>1.2.1</b> Vectors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html"><i class="fa fa-check"></i><b>2</b> Data: Representation, Analysis &amp; Processing</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-and-distance-measures"><i class="fa fa-check"></i><b>2.1</b> Similarity and Distance Measures</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#metrics"><i class="fa fa-check"></i><b>2.1.1</b> Metrics</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-measures-on-vectors"><i class="fa fa-check"></i><b>2.1.2</b> Similarity measures on vectors</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#kernels"><i class="fa fa-check"></i><b>2.1.3</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#data-analysis"><i class="fa fa-check"></i><b>2.2</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>2.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#output-analysis"><i class="fa fa-check"></i><b>2.2.2</b> Output Analysis</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#monitoring"><i class="fa fa-check"></i><b>2.2.3</b> Monitoring</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#preprocessing-data"><i class="fa fa-check"></i><b>2.3</b> Preprocessing data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#handling-missing-wrong-data"><i class="fa fa-check"></i><b>2.3.1</b> Handling missing &amp; wrong data</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#working-with-date-and-time"><i class="fa fa-check"></i><b>2.3.2</b> Working with Date and Time</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#encoding-discretizing-data"><i class="fa fa-check"></i><b>2.3.3</b> Encoding &amp; discretizing data</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#standardization"><i class="fa fa-check"></i><b>2.3.4</b> Standardization</a></li>
<li class="chapter" data-level="2.3.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#splitting-in-training--and-test-data"><i class="fa fa-check"></i><b>2.3.5</b> Splitting in training- and test-data</a></li>
<li class="chapter" data-level="2.3.6" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#feature-selection"><i class="fa fa-check"></i><b>2.3.6</b> Feature selection</a></li>
<li class="chapter" data-level="2.3.7" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>2.3.7</b> Hyper-parameter tuning</a></li>
<li class="chapter" data-level="2.3.8" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#model-selection"><i class="fa fa-check"></i><b>2.3.8</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#errors-regularization"><i class="fa fa-check"></i><b>2.4</b> Errors &amp; regularization</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bias-and-variance"><i class="fa fa-check"></i><b>2.4.1</b> Bias and Variance</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#regularization"><i class="fa fa-check"></i><b>2.4.2</b> Regularization</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bagging"><i class="fa fa-check"></i><b>2.4.3</b> Bagging</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#boosting"><i class="fa fa-check"></i><b>2.4.4</b> Boosting</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#stacking"><i class="fa fa-check"></i><b>2.4.5</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#tips-for-machine-learning-projects"><i class="fa fa-check"></i><b>2.5</b> Tips for machine learning projects</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#general-advice"><i class="fa fa-check"></i><b>2.5.1</b> General advice</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#common-mistakes"><i class="fa fa-check"></i><b>2.5.2</b> Common mistakes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-methods.html"><a href="classification-methods.html"><i class="fa fa-check"></i><b>3</b> Classification Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-methods.html"><a href="classification-methods.html#evaluation-of-classifiers"><i class="fa fa-check"></i><b>3.1</b> Evaluation of Classifiers</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-methods.html"><a href="classification-methods.html#confusion-matrix"><i class="fa fa-check"></i><b>3.1.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-methods.html"><a href="classification-methods.html#basic-quality-measures"><i class="fa fa-check"></i><b>3.1.2</b> Basic Quality Measures</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-methods.html"><a href="classification-methods.html#area-under-the-curve"><i class="fa fa-check"></i><b>3.1.3</b> Area under the Curve</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-methods.html"><a href="classification-methods.html#handling-unbalanced-data"><i class="fa fa-check"></i><b>3.1.4</b> Handling Unbalanced Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-methods.html"><a href="classification-methods.html#classification-algorithms"><i class="fa fa-check"></i><b>3.2</b> Classification Algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-methods.html"><a href="classification-methods.html#nearest-neighbors-classifier"><i class="fa fa-check"></i><b>3.2.1</b> Nearest Neighbors Classifier</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-methods.html"><a href="classification-methods.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>3.2.2</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-methods.html"><a href="classification-methods.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>3.2.3</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-methods.html"><a href="classification-methods.html#support-vector-classifier-svc"><i class="fa fa-check"></i><b>3.2.4</b> Support Vector Classifier (SVC)</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-methods.html"><a href="classification-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.2.5</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#custering-methods"><i class="fa fa-check"></i><b>4.1</b> Custering Methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#metrics-for-clustering-algorithms"><i class="fa fa-check"></i><b>4.1.1</b> metrics for Clustering algorithms</a></li>
<li class="chapter" data-level="4.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#graph-based-clustering"><i class="fa fa-check"></i><b>4.1.3</b> Graph-Based Clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#spectral-clustering"><i class="fa fa-check"></i><b>4.1.4</b> Spectral Clustering</a></li>
<li class="chapter" data-level="4.1.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#SSP"><i class="fa fa-check"></i><b>4.1.5</b> Sparse Subspace Clustering (SSP)</a></li>
<li class="chapter" data-level="4.1.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#soft-assignment-clustering"><i class="fa fa-check"></i><b>4.1.6</b> Soft-assignment Clustering</a></li>
<li class="chapter" data-level="4.1.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#artificial-neural-networks-for-clustering"><i class="fa fa-check"></i><b>4.1.7</b> Artificial Neural Networks for Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#mapping-to-lower-dimensions"><i class="fa fa-check"></i><b>4.2</b> Mapping to lower dimensions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#manifold-learning"><i class="fa fa-check"></i><b>4.2.1</b> Manifold learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#decomposition-techniques"><i class="fa fa-check"></i><b>4.2.2</b> Decomposition techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#outlier-detection"><i class="fa fa-check"></i><b>4.3</b> Outlier detection</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#local-outlier-factor"><i class="fa fa-check"></i><b>4.3.1</b> Local outlier factor</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#isolation-forest"><i class="fa fa-check"></i><b>4.3.2</b> Isolation forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generative-models.html"><a href="generative-models.html"><i class="fa fa-check"></i><b>5</b> Generative models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="generative-models.html"><a href="generative-models.html#generative-models-for-discrete-data"><i class="fa fa-check"></i><b>5.1</b> Generative Models for Discrete Data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="generative-models.html"><a href="generative-models.html#bayesian-concept-learning"><i class="fa fa-check"></i><b>5.1.1</b> Bayesian Concept Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="generative-models.html"><a href="generative-models.html#beta-binomial-model"><i class="fa fa-check"></i><b>5.1.2</b> Beta-binomial model</a></li>
<li class="chapter" data-level="5.1.3" data-path="generative-models.html"><a href="generative-models.html#dirichlet-multinomial-model"><i class="fa fa-check"></i><b>5.1.3</b> Dirichlet-multinomial model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#evaluation-of-regression-models"><i class="fa fa-check"></i><b>6.1</b> Evaluation of regression models</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#mean-squared-error"><i class="fa fa-check"></i><b>6.1.1</b> Mean squared error</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#r2-score-coefficient-of-determination"><i class="fa fa-check"></i><b>6.1.2</b> R^2 score / coefficient of determination</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#visual-tools"><i class="fa fa-check"></i><b>6.1.3</b> Visual tools</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#linear-models"><i class="fa fa-check"></i><b>6.2</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>6.2.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#lasso-regression"><i class="fa fa-check"></i><b>6.2.2</b> Lasso regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="regression.html"><a href="regression.html#ridge-regression"><i class="fa fa-check"></i><b>6.2.3</b> Ridge regression</a></li>
<li class="chapter" data-level="6.2.4" data-path="regression.html"><a href="regression.html#bayesian-regression"><i class="fa fa-check"></i><b>6.2.4</b> Bayesian regression</a></li>
<li class="chapter" data-level="6.2.5" data-path="regression.html"><a href="regression.html#anova"><i class="fa fa-check"></i><b>6.2.5</b> ANOVA</a></li>
<li class="chapter" data-level="6.2.6" data-path="regression.html"><a href="regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>6.2.6</b> Generalized linear models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.3</b> Gaussian process regression</a></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#gradient-boosted-tree-regression"><i class="fa fa-check"></i><b>6.4</b> Gradient boosted tree regression</a></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#time-series-forecasting"><i class="fa fa-check"></i><b>6.5</b> Time Series Forecasting</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#arimax-model"><i class="fa fa-check"></i><b>6.5.1</b> ARIMA(X) Model</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#varmmax-model"><i class="fa fa-check"></i><b>6.5.2</b> VARMMA(X) Model</a></li>
<li class="chapter" data-level="6.5.3" data-path="regression.html"><a href="regression.html#prophet-model"><i class="fa fa-check"></i><b>6.5.3</b> Prophet-Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Networks {#Neural Networks}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#non-linearities"><i class="fa fa-check"></i><b>7.1.1</b> Non-Linearities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#feedforward-neural-network-multi-layer-perceptron"><i class="fa fa-check"></i><b>7.2</b> Feedforward Neural Network / Multi-Layer Perceptron</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>7.2.1</b> Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>7.3</b> Convolutional Neural Networks</a></li>
<li class="chapter" data-level="7.4" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>7.4</b> Autoencoders</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders-for-clustering"><i class="fa fa-check"></i><b>7.4.1</b> Autoencoders for clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>7.5</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="7.6" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>7.6</b> Recurrent neural networks</a></li>
<li class="chapter" data-level="7.7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#long-short-term-memory-networks"><i class="fa fa-check"></i><b>7.7</b> Long short-term memory networks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html"><i class="fa fa-check"></i><b>8</b> Machine learning Automation &amp; Productivity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#holistic-approach"><i class="fa fa-check"></i><b>8.1</b> Holistic approach</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autosklearn"><i class="fa fa-check"></i><b>8.1.1</b> Autosklearn</a></li>
<li class="chapter" data-level="8.1.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#flaml"><i class="fa fa-check"></i><b>8.1.2</b> FLAML</a></li>
<li class="chapter" data-level="8.1.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#h2o.ai"><i class="fa fa-check"></i><b>8.1.3</b> H2O.ai</a></li>
<li class="chapter" data-level="8.1.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autokeras"><i class="fa fa-check"></i><b>8.1.4</b> AutoKeras</a></li>
<li class="chapter" data-level="8.1.5" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#fastai"><i class="fa fa-check"></i><b>8.1.5</b> Fastai</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#model-selection-automation"><i class="fa fa-check"></i><b>8.2</b> Model selection automation</a></li>
<li class="chapter" data-level="8.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#feature-engineering-automation"><i class="fa fa-check"></i><b>8.3</b> Feature engineering automation</a></li>
<li class="chapter" data-level="8.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#cookie-cutter-visualizations"><i class="fa fa-check"></i><b>8.4</b> cookie-cutter Visualizations</a></li>
<li class="chapter" data-level="8.5" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#comprehensive-validation-reports"><i class="fa fa-check"></i><b>8.5</b> Comprehensive validation reports</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html"><i class="fa fa-check"></i><b>9</b> Machine Learning Project Management</a>
<ul>
<li class="chapter" data-level="9.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#basis-for-machine-learning-in-companies"><i class="fa fa-check"></i><b>9.1</b> Basis for Machine Learning in Companies</a></li>
<li class="chapter" data-level="9.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-automate-business-processes-with-machine-learning"><i class="fa fa-check"></i><b>9.2</b> How to automate business processes with machine learning</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#the-stages-from-manual-to-ml"><i class="fa fa-check"></i><b>9.2.1</b> The stages from ManuaL to ML</a></li>
<li class="chapter" data-level="9.2.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#phases-of-the-ml-project"><i class="fa fa-check"></i><b>9.2.2</b> Phases of the ML project</a></li>
<li class="chapter" data-level="9.2.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-frame-ml-problems"><i class="fa fa-check"></i><b>9.2.3</b> How to frame ML problems</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#common-pitfalls-in-machine-learning"><i class="fa fa-check"></i><b>9.3</b> Common pitfalls in machine learning</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Reference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-representation-analysis-processing" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Data: Representation, Analysis &amp; Processing<a href="data-representation-analysis-processing.html#data-representation-analysis-processing" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="similarity-and-distance-measures" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Similarity and Distance Measures<a href="data-representation-analysis-processing.html#similarity-and-distance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Choosing the right distance measures is important for achieving good
results in statistics, predictions and clusterings.</p>
<div id="metrics" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Metrics<a href="data-representation-analysis-processing.html#metrics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a distance measure to be called a metric <span class="math inline">\(d\)</span>, the following criteria
need to be fulfilled:</p>
<ul>
<li><p>Positivity: <span class="math inline">\(d(x_1,x_2)â¥0\)</span></p></li>
<li><p><span class="math inline">\(d(x_1,x_2)=0 \text{ if and only if } x_1 = x_2\)</span></p></li>
<li><p>Symmetry: <span class="math inline">\(d(x_1, x_2) = d(x_2, x_1)\)</span></p></li>
<li><p>Triangle inequality: <span class="math inline">\(d(x_1, x_3) â¤ d(x_1, x_2) + d(x_2, x_3)\)</span></p></li>
</ul>
<p>There may be distance measures that do not fulfill these criteria, but
those are not metrics.</p>
</div>
<div id="similarity-measures-on-vectors" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Similarity measures on vectors<a href="data-representation-analysis-processing.html#similarity-measures-on-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>These measures are used in many objective functions to compare data
points.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="data-representation-analysis-processing.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> pairwise_distances</span>
<span id="cb1-2"><a href="data-representation-analysis-processing.html#cb1-2" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> np.array([[<span class="dv">2</span>,<span class="dv">3</span>]])</span>
<span id="cb1-3"><a href="data-representation-analysis-processing.html#cb1-3" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> np.array([[<span class="dv">2</span>,<span class="dv">4</span>]])</span>
<span id="cb1-4"><a href="data-representation-analysis-processing.html#cb1-4" aria-hidden="true" tabindex="-1"></a>pairwise_distances(X1,X2, metric<span class="op">=</span><span class="st">&quot;manhattan&quot;</span>)</span></code></pre></div>
<p>The available metrics in sklearn are: âcityblockâ, âcosineâ,
âeuclideanâ, âl1â, âl2â, âmanhattanâ, and from scipy: âbraycurtisâ,
âcanberraâ, âchebyshevâ, âcorrelationâ, âdiceâ, âhammingâ, âjaccardâ,
âkulsinskiâ, âmahalanobisâ, âminkowskiâ, ârogerstanimotoâ, ârussellraoâ,
âseuclideanâ, âsokalmichenerâ, âsokalsneathâ, âsqeuclideanâ, âyuleâ<br />
More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html">scikit-learn.org</a></p>
<div id="manhattan-distance" class="section level4 hasAnchor" number="2.1.2.1">
<h4><span class="header-section-number">2.1.2.1</span> Manhattan distance<a href="data-representation-analysis-processing.html#manhattan-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The distance is the sum of the absolute differences of the components
(single coordinates) of the two points:
<span class="math display">\[d(A, B) = \sum_{i=1}^d | A_i - B_i |\]</span></p>
<p>More info at
<a href="https://en.wikipedia.org/wiki/Taxicab_geometry">wikipedia.org</a>.</p>
</div>
<div id="hamming-distance" class="section level4 hasAnchor" number="2.1.2.2">
<h4><span class="header-section-number">2.1.2.2</span> Hamming distance<a href="data-representation-analysis-processing.html#hamming-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This metric is used for pairs of strings and works equivalently to the
Manhattan distance. It is the number of positions that are different
between the strings.<br />
More info at
<a href="https://en.wikipedia.org/wiki/Hamming_distance">wikipedia.org</a>.</p>
</div>
<div id="euclidian-distance" class="section level4 hasAnchor" number="2.1.2.3">
<h4><span class="header-section-number">2.1.2.3</span> Euclidian distance<a href="data-representation-analysis-processing.html#euclidian-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[d(A, B) = | A - B | = \sqrt{\sum_{i=1}^d (A_i-B_i)^2} \]</span></p>
<p>More info on the euclidian distance on
<a href="https://en.wikipedia.org/wiki/Euclidean_distance">wikipedia.org</a>.<br />
The usefulness of this metric can deteriorate in high dimensional
spaces. See <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_function">curse of
dimensionality</a></p>
</div>
<div id="chebyshev-distance" class="section level4 hasAnchor" number="2.1.2.4">
<h4><span class="header-section-number">2.1.2.4</span> Chebyshev distance<a href="data-representation-analysis-processing.html#chebyshev-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Chebyshev distance is the largest difference along any of the
components of the two vectors.</p>
<p><span class="math display">\[d(A, B) = \max_i(|A_i-B_i|) \]</span></p>
<p>More info at
<a href="https://en.wikipedia.org/wiki/Chebyshev_distance">wikipedia.org</a>.</p>
</div>
<div id="minkowski-distance" class="section level4 hasAnchor" number="2.1.2.5">
<h4><span class="header-section-number">2.1.2.5</span> Minkowski Distance<a href="data-representation-analysis-processing.html#minkowski-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[d(A, B) = (\sum_{i=1}^d |A_i-B_i|^p)^\frac{1}{p} \]</span></p>
<p>For <span class="math inline">\(p=2\)</span> the Minkowski distance is equal to the Euclidian distance, for
<span class="math inline">\(p=1\)</span> it corresponds to the Manhattan distance and it converges to the
Chebyshev distance for <span class="math inline">\(p \to \infty\)</span>. Â  More info at
<a href="https://en.wikipedia.org/wiki/Minkowski_distance">wikipedia.org</a>.</p>
<!--# ### Similarity measures on sets of objects

#### Jaccard coefficient

#### Jaccard distance

#### Overlap coefficient

#### Sorensen-Dice coefficient

### Similarity measures on sets of vectors

#### Single link distance function

#### Complete link distance function

#### Average link distance function

### Similarity measures on sets of strings

#### k-mer based similarity measures

### Similartiy measures for nodes in a Graph

#### Shortest path distance

### Similarity measures on Graphs

#### Wiener index

#### Weisfeiler Lehmann Kernel

### Similarty measures for time series

#### Dynamic Time Warping (DTW) distance -->
</div>
</div>
<div id="kernels" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Kernels<a href="data-representation-analysis-processing.html#kernels" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Kernels are functions that output the relationship between points in
your data. They correspond to mapping the data into high-dimensional
space and allow to implicitly draw nonlinear decision boundaries with
linear models.</p>
<div id="closure-properties-of-kernels" class="section level4 hasAnchor" number="2.1.3.1">
<h4><span class="header-section-number">2.1.3.1</span> Closure properties of kernels<a href="data-representation-analysis-processing.html#closure-properties-of-kernels" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>If <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> are kernels, then <span class="math inline">\(k_1 + k_2\)</span> is a kernel as
well.</p></li>
<li><p>If <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> are kernels, then their product is a kernel as
well.</p></li>
<li><p>If <span class="math inline">\(k\)</span> is a kernel and <span class="math inline">\(\alpha\)</span> is a kernel, then <span class="math inline">\(\alpha k\)</span> is a
kernel as well.</p></li>
<li><p>If you define <span class="math inline">\(k\)</span> only on a set <span class="math inline">\(D\)</span>, then points that are not in <span class="math inline">\(D\)</span>
will have a value of <span class="math inline">\(k_0=0\)</span> which is still a valid kernel.</p></li>
</ul>
<p><font color="grey"></p>
</div>
<div id="linear-kernel" class="section level4 hasAnchor" number="2.1.3.2">
<h4><span class="header-section-number">2.1.3.2</span> Linear kernel<a href="data-representation-analysis-processing.html#linear-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="polynomial-kernel" class="section level4 hasAnchor" number="2.1.3.3">
<h4><span class="header-section-number">2.1.3.3</span> polynomial kernel<a href="data-representation-analysis-processing.html#polynomial-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="gaussian-radial-basis-function-rbf-kernel" class="section level4 hasAnchor" number="2.1.3.4">
<h4><span class="header-section-number">2.1.3.4</span> Gaussian Radial Basis Function (RBF) kernel<a href="data-representation-analysis-processing.html#gaussian-radial-basis-function-rbf-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="constant-kernel" class="section level4 hasAnchor" number="2.1.3.5">
<h4><span class="header-section-number">2.1.3.5</span> constant kernel<a href="data-representation-analysis-processing.html#constant-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="delta-dirac-kernel" class="section level4 hasAnchor" number="2.1.3.6">
<h4><span class="header-section-number">2.1.3.6</span> delta dirac kernel<a href="data-representation-analysis-processing.html#delta-dirac-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="r-convolution-kernels" class="section level4 hasAnchor" number="2.1.3.7">
<h4><span class="header-section-number">2.1.3.7</span> R convolution kernels<a href="data-representation-analysis-processing.html#r-convolution-kernels" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="string-kernels" class="section level4 hasAnchor" number="2.1.3.8">
<h4><span class="header-section-number">2.1.3.8</span> String kernels<a href="data-representation-analysis-processing.html#string-kernels" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p></font></p>
</div>
</div>
</div>
<div id="data-analysis" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Data Analysis<a href="data-representation-analysis-processing.html#data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Data analysis is conducted iteratively once you get hold of your data,
when you cleaned it, when you processed it and when you analyse the
outputs of your model.</p>
<div id="exploratory-data-analysis" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Exploratory data analysis<a href="data-representation-analysis-processing.html#exploratory-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="initial-analysis" class="section level4 hasAnchor" number="2.2.1.1">
<h4><span class="header-section-number">2.2.1.1</span> Initial analysis<a href="data-representation-analysis-processing.html#initial-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After getting hold of the data, these are important properties to
extract:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="data-representation-analysis-processing.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="data-representation-analysis-processing.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;First 5 samples:&quot;</span></span>
<span id="cb2-3"><a href="data-representation-analysis-processing.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span>
<span id="cb2-4"><a href="data-representation-analysis-processing.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;:.. and last 5 samples:&quot;</span>)</span>
<span id="cb2-5"><a href="data-representation-analysis-processing.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.tail())</span>
<span id="cb2-6"><a href="data-representation-analysis-processing.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;First sample per month:&quot;</span>)</span>
<span id="cb2-7"><a href="data-representation-analysis-processing.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_transport.groupby(<span class="st">&#39;Month&#39;</span>).first())</span>
<span id="cb2-8"><a href="data-representation-analysis-processing.html#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The number of non-null values and the respective data type per column:</span></span>
<span id="cb2-9"><a href="data-representation-analysis-processing.html#cb2-9" aria-hidden="true" tabindex="-1"></a>df.info() </span>
<span id="cb2-10"><a href="data-representation-analysis-processing.html#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># The count, uniques, mean, standard deviation, min, max, quartiles per column:</span></span>
<span id="cb2-11"><a href="data-representation-analysis-processing.html#cb2-11" aria-hidden="true" tabindex="-1"></a>df.describe(include<span class="op">=</span><span class="st">&#39;all&#39;</span>) </span>
<span id="cb2-12"><a href="data-representation-analysis-processing.html#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;rows: &quot;</span><span class="op">+</span> df.shape[<span class="dv">0</span>])</span>
<span id="cb2-13"><a href="data-representation-analysis-processing.html#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;columns: &quot;</span><span class="op">+</span> df.shape[<span class="dv">1</span>])</span>
<span id="cb2-14"><a href="data-representation-analysis-processing.html#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;empty rows: &quot;</span><span class="op">+</span> df_transport.isnull().<span class="bu">sum</span>())</span></code></pre></div>
<p>Check:</p>
<ul class="task-list">
<li><p><input type="checkbox" disabled="" />Was the dataset correctly imported?</p><ul>
<li><p>No column index as first row values.</p></li>
<li><p>No trailing comment as last row values.</p></li>
</ul></li>
<li><p><input type="checkbox" disabled="" />Are the sample values what you expect?</p></li>
<li><p><input type="checkbox" disabled="" />Are columns in correct and efficient data type?</p><ul>
<li><p>Has there been a shift of data between columns / rows?</p></li>
<li><p>Are there strings in a column for numerical values?</p></li>
</ul></li>
<li><p><input type="checkbox" disabled="" />Is the range what you expect?</p><ul>
<li><p>Are there heavy outliers?</p></li>
<li><p>Is the data biased towards certain values?</p></li>
</ul></li>
<li><p><input type="checkbox" disabled="" />How many empty values are there?</p></li>
</ul>
</div>
<div id="after-preprocessing" class="section level4 hasAnchor" number="2.2.1.2">
<h4><span class="header-section-number">2.2.1.2</span> After preprocessing<a href="data-representation-analysis-processing.html#after-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="univariate-analysis" class="section level5 hasAnchor" number="2.2.1.2.1">
<h5><span class="header-section-number">2.2.1.2.1</span> Univariate Analysis<a href="data-representation-analysis-processing.html#univariate-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Analyse only one attribute.</p>
<div id="categorical-discrete-data-bar-chart" class="section level6 hasAnchor" number="2.2.1.2.1.1">
<h6><span class="header-section-number">2.2.1.2.1.1</span> Categorical / discrete data: Bar chart<a href="data-representation-analysis-processing.html#categorical-discrete-data-bar-chart" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Plot the number of occurrences of each category / number. This helps you
find the distribution of your data.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="data-representation-analysis-processing.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-2"><a href="data-representation-analysis-processing.html#cb3-2" aria-hidden="true" tabindex="-1"></a>sns.countplot(df[<span class="st">&quot;sex&quot;</span>])</span></code></pre></div>
<p><img src="figures/countplot.png" /></p>
</div>
<div id="continuous-data-histogram" class="section level6 hasAnchor" number="2.2.1.2.1.2">
<h6><span class="header-section-number">2.2.1.2.1.2</span> Continuous data: Histogram<a href="data-representation-analysis-processing.html#continuous-data-histogram" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Group data into ranges and plot number of occurrences in each range.
This helps you find the distribution of your data.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="data-representation-analysis-processing.html#cb4-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">&#39;whitegrid&#39;</span>)</span>
<span id="cb4-2"><a href="data-representation-analysis-processing.html#cb4-2" aria-hidden="true" tabindex="-1"></a>sns.histplot(data<span class="op">=</span>df_USAhousing, x<span class="op">=</span><span class="st">&#39;median_house_value&#39;</span>, bins<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb4-3"><a href="data-representation-analysis-processing.html#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;median value&#39;</span>)</span></code></pre></div>
<p><img src="figures/hist_med_house_value.png" style="width:60.0%" /></p>
<p>More info:
<a href="https://seaborn.pydata.org/tutorial/distributions.html">seaborn.pydata.org</a></p>
</div>
</div>
<div id="multivariate-analysis" class="section level5 hasAnchor" number="2.2.1.2.2">
<h5><span class="header-section-number">2.2.1.2.2</span> Multivariate Analysis<a href="data-representation-analysis-processing.html#multivariate-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div id="continuous-vs-continuous" class="section level6 hasAnchor" number="2.2.1.2.2.1">
<h6><span class="header-section-number">2.2.1.2.2.1</span> Continuous vs Continuous<a href="data-representation-analysis-processing.html#continuous-vs-continuous" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><strong>Scatter-plots</strong> plot the values of the datapoints of one attribute on
the x-axis and the other attribute on the y-axis. This helps you find
the correlations, order of the relationship, outliers etc.</p>
<p>Use a <strong>pairplot</strong> to make a scatter plot of multiple features against
each other.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="data-representation-analysis-processing.html#cb5-1" aria-hidden="true" tabindex="-1"></a>sns.pairplot(df_USAhousing[[<span class="st">&quot;median_income&quot;</span>, <span class="st">&quot;median_house_value&quot;</span>, <span class="st">&quot;total_rooms&quot;</span>]], diag_kind<span class="op">=</span><span class="st">&quot;hist&quot;</span>)</span></code></pre></div>
<p><img src="figures/pairplot_usa_housing.png" style="width:80.0%" /></p>
<p>Alternatively use <strong>joint plots</strong>, to visualize the marginal
(univariate) distributions on the sides:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="data-representation-analysis-processing.html#cb6-1" aria-hidden="true" tabindex="-1"></a>sns.jointplot(data<span class="op">=</span>df_USAhousing, x<span class="op">=</span><span class="st">&quot;median_income&quot;</span>, y<span class="op">=</span><span class="st">&quot;median_house_value&quot;</span>)</span></code></pre></div>
<p><img src="figures/jointplot_housing.png" style="width:65.0%" /></p>
<p><strong>Heatmaps</strong> plot the magnitude of values in different categories. It is
commonly used in exploratory data analysis to show the correlation of
the different attributes.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="data-representation-analysis-processing.html#cb7-1" aria-hidden="true" tabindex="-1"></a>sns.heatmap(df.corr(), cmap<span class="op">=</span><span class="st">&quot;coolwarm&quot;</span>, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>, annot<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p><img src="figures/heatmap_usa_housing.png" style="width:65.0%" /></p>
<p>More info:
<a href="https://seaborn.pydata.org/tutorial/relational.html">seaborn.pydata.org</a></p>
</div>
<div id="continuous-vs.-categorical-data" class="section level6 hasAnchor" number="2.2.1.2.2.2">
<h6><span class="header-section-number">2.2.1.2.2.2</span> Continuous vs.Â Categorical data<a href="data-representation-analysis-processing.html#continuous-vs.-categorical-data" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><strong>Overlapping histograms</strong> plot the marginal distribution of the
continuous distributions, using different colors for each category:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="data-representation-analysis-processing.html#cb8-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">&#39;whitegrid&#39;</span>)</span>
<span id="cb8-2"><a href="data-representation-analysis-processing.html#cb8-2" aria-hidden="true" tabindex="-1"></a>sns.histplot(data<span class="op">=</span>df_USAhousing, x<span class="op">=</span><span class="st">&#39;median_house_value&#39;</span>, hue<span class="op">=</span><span class="st">&quot;ocean_proximity&quot;</span>, element<span class="op">=</span><span class="st">&quot;poly&quot;</span>,  bins<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb8-3"><a href="data-representation-analysis-processing.html#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;median value&#39;</span>)</span></code></pre></div>
<p><img src="figures/overlap_hist_usa_housing.png" style="width:60.0%" /></p>
<p>Use separate <strong>violin plots</strong> for each of the different categories:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="data-representation-analysis-processing.html#cb9-1" aria-hidden="true" tabindex="-1"></a>sns.catplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">&quot;cont_col&quot;</span>, y<span class="op">=</span><span class="st">&quot;cat_col&quot;</span>, hue<span class="op">=</span><span class="st">&quot;binary_col&quot;</span>, kind<span class="op">=</span><span class="st">&quot;violin&quot;</span>)</span></code></pre></div>
<p>Use <strong>heatmaps</strong> with two categorical feature as x- and y-axis
respectively and a continuous attribute as magnitude (âheatâ).</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="data-representation-analysis-processing.html#cb10-1" aria-hidden="true" tabindex="-1"></a>sns.heatmap(df.pivot(index<span class="op">=</span><span class="st">&quot;cat_col1&quot;</span>, columns<span class="op">=</span><span class="st">&quot;cat_col2&quot;</span>, values<span class="op">=</span><span class="st">&quot;cont_col&quot;</span>), annot<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span></code></pre></div>
</div>
<div id="categorical-vs-categorical" class="section level6 hasAnchor" number="2.2.1.2.2.3">
<h6><span class="header-section-number">2.2.1.2.2.3</span> Categorical vs Categorical<a href="data-representation-analysis-processing.html#categorical-vs-categorical" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><strong>Categorical plots</strong> plot the count / percentage of different
categorical attributes in side-by-side bar charts</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="data-representation-analysis-processing.html#cb11-1" aria-hidden="true" tabindex="-1"></a>sns.catplot(data<span class="op">=</span>df, y<span class="op">=</span><span class="st">&quot;cat_col1&quot;</span>, hue<span class="op">=</span><span class="st">&quot;cat_col2&quot;</span>, kind<span class="op">=</span><span class="st">&quot;bar&quot;</span>)</span></code></pre></div>
<p>More info:
<a href="https://seaborn.pydata.org/tutorial/categorical.html">seaborn.pydata.org</a></p>
<p><font color="grey"></p>
<!-- For feature selection -->
<!-- bias analysis -->
</div>
</div>
</div>
</div>
<div id="output-analysis" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Output Analysis<a href="data-representation-analysis-processing.html#output-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="ethical-ai" class="section level4 hasAnchor" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Ethical AI<a href="data-representation-analysis-processing.html#ethical-ai" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<!-- fairness / bias / discrimination  analysis -->
</div>
<div id="explanation-methods" class="section level4 hasAnchor" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> Explanation methods<a href="data-representation-analysis-processing.html#explanation-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<!-- Reference to explanation  methods -->
</div>
<div id="performance" class="section level4 hasAnchor" number="2.2.2.3">
<h4><span class="header-section-number">2.2.2.3</span> Performance<a href="data-representation-analysis-processing.html#performance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<!-- Reference to quality measures -->
<!-- Distribution of errors ... -->
</div>
</div>
<div id="monitoring" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Monitoring<a href="data-representation-analysis-processing.html#monitoring" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- Surprise analysis -->
<p></font></p>
</div>
</div>
<div id="preprocessing-data" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Preprocessing data<a href="data-representation-analysis-processing.html#preprocessing-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="handling-missing-wrong-data" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Handling missing &amp; wrong data<a href="data-representation-analysis-processing.html#handling-missing-wrong-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some algorithms assume that all features of all samples have numerical
values. In these cases missing values have to be imputed (i.e.Â inferred)
or (if affordable) the samples with missing feature values can be
deleted from the data set.</p>
<div id="iterative-imputor-by-sklearn" class="section level4 hasAnchor" number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> Iterative imputor by sklearn<a href="data-representation-analysis-processing.html#iterative-imputor-by-sklearn" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For features with missing values, this imputor imputes the missing
values by modelling each feature using the existing values from the
other features. It uses several iterations until the results converge.<br />
<strong>!</strong> This method scales with <span class="math inline">\(O(nd^3)\)</span>, where <span class="math inline">\(n\)</span> is the number of
samples and <span class="math inline">\(d\)</span> is the number of features.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="data-representation-analysis-processing.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.experimental <span class="im">import</span> enable_iterative_imputer <span class="co"># necessary since the imputor is still experimental</span></span>
<span id="cb12-2"><a href="data-representation-analysis-processing.html#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> IterativeImputer</span>
<span id="cb12-3"><a href="data-representation-analysis-processing.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor </span>
<span id="cb12-4"><a href="data-representation-analysis-processing.html#cb12-4" aria-hidden="true" tabindex="-1"></a>rf_estimator <span class="op">=</span> RamdomForestRegressor(n_estimators <span class="op">=</span> <span class="dv">8</span>, max_depth <span class="op">=</span> <span class="dv">6</span>, bootstrap <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb12-5"><a href="data-representation-analysis-processing.html#cb12-5" aria-hidden="true" tabindex="-1"></a>imputor <span class="op">=</span> IterativeImputer(random_state<span class="op">=</span><span class="dv">0</span>, estimator <span class="op">=</span> rf_estimator, max_iter <span class="op">=</span> <span class="dv">25</span>)</span>
<span id="cb12-6"><a href="data-representation-analysis-processing.html#cb12-6" aria-hidden="true" tabindex="-1"></a>imputor.fit_transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html">scikit-learn.org</a><br />
</p>
</div>
<div id="deleting-missing-values" class="section level4 hasAnchor" number="2.3.1.2">
<h4><span class="header-section-number">2.3.1.2</span> Deleting missing values<a href="data-representation-analysis-processing.html#deleting-missing-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="data-representation-analysis-processing.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-2"><a href="data-representation-analysis-processing.html#cb13-2" aria-hidden="true" tabindex="-1"></a>df.dropna(how<span class="op">=</span><span class="st">&quot;any&quot;</span>) <span class="co"># how=&quot;all&quot; would delete a sample if all values were missing</span></span></code></pre></div>
<p>More info:
<a href="https://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.DataFrame.dropna.html">pandas.pydata.org</a><br />
</p>
</div>
<div id="deleting-duplicate-entries" class="section level4 hasAnchor" number="2.3.1.3">
<h4><span class="header-section-number">2.3.1.3</span> Deleting duplicate entries<a href="data-representation-analysis-processing.html#deleting-duplicate-entries" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Duplicate entries need to be removed (exception: time series), to avoid
over representation and leakage into test set.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="data-representation-analysis-processing.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-2"><a href="data-representation-analysis-processing.html#cb14-2" aria-hidden="true" tabindex="-1"></a>df.drop_duplicates(keep<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div id="replacing-data" class="section level4 hasAnchor" number="2.3.1.4">
<h4><span class="header-section-number">2.3.1.4</span> Replacing data<a href="data-representation-analysis-processing.html#replacing-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="data-representation-analysis-processing.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-2"><a href="data-representation-analysis-processing.html#cb15-2" aria-hidden="true" tabindex="-1"></a>df.Col.<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">0</span> <span class="cf">if</span> x<span class="op">==</span><span class="st">&#39;zero&#39;</span> <span class="cf">else</span> <span class="dv">1</span>)</span></code></pre></div>
</div>
</div>
<div id="working-with-date-and-time" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Working with Date and Time<a href="data-representation-analysis-processing.html#working-with-date-and-time" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You can convert to the <strong>datetime format</strong> as follows:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="data-representation-analysis-processing.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-2"><a href="data-representation-analysis-processing.html#cb16-2" aria-hidden="true" tabindex="-1"></a>pd.to_datetime(df.date_col, infer_datetime_format<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>You create columns for <strong>year</strong>, <strong>month</strong>, <strong>day</strong> like this:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="data-representation-analysis-processing.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="data-representation-analysis-processing.html#cb17-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;year&#39;</span>] <span class="op">=</span> df.Date.dt.year</span>
<span id="cb17-3"><a href="data-representation-analysis-processing.html#cb17-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;month&#39;</span>] <span class="op">=</span> df.Date.dt.month</span>
<span id="cb17-4"><a href="data-representation-analysis-processing.html#cb17-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;day&#39;</span>] <span class="op">=</span> df.Date.dt.day</span></code></pre></div>
</div>
<div id="encoding-discretizing-data" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Encoding &amp; discretizing data<a href="data-representation-analysis-processing.html#encoding-discretizing-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are multiple ways to encode data, especially non-vectorized data,
to make it suitable for machine learning algorithms. The string values
(e.g.Â âmaleâ, âfemaleâ) of categorical features have to be converted
into integers. This can be done by two methods:</p>
<div id="ordinal-encoding" class="section level4 hasAnchor" number="2.3.3.1">
<h4><span class="header-section-number">2.3.3.1</span> Ordinal Encoding<a href="data-representation-analysis-processing.html#ordinal-encoding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An integer is assigned to each category (e.g.Â âmaleâ=0, âfemaleâ=1)</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="data-representation-analysis-processing.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OrdinalEncoder</span>
<span id="cb18-2"><a href="data-representation-analysis-processing.html#cb18-2" aria-hidden="true" tabindex="-1"></a>ord_enc <span class="op">=</span> preprocessing.OrdinalEncoder()</span>
<span id="cb18-3"><a href="data-representation-analysis-processing.html#cb18-3" aria-hidden="true" tabindex="-1"></a>ord_enc.fit(X)</span>
<span id="cb18-4"><a href="data-representation-analysis-processing.html#cb18-4" aria-hidden="true" tabindex="-1"></a>ord_enc.transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder">scikit-learn.org</a><br />
This method is useful when the categories have an ordered relationship
(e.g.Â âbadâ, âmediumâ, âgoodâ). If this is not the case (e.g.Â âdogâ,
âcatâ, âbunnyâ) this is to be avoided since the algorithm might deduct
an ordered relationship where there is none. For these cases
one-hot-encoding is to be used.</p>
</div>
<div id="one-hot-encoding" class="section level4 hasAnchor" number="2.3.3.2">
<h4><span class="header-section-number">2.3.3.2</span> One-Hot Encoding<a href="data-representation-analysis-processing.html#one-hot-encoding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One-hot encoding assigns a separate feature-column for each category and
encodes it binarily (e.g.Â if the sample is a dog, it has 1 in the
dog-column and 0 in the cat and bunny column).</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="data-representation-analysis-processing.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb19-2"><a href="data-representation-analysis-processing.html#cb19-2" aria-hidden="true" tabindex="-1"></a>onehot_enc <span class="op">=</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&#39;ignore&#39;</span>)</span>
<span id="cb19-3"><a href="data-representation-analysis-processing.html#cb19-3" aria-hidden="true" tabindex="-1"></a>onehot_enc.fit(X)</span>
<span id="cb19-4"><a href="data-representation-analysis-processing.html#cb19-4" aria-hidden="true" tabindex="-1"></a>onehot_enc.transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">scikit-learn.org</a><br />
</p>
<p>Alternative:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="data-representation-analysis-processing.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-2"><a href="data-representation-analysis-processing.html#cb20-2" aria-hidden="true" tabindex="-1"></a>pd.get_dummies(X, columns <span class="op">=</span> [<span class="st">&quot;Sex&quot;</span>, <span class="st">&quot;Type&quot;</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>More info:
<a href="https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html">pandas.pydata.org</a></p>
</div>
<div id="discretizing-binning-data" class="section level4 hasAnchor" number="2.3.3.3">
<h4><span class="header-section-number">2.3.3.3</span> Discretizing / binning data<a href="data-representation-analysis-processing.html#discretizing-binning-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>You can discretize features and targets from continuous to
discrete/categorical (e.g.Â age in years to child, teenager, adult,
elderly).</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="data-representation-analysis-processing.html#cb21-1" aria-hidden="true" tabindex="-1"></a>pd.cut(x<span class="op">=</span>cont_series, bins<span class="op">=</span> <span class="dv">4</span>, labels<span class="op">=</span>[<span class="st">&quot;child&quot;</span>, <span class="st">&quot;teenager&quot;</span>, <span class="st">&quot;adult&quot;</span>, <span class="st">&quot;elderly&quot;</span>])</span></code></pre></div>
<p>More info:
<a href="https://pandas.pydata.org/docs/reference/api/pandas.cut.html">pandas.pydata.org</a><br />
</p>
<p>Pros:</p>
<ul>
<li><p>It makes sense for the specific problem (e.g.Â targeting groups for
marketing).</p></li>
<li><p>Improved signal-to-noise ratio (bins work like regularization).</p></li>
<li><p>possibly highly non-linear relationship of continuous feature to
target is hard to learn for model.</p></li>
<li><p>Better interpretability of features, results and model.</p></li>
<li><p>Can be used to incorporate domain knowledge and make learning
easier.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Your model and results lose information</p></li>
<li><p>Senseless cut-offs between bins can create âartificial noiseâ and
make learning harder.</p></li>
</ul>
<p>More info:
<a href="https://datascience.stackexchange.com/q/19782/148392">stackexchange.com</a><br />
See also: <a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)">wikipedia: Sampling (signal
processing)</a>.</p>
<p><font color="grey"></p>
</div>
<div id="graph-representation-of-data" class="section level4 hasAnchor" number="2.3.3.4">
<h4><span class="header-section-number">2.3.3.4</span> Graph representation of data<a href="data-representation-analysis-processing.html#graph-representation-of-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The similarity/distance between points can be represented in graphs. The
data points are represented as nodes, the distances/similarities as
edges.</p>
<!-- include schema of a graph, explain pros and cons, include link to libraries and wikipedia -->
</div>
<div id="encoding-text-data" class="section level4 hasAnchor" number="2.3.3.5">
<h4><span class="header-section-number">2.3.3.5</span> Encoding Text data<a href="data-representation-analysis-processing.html#encoding-text-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="bag-of-words" class="section level5 hasAnchor" number="2.3.3.5.1">
<h5><span class="header-section-number">2.3.3.5.1</span> Bag of words<a href="data-representation-analysis-processing.html#bag-of-words" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div id="word-counts" class="section level6 hasAnchor" number="2.3.3.5.1.1">
<h6><span class="header-section-number">2.3.3.5.1.1</span> Word Counts<a href="data-representation-analysis-processing.html#word-counts" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<!-- Include countvectorizer from sklearn -->
</div>
<div id="word-frequencies-tfidf" class="section level6 hasAnchor" number="2.3.3.5.1.2">
<h6><span class="header-section-number">2.3.3.5.1.2</span> Word Frequencies / Tfidf<a href="data-representation-analysis-processing.html#word-frequencies-tfidf" class="anchor-section" aria-label="Anchor link to header"></a></h6>
</div>
<div id="hashing" class="section level6 hasAnchor" number="2.3.3.5.1.3">
<h6><span class="header-section-number">2.3.3.5.1.3</span> Hashing<a href="data-representation-analysis-processing.html#hashing" class="anchor-section" aria-label="Anchor link to header"></a></h6>
</div>
</div>
</div>
<div id="encoding-image-data" class="section level4 hasAnchor" number="2.3.3.6">
<h4><span class="header-section-number">2.3.3.6</span> Encoding image data<a href="data-representation-analysis-processing.html#encoding-image-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="patches" class="section level5 hasAnchor" number="2.3.3.6.1">
<h5><span class="header-section-number">2.3.3.6.1</span> Patches<a href="data-representation-analysis-processing.html#patches" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p></font></p>
</div>
</div>
</div>
<div id="standardization" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Standardization<a href="data-representation-analysis-processing.html#standardization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Many machine learning models assume that the features are centered
around 0 and that all have a similar variance. Therefore the data has to
be centered and scaled to unit variance before training and prediction.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="data-representation-analysis-processing.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb22-2"><a href="data-representation-analysis-processing.html#cb22-2" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb22-3"><a href="data-representation-analysis-processing.html#cb22-3" aria-hidden="true" tabindex="-1"></a>scaler.fit_transform(input_df)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">scikit-learn.org</a></p>
<p>Another option for scaling is normalization. This is used, when the
values have to fall strictly between a max and min value.<br />
More info:
<a href="https://scikit-learn.org/stable/modules/preprocessing.html#normalization">scikit-learn.org</a></p>
</div>
<div id="splitting-in-training--and-test-data" class="section level3 hasAnchor" number="2.3.5">
<h3><span class="header-section-number">2.3.5</span> Splitting in training- and test-data<a href="data-representation-analysis-processing.html#splitting-in-training--and-test-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You need to split your training set into test- and training-samples. The
algorithm uses the training samples with the known label/target value
for fitting the parameters. The test-set is used to determine if the
trained algorithm performs well on new samples as well. You need to give
special considerations to the following points:</p>
<ul>
<li><p>Avoiding data or other information to leak from the training set to
the test-set</p></li>
<li><p>Validating if the predictive performance deteriorates over time
(i.e.Â the algorithm will perform worse on new samples). This is
especially important for models that make predictions for future
events.</p></li>
<li><p>Conversely, sampling the test- and training-sets randomly to avoid
introducing bias in the two sets.</p></li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="data-representation-analysis-processing.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># assuming you already imported the data and separated the label column:</span></span>
<span id="cb23-2"><a href="data-representation-analysis-processing.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb23-3"><a href="data-representation-analysis-processing.html#cb23-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.33</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">scikit-learn.org</a></p>
</div>
<div id="feature-selection" class="section level3 hasAnchor" number="2.3.6">
<h3><span class="header-section-number">2.3.6</span> Feature selection<a href="data-representation-analysis-processing.html#feature-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Usually the label does not depend on all available features. To detect
causal features, remove noisy ones and reduce the running and training
costs of the algorithm, we reduce the amount of features to the relevant
ones. This can be done a priori (before training) or using wrapper
methods (integrated with the prediction algorithm to be used).<br />
<strong>!</strong> There are methods that have feature selection already built-in,
such as decision trees.</p>
<div id="a-priori-feature-selection" class="section level4 hasAnchor" number="2.3.6.1">
<h4><span class="header-section-number">2.3.6.1</span> A priori feature selection<a href="data-representation-analysis-processing.html#a-priori-feature-selection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A cheap method is to remove all features with <strong>variance</strong> below a
certain threshold.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="data-representation-analysis-processing.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb24-2"><a href="data-representation-analysis-processing.html#cb24-2" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> VarianceThreshold(threshold<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb24-3"><a href="data-representation-analysis-processing.html#cb24-3" aria-hidden="true" tabindex="-1"></a>selector.fit_transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold">scikit-learn.org</a></p>
<p>The <strong>Mutual information {#mutual_info} score</strong> works by choosing the
features that have the highest dependency between the features and the
label.</p>
<p><span class="math display">\[ I(X, Y) =D_{KL} \left( P(X=x, Y=y), P(X=x) \otimes P(Y=y) \right) =\sum_{y \in Y} \sum_{x \in X}
    { P(X=x, Y=y) \log\left(\frac{P(X=x, Y=y)}{P(X=x)P(Y=y)}\right) }\]</span></p>
<p>where, <span class="math inline">\(D_{KL}\)</span> is the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KullbackâLeibler
divergence</a>
(A measure of similarity between distributions). The <span class="math inline">\(\log\)</span>-Term is for
quantifying how different the joint distribution is from the product of
the marginal distributions.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="data-representation-analysis-processing.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SelectKBest</span>
<span id="cb25-2"><a href="data-representation-analysis-processing.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> mutual_info_classif <span class="co"># for regression use mutual_info_regression</span></span>
<span id="cb25-3"><a href="data-representation-analysis-processing.html#cb25-3" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> SelectKBest(mutual_info_classif, k<span class="op">=</span><span class="dv">8</span>).fit_transform(X, y)</span></code></pre></div>
<p>More <a href="%5Binfo:%5D(info:)%7B.uri%7D" class="uri">[info:\\](info:){.uri}</a>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">scikit-learn.org</a><br />
<a href="https://en.wikipedia.org/wiki/Mutual_information">wikipedia.org/wiki/Mutual_information</a></p>
</div>
<div id="wrapper-methods" class="section level4 hasAnchor" number="2.3.6.2">
<h4><span class="header-section-number">2.3.6.2</span> wrapper methods<a href="data-representation-analysis-processing.html#wrapper-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Using <strong>greedy feature selection</strong> as a wrapper method, one commonly
starts with 0 features and adds the feature that returns the highest
score with the used classifier.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="data-representation-analysis-processing.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SequentialFeatureSelector</span>
<span id="cb26-2"><a href="data-representation-analysis-processing.html#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb26-3"><a href="data-representation-analysis-processing.html#cb26-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb26-4"><a href="data-representation-analysis-processing.html#cb26-4" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> SequentialFeatureSelector(classifier, n_features_to_select<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb26-5"><a href="data-representation-analysis-processing.html#cb26-5" aria-hidden="true" tabindex="-1"></a>selector.fit_transform(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html">scikit-learn.org</a></p>
</div>
<div id="advice-pitfalls" class="section level4 hasAnchor" number="2.3.6.3">
<h4><span class="header-section-number">2.3.6.3</span> Advice &amp; Pitfalls<a href="data-representation-analysis-processing.html#advice-pitfalls" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Selected advice from paper from <a href="https://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf">Guyon and
Elisseeff</a>:</p>
<ul>
<li><p>If you have domain knowledge: Use it.</p></li>
<li><p>Are your features commensurate (same proportion): Normalize them.</p></li>
<li><p>Do you suspect interdependent features: Construct conjunctive
features or products of features.</p></li>
</ul>
<p>Other advice:</p>
<ul>
<li><p>Features that are useless on their own, can be useful in combination
with other features.</p></li>
<li><p>Using multiple redundant variables can be useful to reduce noise.</p></li>
<li><p>There are also models (e.g.Â lasso regression, decision trees) that
have feature selection built into the model (i.e.Â by only allowing
for a certain number of features to be used or penalizing the use of
additional features).</p></li>
</ul>
</div>
</div>
<div id="hyper-parameter-tuning" class="section level3 hasAnchor" number="2.3.7">
<h3><span class="header-section-number">2.3.7</span> Hyper-parameter tuning<a href="data-representation-analysis-processing.html#hyper-parameter-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The hyper-parameters (e.g.Â kernel, gamma, number of nodes in tree) are
not trained by algorithm itself. An outer loop of hyper-parameter tuning
is needed to find the optimal hyper parameters.<br />
<strong>!</strong> It is strongly recommended to separate another validation set from
the training set for hyper-parameter tuning (youâll end up with
training-, validation- and test-set). See <a href="data-representation-analysis-processing.html#crossval">Cross Validation</a>
for best practice.</p>
<div id="grid-search" class="section level4 hasAnchor" number="2.3.7.1">
<h4><span class="header-section-number">2.3.7.1</span> Grid search<a href="data-representation-analysis-processing.html#grid-search" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The classic approach is exhaustive grid search: You create a grid of
hyper-parameters and iterate over all combinations. The combination with
the best score is used in the end. This approach causes big
computational costs due to the combinatorial explosion.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="data-representation-analysis-processing.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV <span class="co"># combines grid search with cross-validation</span></span>
<span id="cb27-2"><a href="data-representation-analysis-processing.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb27-3"><a href="data-representation-analysis-processing.html#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="data-representation-analysis-processing.html#cb27-4" aria-hidden="true" tabindex="-1"></a>kn_model <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb27-5"><a href="data-representation-analysis-processing.html#cb27-5" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> {<span class="st">&quot;n_neighbors&quot;</span>: <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">10</span>), <span class="st">&quot;p&quot;</span>: [<span class="dv">1</span>,<span class="dv">2</span>], <span class="st">&quot;weights&quot;</span>: [<span class="st">&quot;uniform&quot;</span>, <span class="st">&quot;distance&quot;</span>]}</span>
<span id="cb27-6"><a href="data-representation-analysis-processing.html#cb27-6" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GridSearchCV(kn_model, parameters, cv<span class="op">=</span><span class="dv">5</span>) </span>
<span id="cb27-7"><a href="data-representation-analysis-processing.html#cb27-7" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">scikit-learn.org</a><br />
</p>
</div>
<div id="randomized-search" class="section level4 hasAnchor" number="2.3.7.2">
<h4><span class="header-section-number">2.3.7.2</span> Randomized search<a href="data-representation-analysis-processing.html#randomized-search" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This approach is used, if there are too many combinations of
hyper-parameters for tuning. You allocate a budget of iterations and the
combinations of parameters are sampled randomly according to the
distributions you provide.</p>
<p>If you want to evaluate on a large set of hyperparameters, you can use a
halving strategy: You tune a large combination of parameters on few
resources (e.g.Â samples, trees). The best performing half of candidates
is re-evaluated on twice as many resources. This continues until the
best-performing candidate is evaluated on the full amount of resources.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="data-representation-analysis-processing.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb28-2"><a href="data-representation-analysis-processing.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.experimental <span class="im">import</span> enable_halving_search_cv  <span class="co"># since this method is still experimental</span></span>
<span id="cb28-3"><a href="data-representation-analysis-processing.html#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> HalvingRandomSearchCV</span>
<span id="cb28-4"><a href="data-representation-analysis-processing.html#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils.fixes <span class="im">import</span> loguniform</span>
<span id="cb28-5"><a href="data-representation-analysis-processing.html#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="data-representation-analysis-processing.html#cb28-6" aria-hidden="true" tabindex="-1"></a>rf_clf <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb28-7"><a href="data-representation-analysis-processing.html#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="data-representation-analysis-processing.html#cb28-8" aria-hidden="true" tabindex="-1"></a>param_distributions <span class="op">=</span> {<span class="st">&quot;max_depth&quot;</span>: [<span class="dv">3</span>, <span class="va">None</span>],</span>
<span id="cb28-9"><a href="data-representation-analysis-processing.html#cb28-9" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;min_samples_split&quot;</span>: loguniform(<span class="dv">1</span>, <span class="dv">10</span>)}</span>
<span id="cb28-10"><a href="data-representation-analysis-processing.html#cb28-10" aria-hidden="true" tabindex="-1"></a>hypa_search <span class="op">=</span> HalvingRandomSearchCV(rf_clf, param_distributions,</span>
<span id="cb28-11"><a href="data-representation-analysis-processing.html#cb28-11" aria-hidden="true" tabindex="-1"></a>                               resource<span class="op">=</span><span class="st">&#39;n_estimators&#39;</span>,</span>
<span id="cb28-12"><a href="data-representation-analysis-processing.html#cb28-12" aria-hidden="true" tabindex="-1"></a>                               max_resources<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb28-13"><a href="data-representation-analysis-processing.html#cb28-13" aria-hidden="true" tabindex="-1"></a>                               n_jobs<span class="op">=-</span><span class="dv">1</span>, <span class="co"># important since hyper-parameter tuning is very costly</span></span>
<span id="cb28-14"><a href="data-representation-analysis-processing.html#cb28-14" aria-hidden="true" tabindex="-1"></a>                               scoring <span class="op">=</span> <span class="st">&#39;balanced_accuracy&#39;</span>,</span>
<span id="cb28-15"><a href="data-representation-analysis-processing.html#cb28-15" aria-hidden="true" tabindex="-1"></a>                               random_state<span class="op">=</span><span class="dv">0</span>).fit(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/grid_search.html#searching-for-optimal-parameters-with-successive-halving">scikit-learn.org</a><br />
</p>
</div>
</div>
<div id="model-selection" class="section level3 hasAnchor" number="2.3.8">
<h3><span class="header-section-number">2.3.8</span> Model selection<a href="data-representation-analysis-processing.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The candidates for hyper-parameters must not be evaluated on the same
data that you trained it on (over-fitting risk). Thus, we separate
another data-set from the training data: The validation set. This is
reduces the amount of training data drastically. Therefore we use the
approaches of Cross Validation and Bootstrapping.</p>
<div id="crossval" class="section level4 hasAnchor" number="2.3.8.1">
<h4><span class="header-section-number">2.3.8.1</span> Cross Validation<a href="data-representation-analysis-processing.html#crossval" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In k-fold Cross Validation, we split the training set into k sub-sets.
We train on the samples in k-1 sub-sets and validate using the data in
the remaining sub-set. We iterate until we have validated on each
sub-set once. We then average out the k scores we obtain.</p>
<div class="figure">
<img src="figures/cross_validation.png" style="width:60.0%" alt="" />
<p class="caption">Schema of the process for 5-fold Cross Validation. The data is first
split into training- and test-data. The training data is split into 5
sub-sets. The algorithm is trained on 4 sub-sets and evaluated on the
remaining sub-set. Each sub-set is used for validation once. <em>Source:
<a href="https://scikit-learn.org/stable/modules/cross_validation.html">scikit-learn.org</a>.</em></p>
</div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="data-representation-analysis-processing.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb29-2"><a href="data-representation-analysis-processing.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb29-3"><a href="data-representation-analysis-processing.html#cb29-3" aria-hidden="true" tabindex="-1"></a>SVM_clf <span class="op">=</span> svm.SVC (kernel<span class="op">=</span><span class="st">&#39;polynomial&#39;</span>)</span>
<span id="cb29-4"><a href="data-representation-analysis-processing.html#cb29-4" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> cross_val_score(SVM_clf, X, y, cv <span class="op">=</span> <span class="dv">7</span>)</span>
<span id="cb29-5"><a href="data-representation-analysis-processing.html#cb29-5" aria-hidden="true" tabindex="-1"></a>cv_score <span class="op">=</span> cv_scores.mean()</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics">scikit-learn.org</a></p>
<p><strong>!</strong> If you have time-series data (and other clearly not i.i.d.) data,
you have to use <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">special cross-validation
strategies</a>.
There are <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">further
strategies</a>
worth considering.</p>
</div>
<div id="bootstrapping" class="section level4 hasAnchor" number="2.3.8.2">
<h4><span class="header-section-number">2.3.8.2</span> Bootstrapping<a href="data-representation-analysis-processing.html#bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Instead of splitting the data into k subsets, you can also just sample
data into training and validation sets.<br />
More info:
<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">wikipedia.org</a>.</p>
</div>
</div>
</div>
<div id="errors-regularization" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Errors &amp; regularization<a href="data-representation-analysis-processing.html#errors-regularization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are irreducible errors and reducible errors. Irreducible errors
stem from unknown variables or variables we have no data on. Reducible
errors are deviations from our model to its desired behavior and can be
reduced. Bias and variance are reducible errors.</p>
<p><span class="math display">\[\text{Error} = \text{Bias} + \text{Var} + \text{irr. Error}\]</span></p>
<div id="bias-and-variance" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Bias and Variance<a href="data-representation-analysis-processing.html#bias-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="bias-of-an-estimator" class="section level4 hasAnchor" number="2.4.1.1">
<h4><span class="header-section-number">2.4.1.1</span> Bias of an estimator<a href="data-representation-analysis-processing.html#bias-of-an-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Bias tells you if your model oversimplifies the true relationship in
your data (underfitting).<br />
You have a model with a parameter <span class="math inline">\(\hat{\theta}\)</span> that is an estimator
for the true <span class="math inline">\(\theta\)</span>. You want to know whether your model over- or
underestimates the true <span class="math inline">\(\theta\)</span> systematically.</p>
<p><span class="math display">\[\text{Bias}[\hat{\theta}]=\text{E}_{X|\mathcal{D}}[\hat{\theta}]- \theta\]</span></p>
<p>E.g. if the parameter captures how polynomial the model / relationship
of your data is, a too high value means that your model is
underfitting.<br />
</p>
<p>More info:
<a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">wikipedia.org</a></p>
</div>
<div id="variance-of-an-estimator" class="section level4 hasAnchor" number="2.4.1.2">
<h4><span class="header-section-number">2.4.1.2</span> Variance of an estimator<a href="data-representation-analysis-processing.html#variance-of-an-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Variance tells you if your model learns from noise instead of the true
relationship in your data (overfitting).</p>
<p><span class="math display">\[\text{Var}[\hat{\theta}]=\text{E}_{X|\mathcal{D}}[(\text{E}_{X|\mathcal{D}}[\hat{\theta}]- \hat{\theta})^2]\]</span>
i.e.Â If you would bootstrap your data, it would show you how much your
parameter would jump around its mean, when it learns from the different
sampled sets.<br />
</p>
<p>Your goal is now to find the sweet spot between a too biased (too simple
model) and a model with too high variance (too complex model).<br />
</p>
<div class="figure">
<img src="figures/Bias_and_variance_contributing_to_total_error.png" style="width:60.0%" alt="" />
<p class="caption">Relationship between bias, variance and the total error. The minimum
of the total error lies at the best compromise between bias and
variance. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Bias_and_variance_contributing_to_total_error.svg">User Bigbossfarin on
wikimedia.org.</a>.</em></p>
</div>
<p>More info:
<a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">wikipedia.org</a></p>
</div>
</div>
<div id="regularization" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Regularization<a href="data-representation-analysis-processing.html#regularization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To combat overfitting, we can introduce a term into our loss-function
that penalizes complex models. For linear regression, our regularized
loss function is will be:</p>
<p><span class="math display">\[\min L(\hat{y},y)= \min_{W,b} f(WX+b,y)+\lambda R(W)\]</span> where <span class="math inline">\(f\)</span> is
the unregularized loss function, <span class="math inline">\(W\)</span> is the weight matrix, <span class="math inline">\(X\)</span> is the
sample matrix and <span class="math inline">\(b\)</span> is the bias or offset term of the model (bias term
<span class="math inline">\(\neq\)</span> bias of estimator!). <span class="math inline">\(R\)</span> is the regularization function and
<span class="math inline">\(\lambda\)</span> is a parameter controlling its strength.<br />
i.e.Â The regularized loss function punishes large weights <span class="math inline">\(W\)</span> and leads
to flatter/smoother functions.<br />
</p>
<p>More info:
<a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">wikipedia.org</a></p>
</div>
<div id="bagging" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Bagging<a href="data-representation-analysis-processing.html#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Train several instances of a complex estimator (aka. strong learner,
like large decision trees or KNN with small radius) on a subset of the
data. Then use a majority vote or average the scores for classifying to
get the final prediction. By training on different subsets and averaging
the results, the chances of overfitting are greatly reduced.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="data-representation-analysis-processing.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb30-2"><a href="data-representation-analysis-processing.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb30-3"><a href="data-representation-analysis-processing.html#cb30-3" aria-hidden="true" tabindex="-1"></a>bagging <span class="op">=</span> BaggingClassifier(KNeighborsClassifier(), max_features<span class="op">=</span><span class="fl">0.5</span>, n_estimators<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html">scikit-learn.org</a><br />
</p>
<p>A classic example for a bagging classifier is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Random Forest
Classifier</a>
or its variant <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Extremely Randomized
Trees</a>
which further reduces variance and increases bias.</p>
</div>
<div id="boosting" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Boosting<a href="data-representation-analysis-processing.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compared to bagging, we use weak learners that are not trained
independently of each other. We start with a single weak learner (e.g.Â a
small decision tree) and repeat the following steps:</p>
<ol style="list-style-type: decimal">
<li>Add an additional model and train it.</li>
<li>Increase weights of training samples that are falsely classified,
decrease weights of correctly classified samples. (to be used by
next added model.)</li>
<li>Reweight results from the models in the combined model to reduce the
training error.</li>
</ol>
<p>The final model is an weighted ensemble of weak classifiers.<br />
The most popular ones are <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier">gradient boosted decision
tree</a>
algorithms.</p>
</div>
<div id="stacking" class="section level3 hasAnchor" number="2.4.5">
<h3><span class="header-section-number">2.4.5</span> Stacking<a href="data-representation-analysis-processing.html#stacking" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Stacking closely resembles bagging: An ensemble of separately trained
base models is used to create an ensemble model. However, the continuous
(instead of discrete) outputs of commonly fewer heterogeneous models
(instead of same type of models) are used. The continuous outputs are
then fed into a final estimator (commonly logistic regression
classifier).</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="data-representation-analysis-processing.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb31-2"><a href="data-representation-analysis-processing.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb31-3"><a href="data-representation-analysis-processing.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb31-4"><a href="data-representation-analysis-processing.html#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb31-5"><a href="data-representation-analysis-processing.html#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb31-6"><a href="data-representation-analysis-processing.html#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> StackingClassifier</span>
<span id="cb31-7"><a href="data-representation-analysis-processing.html#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="data-representation-analysis-processing.html#cb31-8" aria-hidden="true" tabindex="-1"></a>classifiers <span class="op">=</span> [</span>
<span id="cb31-9"><a href="data-representation-analysis-processing.html#cb31-9" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;svc&#39;</span>, SVC()),</span>
<span id="cb31-10"><a href="data-representation-analysis-processing.html#cb31-10" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;knn&#39;</span>, KNeighborsClassifier()),</span>
<span id="cb31-11"><a href="data-representation-analysis-processing.html#cb31-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;dtc&#39;</span>, DecisionTreeClassifier())</span>
<span id="cb31-12"><a href="data-representation-analysis-processing.html#cb31-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb31-13"><a href="data-representation-analysis-processing.html#cb31-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-14"><a href="data-representation-analysis-processing.html#cb31-14" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> StackingClassifier(</span>
<span id="cb31-15"><a href="data-representation-analysis-processing.html#cb31-15" aria-hidden="true" tabindex="-1"></a>    classifiers<span class="op">=</span>estimators, final_estimator<span class="op">=</span>LogisticRegression()</span>
<span id="cb31-16"><a href="data-representation-analysis-processing.html#cb31-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb31-17"><a href="data-representation-analysis-processing.html#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="data-representation-analysis-processing.html#cb31-18" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html">scikit-learn.org</a></p>
</div>
</div>
<div id="tips-for-machine-learning-projects" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Tips for machine learning projects<a href="data-representation-analysis-processing.html#tips-for-machine-learning-projects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="general-advice" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> General advice<a href="data-representation-analysis-processing.html#general-advice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>General advice for machine learning from <a href="https://courses.cs.duke.edu/spring20/compsci527/papers/Domingos.pdf">Pedro
Domingos</a>:</p>
<ul>
<li><p>Let your knowledge about the problem help you choose the candidate
algorithms. E.g. You know the rules on which comparing samples makes
most sense <span class="math inline">\(\rightarrow\)</span> Choose instance based learners. If you know
that statistical dependencies are relevant <span class="math inline">\(\rightarrow\)</span> choose
Graph based models.</p></li>
<li><p>Donât underestimate the impact of feature engineering: Many domain
specific features can boost the accuracy.</p></li>
<li><p>Get more samples and candidate features (instead of focussing on the
algorithm)</p></li>
<li><p>Donât confuse correlation with causation. Just because your model
can predict something, it does not mean that the features cause the
target and you thus cannot easily deduct a clear action from it.</p></li>
</ul>
</div>
<div id="common-mistakes" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Common mistakes<a href="data-representation-analysis-processing.html#common-mistakes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Be aware: This list will never capture everything that can go wrong. ;-)</p>
<ul>
<li><p><a href="https://scikit-learn.org/stable/common_pitfalls.html#data-leakage"><strong>Data
Leakage</strong></a><strong>:</strong>
Information from Samples in your test data have leaked into your
training data.</p>
<ul>
<li>You have not deleted duplicates beforehand</li>
<li>You falsely assumed that your samples where drawn independently
and have sampled the training set randomly. (E.g. multiple
samples from the same patient, time series data)</li>
<li>You have the class label encoded in the training features in a
way that you will not find in âNatureâ.</li>
<li>You just used the wrong training / test set while programming.</li>
<li>You did feature engineering like finding n-grams or Max, Min of
data using your test-set data.</li>
<li><strong>Remedy:</strong> Careful preliminary data analysis, deduplication,</li>
</ul></li>
<li><p><strong>Using the wrong quality measures on unbalanced data:</strong> E.g.
Accuracy on unbalanced data is not a reasonable quality measure.</p></li>
<li><p><a href="https://scikit-learn.org/stable/common_pitfalls.html#inconsistent-preprocessing"><strong>Inconsistent
preprocessing</strong></a><strong>:</strong>
If you preprocess your training data in a certain way, you have to
do the same with the test- and prediction-data.</p>
<ul>
<li><strong>Remedy</strong>: Use one preprocessing pipeline that you can use for
training, testing and prediction.</li>
</ul></li>
<li><p><strong>Curse of dimensionality:</strong></p>
<ul>
<li>You use too many features for the amount of samples that you
have</li>
<li>Your distance measure is not suitable for high-dimensional space
(e.g.Â Hamming distance, Euclidean distance)</li>
<li><strong>Remedy:</strong> Use lower-dimensional mapping, feature selection.</li>
</ul></li>
<li><p><strong>Overfitting:</strong></p>
<ul>
<li>You use a too complex algorithm (too many degrees of freedom)
for the amount of data you have</li>
<li>You have too many features</li>
<li><strong>Remedy:</strong> Get more samples, reduce the dimensionanlity,
feature selection, regularization, bagging, boosting, stacking.</li>
</ul></li>
<li><p><strong>Bad Data:</strong></p>
<ul>
<li>Your data is not representative of what you would find in the
âreal worldâ. (skewed population, too old data, only of specific
sensors, locationsâ¦)</li>
<li>Your have many missing values among your features.</li>
<li>The data that you have is only remotely linked to the target
that you want to predict.</li>
<li>There are erroneous entries in your data.</li>
<li><strong>Remedy:</strong> Clean data at source, impute data, clean data during
preprocessing, get more representative data, limit scope of
application.</li>
</ul></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability-theory-linear-algebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/MoritzGuck/Machine-Learning-Reference/edit/master/02-Data.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
