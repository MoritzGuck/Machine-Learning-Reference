<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Unsupervised Learning | Machine Learning Reference</title>
  <meta name="description" content="Chapter 4 Unsupervised Learning | Machine Learning Reference" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Unsupervised Learning | Machine Learning Reference" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="MoritzGuck/Machine-Learning-Reference" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Unsupervised Learning | Machine Learning Reference" />
  
  
  

<meta name="author" content="Moritz Gück" />


<meta name="date" content="2023-04-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-methods.html"/>
<link rel="next" href="generative-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>Machine Learning Reference</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html"><i class="fa fa-check"></i><b>1</b> Probability Theory &amp; Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-theory"><i class="fa fa-check"></i><b>1.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-basics"><i class="fa fa-check"></i><b>1.1.1</b> Probability Basics</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-distributions"><i class="fa fa-check"></i><b>1.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#CLT"><i class="fa fa-check"></i><b>1.1.3</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#bayesian-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian probability</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#further-concepts"><i class="fa fa-check"></i><b>1.1.5</b> Further Concepts</a></li>
<li class="chapter" data-level="1.1.6" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#statistical-tests"><i class="fa fa-check"></i><b>1.1.6</b> Statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#vectors"><i class="fa fa-check"></i><b>1.2.1</b> Vectors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html"><i class="fa fa-check"></i><b>2</b> Data: Representation, Analysis &amp; Processing</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-and-distance-measures"><i class="fa fa-check"></i><b>2.1</b> Similarity and Distance Measures</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#metrics"><i class="fa fa-check"></i><b>2.1.1</b> Metrics</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#similarity-measures-on-vectors"><i class="fa fa-check"></i><b>2.1.2</b> Similarity measures on vectors</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#kernels"><i class="fa fa-check"></i><b>2.1.3</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#data-analysis"><i class="fa fa-check"></i><b>2.2</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>2.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#output-analysis"><i class="fa fa-check"></i><b>2.2.2</b> Output Analysis</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#monitoring"><i class="fa fa-check"></i><b>2.2.3</b> Monitoring</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#preprocessing-data"><i class="fa fa-check"></i><b>2.3</b> Preprocessing data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#handling-missing-wrong-data"><i class="fa fa-check"></i><b>2.3.1</b> Handling missing &amp; wrong data</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#working-with-date-and-time"><i class="fa fa-check"></i><b>2.3.2</b> Working with Date and Time</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#encoding-discretizing-data"><i class="fa fa-check"></i><b>2.3.3</b> Encoding &amp; discretizing data</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#standardization"><i class="fa fa-check"></i><b>2.3.4</b> Standardization</a></li>
<li class="chapter" data-level="2.3.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#splitting-in-training--and-test-data"><i class="fa fa-check"></i><b>2.3.5</b> Splitting in training- and test-data</a></li>
<li class="chapter" data-level="2.3.6" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#feature-selection"><i class="fa fa-check"></i><b>2.3.6</b> Feature selection</a></li>
<li class="chapter" data-level="2.3.7" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>2.3.7</b> Hyper-parameter tuning</a></li>
<li class="chapter" data-level="2.3.8" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#model-selection"><i class="fa fa-check"></i><b>2.3.8</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#errors-regularization"><i class="fa fa-check"></i><b>2.4</b> Errors &amp; regularization</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bias-and-variance"><i class="fa fa-check"></i><b>2.4.1</b> Bias and Variance</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#regularization"><i class="fa fa-check"></i><b>2.4.2</b> Regularization</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#bagging"><i class="fa fa-check"></i><b>2.4.3</b> Bagging</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#boosting"><i class="fa fa-check"></i><b>2.4.4</b> Boosting</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#stacking"><i class="fa fa-check"></i><b>2.4.5</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#tips-for-machine-learning-projects"><i class="fa fa-check"></i><b>2.5</b> Tips for machine learning projects</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#general-advice"><i class="fa fa-check"></i><b>2.5.1</b> General advice</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-representation-analysis-processing.html"><a href="data-representation-analysis-processing.html#common-mistakes"><i class="fa fa-check"></i><b>2.5.2</b> Common mistakes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-methods.html"><a href="classification-methods.html"><i class="fa fa-check"></i><b>3</b> Classification Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-methods.html"><a href="classification-methods.html#evaluation-of-classifiers"><i class="fa fa-check"></i><b>3.1</b> Evaluation of Classifiers</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-methods.html"><a href="classification-methods.html#confusion-matrix"><i class="fa fa-check"></i><b>3.1.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-methods.html"><a href="classification-methods.html#basic-quality-measures"><i class="fa fa-check"></i><b>3.1.2</b> Basic Quality Measures</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-methods.html"><a href="classification-methods.html#area-under-the-curve"><i class="fa fa-check"></i><b>3.1.3</b> Area under the Curve</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-methods.html"><a href="classification-methods.html#handling-unbalanced-data"><i class="fa fa-check"></i><b>3.1.4</b> Handling Unbalanced Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-methods.html"><a href="classification-methods.html#classification-algorithms"><i class="fa fa-check"></i><b>3.2</b> Classification Algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-methods.html"><a href="classification-methods.html#nearest-neighbors-classifier"><i class="fa fa-check"></i><b>3.2.1</b> Nearest Neighbors Classifier</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-methods.html"><a href="classification-methods.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>3.2.2</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-methods.html"><a href="classification-methods.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>3.2.3</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-methods.html"><a href="classification-methods.html#support-vector-classifier-svc"><i class="fa fa-check"></i><b>3.2.4</b> Support Vector Classifier (SVC)</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-methods.html"><a href="classification-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.2.5</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#custering-methods"><i class="fa fa-check"></i><b>4.1</b> Custering Methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#metrics-for-clustering-algorithms"><i class="fa fa-check"></i><b>4.1.1</b> metrics for Clustering algorithms</a></li>
<li class="chapter" data-level="4.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#graph-based-clustering"><i class="fa fa-check"></i><b>4.1.3</b> Graph-Based Clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#spectral-clustering"><i class="fa fa-check"></i><b>4.1.4</b> Spectral Clustering</a></li>
<li class="chapter" data-level="4.1.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#SSP"><i class="fa fa-check"></i><b>4.1.5</b> Sparse Subspace Clustering (SSP)</a></li>
<li class="chapter" data-level="4.1.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#soft-assignment-clustering"><i class="fa fa-check"></i><b>4.1.6</b> Soft-assignment Clustering</a></li>
<li class="chapter" data-level="4.1.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#artificial-neural-networks-for-clustering"><i class="fa fa-check"></i><b>4.1.7</b> Artificial Neural Networks for Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#mapping-to-lower-dimensions"><i class="fa fa-check"></i><b>4.2</b> Mapping to lower dimensions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#manifold-learning"><i class="fa fa-check"></i><b>4.2.1</b> Manifold learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#decomposition-techniques"><i class="fa fa-check"></i><b>4.2.2</b> Decomposition techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#outlier-detection"><i class="fa fa-check"></i><b>4.3</b> Outlier detection</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#local-outlier-factor"><i class="fa fa-check"></i><b>4.3.1</b> Local outlier factor</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#isolation-forest"><i class="fa fa-check"></i><b>4.3.2</b> Isolation forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generative-models.html"><a href="generative-models.html"><i class="fa fa-check"></i><b>5</b> Generative models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="generative-models.html"><a href="generative-models.html#generative-models-for-discrete-data"><i class="fa fa-check"></i><b>5.1</b> Generative Models for Discrete Data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="generative-models.html"><a href="generative-models.html#bayesian-concept-learning"><i class="fa fa-check"></i><b>5.1.1</b> Bayesian Concept Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="generative-models.html"><a href="generative-models.html#beta-binomial-model"><i class="fa fa-check"></i><b>5.1.2</b> Beta-binomial model</a></li>
<li class="chapter" data-level="5.1.3" data-path="generative-models.html"><a href="generative-models.html#dirichlet-multinomial-model"><i class="fa fa-check"></i><b>5.1.3</b> Dirichlet-multinomial model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#evaluation-of-regression-models"><i class="fa fa-check"></i><b>6.1</b> Evaluation of regression models</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#mean-squared-error"><i class="fa fa-check"></i><b>6.1.1</b> Mean squared error</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#r2-score-coefficient-of-determination"><i class="fa fa-check"></i><b>6.1.2</b> R^2 score / coefficient of determination</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#visual-tools"><i class="fa fa-check"></i><b>6.1.3</b> Visual tools</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#linear-models"><i class="fa fa-check"></i><b>6.2</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>6.2.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#lasso-regression"><i class="fa fa-check"></i><b>6.2.2</b> Lasso regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="regression.html"><a href="regression.html#ridge-regression"><i class="fa fa-check"></i><b>6.2.3</b> Ridge regression</a></li>
<li class="chapter" data-level="6.2.4" data-path="regression.html"><a href="regression.html#bayesian-regression"><i class="fa fa-check"></i><b>6.2.4</b> Bayesian regression</a></li>
<li class="chapter" data-level="6.2.5" data-path="regression.html"><a href="regression.html#anova"><i class="fa fa-check"></i><b>6.2.5</b> ANOVA</a></li>
<li class="chapter" data-level="6.2.6" data-path="regression.html"><a href="regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>6.2.6</b> Generalized linear models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.3</b> Gaussian process regression</a></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#gradient-boosted-tree-regression"><i class="fa fa-check"></i><b>6.4</b> Gradient boosted tree regression</a></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#time-series-forecasting"><i class="fa fa-check"></i><b>6.5</b> Time Series Forecasting</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#arimax-model"><i class="fa fa-check"></i><b>6.5.1</b> ARIMA(X) Model</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#varmmax-model"><i class="fa fa-check"></i><b>6.5.2</b> VARMMA(X) Model</a></li>
<li class="chapter" data-level="6.5.3" data-path="regression.html"><a href="regression.html#prophet-model"><i class="fa fa-check"></i><b>6.5.3</b> Prophet-Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Networks {#Neural Networks}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#non-linearities"><i class="fa fa-check"></i><b>7.1.1</b> Non-Linearities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#feedforward-neural-network-multi-layer-perceptron"><i class="fa fa-check"></i><b>7.2</b> Feedforward Neural Network / Multi-Layer Perceptron</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>7.2.1</b> Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>7.3</b> Convolutional Neural Networks</a></li>
<li class="chapter" data-level="7.4" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>7.4</b> Autoencoders</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders-for-clustering"><i class="fa fa-check"></i><b>7.4.1</b> Autoencoders for clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>7.5</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="7.6" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>7.6</b> Recurrent neural networks</a></li>
<li class="chapter" data-level="7.7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#long-short-term-memory-networks"><i class="fa fa-check"></i><b>7.7</b> Long short-term memory networks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html"><i class="fa fa-check"></i><b>8</b> Machine learning Automation &amp; Productivity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#holistic-approach"><i class="fa fa-check"></i><b>8.1</b> Holistic approach</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autosklearn"><i class="fa fa-check"></i><b>8.1.1</b> Autosklearn</a></li>
<li class="chapter" data-level="8.1.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#flaml"><i class="fa fa-check"></i><b>8.1.2</b> FLAML</a></li>
<li class="chapter" data-level="8.1.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#h2o.ai"><i class="fa fa-check"></i><b>8.1.3</b> H2O.ai</a></li>
<li class="chapter" data-level="8.1.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#autokeras"><i class="fa fa-check"></i><b>8.1.4</b> AutoKeras</a></li>
<li class="chapter" data-level="8.1.5" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#fastai"><i class="fa fa-check"></i><b>8.1.5</b> Fastai</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#model-selection-automation"><i class="fa fa-check"></i><b>8.2</b> Model selection automation</a></li>
<li class="chapter" data-level="8.3" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#feature-engineering-automation"><i class="fa fa-check"></i><b>8.3</b> Feature engineering automation</a></li>
<li class="chapter" data-level="8.4" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#cookie-cutter-visualizations"><i class="fa fa-check"></i><b>8.4</b> cookie-cutter Visualizations</a></li>
<li class="chapter" data-level="8.5" data-path="machine-learning-automation-productivity.html"><a href="machine-learning-automation-productivity.html#comprehensive-validation-reports"><i class="fa fa-check"></i><b>8.5</b> Comprehensive validation reports</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html"><i class="fa fa-check"></i><b>9</b> Machine Learning Project Management</a>
<ul>
<li class="chapter" data-level="9.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#basis-for-machine-learning-in-companies"><i class="fa fa-check"></i><b>9.1</b> Basis for Machine Learning in Companies</a></li>
<li class="chapter" data-level="9.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-automate-business-processes-with-machine-learning"><i class="fa fa-check"></i><b>9.2</b> How to automate business processes with machine learning</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#the-stages-from-manual-to-ml"><i class="fa fa-check"></i><b>9.2.1</b> The stages from ManuaL to ML</a></li>
<li class="chapter" data-level="9.2.2" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#phases-of-the-ml-project"><i class="fa fa-check"></i><b>9.2.2</b> Phases of the ML project</a></li>
<li class="chapter" data-level="9.2.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#how-to-frame-ml-problems"><i class="fa fa-check"></i><b>9.2.3</b> How to frame ML problems</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machine-learning-project-management.html"><a href="machine-learning-project-management.html#common-pitfalls-in-machine-learning"><i class="fa fa-check"></i><b>9.3</b> Common pitfalls in machine learning</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Reference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-learning" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Unsupervised Learning<a href="unsupervised-learning.html#unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="custering-methods" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Custering Methods<a href="unsupervised-learning.html#custering-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Clustering methods are used to group data when no class labels are
present. You thereby want to learn an intrinsic structure of the data.</p>
<div id="metrics-for-clustering-algorithms" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> metrics for Clustering algorithms<a href="unsupervised-learning.html#metrics-for-clustering-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="silhouette-coefficient" class="section level4 hasAnchor" number="4.1.1.1">
<h4><span class="header-section-number">4.1.1.1</span> Silhouette coefficient<a href="unsupervised-learning.html#silhouette-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The silhouette coefficient compares the average distance of a point and
the points in its own cluster <span class="math inline">\(d(x,\mu_{C})\)</span> to the average distance
between the point and and the points of the second nearest Cluster
<span class="math inline">\(d(x,\mu_{C&#39;})\)</span>.</p>
<p><span class="math display">\[s(x) = \frac{d(x,\mu_{C&#39;})-d(x,\mu_{C})}{\max(d(x,\mu_{C}), d(x,\mu_{C&#39;}))}\]</span>
where <span class="math inline">\(C\)</span> is the own cluster and <span class="math inline">\(C&#39;\)</span> is the second nearest cluster. If
a point is clearly in its own cluster, <span class="math inline">\(s(x)\)</span> is close to <span class="math inline">\(1\)</span>. If a
point is between two clusters, <span class="math inline">\(s(x)\)</span> is close to 0. If a point is
closer to another cluster, <span class="math inline">\(s(x)\)</span> is negative.<br />
By varying the number of clusters, one can find the number with the
highest silhouette coefficients.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>The score is high for dense and highly separated clusters.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>The silhouette coefficient is mainly suitable for convex clusters,
since it gives high values to this kind of clusters.</li>
</ul>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="unsupervised-learning.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb41-2"><a href="unsupervised-learning.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb41-3"><a href="unsupervised-learning.html#cb41-3" aria-hidden="true" tabindex="-1"></a>clu <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb41-4"><a href="unsupervised-learning.html#cb41-4" aria-hidden="true" tabindex="-1"></a>clu.fit(X)</span>
<span id="cb41-5"><a href="unsupervised-learning.html#cb41-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> clu.labels_</span>
<span id="cb41-6"><a href="unsupervised-learning.html#cb41-6" aria-hidden="true" tabindex="-1"></a>silhouette_score(X, labels, metric<span class="op">=</span><span class="st">&#39;manhattan&#39;</span>)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html">scikit-learn.org</a><br />
</p>
<p>A faster alternative is the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html">Davies-Bouldin
score</a>,
where values closer to 0 indicate a better clustering.</p>
</div>
<div id="adjusted-mutual-information-score" class="section level4 hasAnchor" number="4.1.1.2">
<h4><span class="header-section-number">4.1.1.2</span> Adjusted mutual information score<a href="unsupervised-learning.html#adjusted-mutual-information-score" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If you have labelled samples, you can use the <a href="#mutual_info">mutual information
score</a> to test if the classes correspond to your clusters.
The <em>adjusted</em> mutual information score adjusts for chance.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="unsupervised-learning.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_mutual_info_score</span>
<span id="cb42-2"><a href="unsupervised-learning.html#cb42-2" aria-hidden="true" tabindex="-1"></a>adjusted_mutual_info_score(Y, clusters)</span></code></pre></div>
</div>
</div>
<div id="k-means-clustering" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> K-Means Clustering<a href="unsupervised-learning.html#k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Goal: Divide data into K clusters so that the variance within the
clusters is minimized. The objective function:</p>
<p><span class="math display">\[V(D) = \sum_{i=1}^k \sum{x_j \in C_i} (x_j - \mu_i)^2,\]</span> where <span class="math inline">\(V\)</span> is
the variance, <span class="math inline">\(C_i\)</span> is a cluster, <span class="math inline">\(\mu_i\)</span> is a cluster mean, <span class="math inline">\(x_j\)</span> is a
datapoint. The algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Assign the data to k initial clusters.</p></li>
<li><p>Calculate the mean of each cluster.</p></li>
<li><p>Assign the data points to the closest cluster mean.</p></li>
<li><p>If a point changed its cluster, repeat from step 2.</p></li>
</ol>
<div class="figure">
<img src="figures/kMeans.png" id="CDF" style="width:60.0%" alt="" />
<p class="caption">Data from the <em>Iris flower data set</em> clustered into 3 clusters using
k-Means. On the right the data points have been assigned to their actual
species. <em>Figure from <a href="https://commons.wikimedia.org/wiki/File:Autoencoder_schema.png">user Chire on
wikimedia.org</a>.</em></p>
</div>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="unsupervised-learning.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb43-2"><a href="unsupervised-learning.html#cb43-2" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb43-3"><a href="unsupervised-learning.html#cb43-3" aria-hidden="true" tabindex="-1"></a>kmeans.fit(X)</span>
<span id="cb43-4"><a href="unsupervised-learning.html#cb43-4" aria-hidden="true" tabindex="-1"></a>kmeans.predict([[<span class="dv">5</span>, <span class="dv">1</span>]])</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">scikit-learn.org</a><br />
</p>
<p>A faster alternative is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html">mini batch
K-Means</a>.</p>
</div>
<div id="graph-based-clustering" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Graph-Based Clustering<a href="unsupervised-learning.html#graph-based-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You represent data set <span class="math inline">\(D\)</span> as a graph <span class="math inline">\(G=(V,E)\)</span> and divide it up in
connected sub-graphs that represent your clusters. Each edge <span class="math inline">\(e_{ij}\)</span>
(between nodes <span class="math inline">\(v_i\)</span> and <span class="math inline">\(v_j\)</span>) has a weight <span class="math inline">\(w_{ij}\)</span> (which is commonly
a similarity or distance measure).</p>
<div id="basic-graph-based-clustering" class="section level4 hasAnchor" number="4.1.3.1">
<h4><span class="header-section-number">4.1.3.1</span> Basic Graph-Based Clustering<a href="unsupervised-learning.html#basic-graph-based-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The basic algorithm works like this:</p>
<ol style="list-style-type: decimal">
<li><p>Define a weight-threshold <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>For all edges: if <span class="math inline">\(w_{ij}\)</span> &gt; <span class="math inline">\(\theta\)</span>: remove <span class="math inline">\(e_{ij}\)</span>.</p></li>
<li><p>If nodes are connected by a path (found via <em>depth first search</em>):
Assign them to the same cluster.</p></li>
</ol>
</div>
<div id="dbscan" class="section level4 hasAnchor" number="4.1.3.2">
<h4><span class="header-section-number">4.1.3.2</span> DBScan<a href="unsupervised-learning.html#dbscan" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Density-Based Spatial Clustering of Applications with Noise</em> is a more
noise robust version of basic graph-based clustering. You create
clusters based on dense and connected regions. It works like this:</p>
<ol style="list-style-type: decimal">
<li><p>A point is a <em>core point</em> if at least <span class="math inline">\(\text{minPts}\)</span> are within a
radius of <span class="math inline">\(\epsilon\)</span> of the point (including the point itself).</p></li>
<li><p>A point is <em>directly reachable</em> if it is not a <em>core point</em> but
within <span class="math inline">\(\epsilon\)</span> from a <span class="math inline">\(core point\)</span>.</p></li>
<li><p>All other points are not part of the cluster (and may not be part of
any cluster).</p></li>
</ol>
<p><strong>!</strong> For points between clusters, the assignment to a cluster depends
on the order of point assignments.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="unsupervised-learning.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb44-2"><a href="unsupervised-learning.html#cb44-2" aria-hidden="true" tabindex="-1"></a>dbscan <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="dv">3</span>, min_samples<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb44-3"><a href="unsupervised-learning.html#cb44-3" aria-hidden="true" tabindex="-1"></a>dbscan.fit(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">scikit-learn.org</a><br />
</p>
<p>There is a newer version of this algorithm (<a href="https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html">Hierarchical
DBSCAN</a>),
that allows for clusters with varying density, more robustness in
cluster assignment and makes tuning <span class="math inline">\(\epsilon\)</span> unnecessary.</p>
</div>
<div id="cut-based-clustering" class="section level4 hasAnchor" number="4.1.3.3">
<h4><span class="header-section-number">4.1.3.3</span> Cut-Based Clustering<a href="unsupervised-learning.html#cut-based-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>You introduce a <strong>adjacency/similarity</strong> matrix <span class="math inline">\(W\)</span> (measures similarity
between data points) and define the number of clusters <span class="math inline">\(k\)</span>. You now try
to minimize the weight of edges <span class="math inline">\(\kappa\)</span> between the clusters <span class="math inline">\(C\)</span> (equal
to cutting edges between nodes that are least similar):</p>
<p><span class="math display">\[\begin{aligned}
                \begin{split}  
                    \min \frac{1}{2} \sum_{a=1}^k \sum_{b=1}^k \kappa(C_a, C_b) \\
                    \text{where } \kappa(C_a, C_b) = \sum_{v_i \in C_a , v_j \in C_b , a \neq b} W_{ij} \\
                    \text{ and } \kappa(C_a, C_a) = 0
                \end{split}
            \end{aligned}\]</span> <span class="math inline">\(\rightarrow\)</span> You only add up the
similarities/edge-weights between your clusters (but not within your
clusters).<br />
For constructing the similarity matrix, different kernels can be used
(commonly the linear kernel or the Gaussian kernel).</p>
</div>
</div>
<div id="spectral-clustering" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Spectral Clustering<a href="unsupervised-learning.html#spectral-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Spectral clustering works by non-linearly mapping the
matrix-representation of the graph onto a lower-dimensional space based
on its spectrum (set of eigenvectors) and group the points there. The
mapping preserves local distances, i.e. close points stay close to each
other after the mapping. It employs three steps: Preprocessing,
decomposition and grouping.</p>
<p><strong>Preprocessing</strong></p>
<p>We create a Laplacian matrix <span class="math inline">\(L\)</span> (Laplacian operator in matrix form,
measuring how strongly a vertex differs from nearby vertices (because
the edges are similarity measures)): <span class="math display">\[L = D - W\]</span>
<span class="math display">\[D_{ij} = \begin{cases}
        \sum_{j=1}^N W_{ij} \\
        0 \text{ if } i \neq j
    \end{cases}\]</span> where <span class="math inline">\(D\)</span> is the degree matrix (the (weighted) degree
of each node is on the diagonal) and <span class="math inline">\(W\)</span> is the adjacency/similarity
matrix (measures similarity between data points).</p>
<p><strong>Decomposition</strong></p>
<p>You first normalize the Laplacian to avoid big impacts of highly
connected vertices/nodes. More info on the calculation on
<a href="https://en.wikipedia.org/wiki/Laplacian_matrix#Laplacian_matrix_normalization">wikipedia.org</a>.</p>
<p>We make <a href="probability-theory-linear-algebra.html#EV_Dec">eigenvalue decomposition</a>:</p>
<p><span class="math display">\[L U = \Lambda U \quad \rightarrow \quad L = U \Lambda U^{-1} \]</span></p>
<p>where <span class="math inline">\(U\)</span> is the matrix of eigenvectors and <span class="math inline">\(\Lambda\)</span> is the diagonal
matrix of eigenvalues. You can now find a lower-dimensional embedding by
choosing the <span class="math inline">\(k\)</span> smallest non-zero eigenvalues. The final data is now
represented as a matrix of k eigenvectors.</p>
<p><strong>Grouping</strong></p>
<p>You have multiple options:</p>
<ul>
<li><p>You can cut the graph by using the chosen eigenvectors and splitting
at 0 or median value.</p></li>
<li><p>You get the final cluster assignments by normalizing the now
k-dimensional data and applying k-means clustering to it.</p></li>
</ul>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="unsupervised-learning.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> SpectralClustering</span>
<span id="cb45-2"><a href="unsupervised-learning.html#cb45-2" aria-hidden="true" tabindex="-1"></a>scl <span class="op">=</span> SpectralClustering(n_clusters<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb45-3"><a href="unsupervised-learning.html#cb45-3" aria-hidden="true" tabindex="-1"></a>        affinity<span class="op">=</span><span class="st">&#39;rbf&#39;</span>,</span>
<span id="cb45-4"><a href="unsupervised-learning.html#cb45-4" aria-hidden="true" tabindex="-1"></a>        assign_labels<span class="op">=</span><span class="st">&#39;cluster_qr&#39;</span>, <span class="co"># assigns labels directly from Eig vecs,</span></span>
<span id="cb45-5"><a href="unsupervised-learning.html#cb45-5" aria-hidden="true" tabindex="-1"></a>        n<span class="op">-</span>jobs <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb45-6"><a href="unsupervised-learning.html#cb45-6" aria-hidden="true" tabindex="-1"></a>scl.fit(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html">scikit-learn.org</a><br />
</p>
</div>
<div id="SSP" class="section level3 hasAnchor" number="4.1.5">
<h3><span class="header-section-number">4.1.5</span> Sparse Subspace Clustering (SSP)<a href="unsupervised-learning.html#SSP" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The underlying assumption of SSP is that the different clusters reside
in different subspaces of the data. Clusters are therefore perpendicular
to each other and points in a cluster can only be reconstructed by
combinations of points in the same cluster (<span class="math inline">\(\rightarrow\)</span>
self-expressiveness, the reconstruction vectors ought to be sparse). For
each point you try to find other points that can be used to recreate
that point - these then form the same cluster. Doing that for all points
gives you a data matrix <span class="math inline">\(X\)</span> and a matrix of reconstruction vectors <span class="math inline">\(V\)</span>:</p>
<p><span class="math display">\[X = X*V\text{ s.t. diag}(V)=0.\]</span></p>
<p>You now try to minimize the V-matrix according to the L1-norm (giving
you a sparse matrix). This matrix can then be used for e.g. spectral
clustering.</p>
<p>More details in the <a href="https://ieeexplore.ieee.org/document/7780794">original paper on
SSC-OMP</a>.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="unsupervised-learning.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cluster.selfrepresentation <span class="im">import</span> SparseSubspaceClusteringOMP</span>
<span id="cb46-2"><a href="unsupervised-learning.html#cb46-2" aria-hidden="true" tabindex="-1"></a>ssc <span class="op">=</span> SparseSubspaceClusteringOMP(n_clusters<span class="op">=</span><span class="dv">3</span>,affinity<span class="op">=</span><span class="st">&quot;symmetrize&quot;</span>)</span>
<span id="cb46-3"><a href="unsupervised-learning.html#cb46-3" aria-hidden="true" tabindex="-1"></a>ssc.fit(X)</span></code></pre></div>
<p>More info: <a href="https://github.com/ChongYou/subspace-clustering">github.com</a></p>
</div>
<div id="soft-assignment-clustering" class="section level3 hasAnchor" number="4.1.6">
<h3><span class="header-section-number">4.1.6</span> Soft-assignment Clustering<a href="unsupervised-learning.html#soft-assignment-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Soft clustering assigns to each point the probabilities of belonging to
each of the clusters instead of assigning it to only one cluster. This
gives you a measure on how certain the algorithm is about the clustering
of a point.</p>
<div id="gaussian-mixture-models" class="section level4 hasAnchor" number="4.1.6.1">
<h4><span class="header-section-number">4.1.6.1</span> Gaussian Mixture Models<a href="unsupervised-learning.html#gaussian-mixture-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Gaussian mixture models try to find an ensemble of gaussian
distributions that best describe your data. These
distributions/components are used as your clusters. Your points belong
to each cluster with a certain probability. To find these distributions,
we use an <em>expectation maximization</em> algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Assume the centers of your Gaussians (e.g. by k-means) and calculate
for each point the probability of being generated by each
distribution (<span class="math inline">\(p(x_i \in C_k | \phi_i, \mu_k, \sigma_k)\)</span>).</p></li>
<li><p>Change the parameters to maximize the likelihood of the data, given
the cluster probabilities for all points.</p></li>
</ol>
<p>The probability of a data point belonging to a cluster can be calculated
via Bayes theorem.</p>
<p>More info on the theory:
<a href="https://brilliant.org/wiki/gaussian-mixture-model/">brilliant.org</a>.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="unsupervised-learning.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb47-2"><a href="unsupervised-learning.html#cb47-2" aria-hidden="true" tabindex="-1"></a>gm <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span><span class="dv">4</span>, covariance_type<span class="op">=</span><span class="st">&#39;full&#39;</span>)</span>
<span id="cb47-3"><a href="unsupervised-learning.html#cb47-3" aria-hidden="true" tabindex="-1"></a>gm.fit(X)</span>
<span id="cb47-4"><a href="unsupervised-learning.html#cb47-4" aria-hidden="true" tabindex="-1"></a>gm.predict_proba(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html">scikit-learn.org</a></p>
</div>
<div id="other-models" class="section level4 hasAnchor" number="4.1.6.2">
<h4><span class="header-section-number">4.1.6.2</span> Other models<a href="unsupervised-learning.html#other-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Other models also have means to calculate cluster probabilities for
points. For the HDBSCAN-algorithm see
<a href="https://hdbscan.readthedocs.io/en/latest/soft_clustering.html">here</a>.</p>
</div>
<div id="hierarchical-clustering" class="section level4 hasAnchor" number="4.1.6.3">
<h4><span class="header-section-number">4.1.6.3</span> Hierarchical Clustering<a href="unsupervised-learning.html#hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Instead of clustering a point to only one cluster, you assign it to a hierarchy.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Hierarchies of clusters reflects the data set and therefore the relationship between points better.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>It is more difficult to make clear statements of cluster membership <span class="math inline">\(\rightarrow\)</span> define a limit for hierarchical depth</li>
</ul>
</div>
</div>
<div id="artificial-neural-networks-for-clustering" class="section level3 hasAnchor" number="4.1.7">
<h3><span class="header-section-number">4.1.7</span> Artificial Neural Networks for Clustering<a href="unsupervised-learning.html#artificial-neural-networks-for-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>See chapter <em>Neural Networks</em>
(<a href="#Neural%20Networks" reference-type="ref" reference="Neural Networks">5</a>)</p>
</div>
</div>
<div id="mapping-to-lower-dimensions" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Mapping to lower dimensions<a href="unsupervised-learning.html#mapping-to-lower-dimensions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><font color="grey"></p>
<div id="manifold-learning" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Manifold learning<a href="unsupervised-learning.html#manifold-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="isomap" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Isomap<a href="unsupervised-learning.html#isomap" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="local-linear-embedding" class="section level4 hasAnchor" number="4.2.1.2">
<h4><span class="header-section-number">4.2.1.2</span> Local linear embedding<a href="unsupervised-learning.html#local-linear-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="multi-dimensional-scaling" class="section level4 hasAnchor" number="4.2.1.3">
<h4><span class="header-section-number">4.2.1.3</span> Multi dimensional scaling<a href="unsupervised-learning.html#multi-dimensional-scaling" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p></font></p>
</div>
</div>
<div id="decomposition-techniques" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Decomposition techniques<a href="unsupervised-learning.html#decomposition-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="SVD2" class="section level4 hasAnchor" number="4.2.2.1">
<h4><span class="header-section-number">4.2.2.1</span> Singular value decomposition<a href="unsupervised-learning.html#SVD2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Singular value decomposition is used to compress large matrices of your
data into smaller ones, with much less data, but without loosing a lot
of information. Please visit the <a href="probability-theory-linear-algebra.html#SVD1">mathematical explanation</a> for
the underlying mechanisms.<br />
</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="unsupervised-learning.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb48-2"><a href="unsupervised-learning.html#cb48-2" aria-hidden="true" tabindex="-1"></a>svd <span class="op">=</span> TruncatedSVD(n_components<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb48-3"><a href="unsupervised-learning.html#cb48-3" aria-hidden="true" tabindex="-1"></a>svd.fit_transform(X_train)</span>
<span id="cb48-4"><a href="unsupervised-learning.html#cb48-4" aria-hidden="true" tabindex="-1"></a>svd.transform(X_test)</span></code></pre></div>
<p><font color="grey"></p>
</div>
<div id="principle-component-analysis-pca" class="section level4 hasAnchor" number="4.2.2.2">
<h4><span class="header-section-number">4.2.2.2</span> Principle Component analysis (PCA)<a href="unsupervised-learning.html#principle-component-analysis-pca" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<!-- use this for the explanation https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca?rq=1 -->
<!--# TODO: Make subchapter: Kernel PCA -->
</div>
</div>
</div>
<div id="outlier-detection" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Outlier detection<a href="unsupervised-learning.html#outlier-detection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="local-outlier-factor" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Local outlier factor<a href="unsupervised-learning.html#local-outlier-factor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="isolation-forest" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Isolation forest<a href="unsupervised-learning.html#isolation-forest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><font color="grey"></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generative-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/MoritzGuck/Machine-Learning-Reference/edit/master/04-unsupervised.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
