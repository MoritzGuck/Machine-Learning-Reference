<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Data Basics | Moritz’ Machine Learning Summary</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Data Basics | Moritz’ Machine Learning Summary" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Data Basics | Moritz’ Machine Learning Summary" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  

<meta name="author" content="Moritz Gück" />


<meta name="date" content="2023-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability-theory-linear-algebra.html"/>
<link rel="next" href="classification-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html"><i class="fa fa-check"></i><b>1</b> Probability Theory &amp; Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-theory"><i class="fa fa-check"></i><b>1.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-basics"><i class="fa fa-check"></i><b>1.1.1</b> Probability Basics</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#probability-distributions"><i class="fa fa-check"></i><b>1.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#CLT"><i class="fa fa-check"></i><b>1.1.3</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#bayesian-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian probability</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#further-concepts"><i class="fa fa-check"></i><b>1.1.5</b> Further Concepts</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-theory-linear-algebra.html"><a href="probability-theory-linear-algebra.html#vectors"><i class="fa fa-check"></i><b>1.2.1</b> Vectors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-basics.html"><a href="data-basics.html"><i class="fa fa-check"></i><b>2</b> Data Basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-basics.html"><a href="data-basics.html#similarity-and-distance-measures"><i class="fa fa-check"></i><b>2.1</b> Similarity and Distance Measures</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-basics.html"><a href="data-basics.html#metrics"><i class="fa fa-check"></i><b>2.1.1</b> Metrics</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-basics.html"><a href="data-basics.html#similarity-measures-on-vectors"><i class="fa fa-check"></i><b>2.1.2</b> Similarity measures on vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-basics.html"><a href="data-basics.html#preprocessing-data"><i class="fa fa-check"></i><b>2.2</b> Preprocessing data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-basics.html"><a href="data-basics.html#standardization"><i class="fa fa-check"></i><b>2.2.1</b> Standardization</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-basics.html"><a href="data-basics.html#encoding-categorical-features"><i class="fa fa-check"></i><b>2.2.2</b> Encoding categorical features</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-basics.html"><a href="data-basics.html#imputing-missing-values"><i class="fa fa-check"></i><b>2.2.3</b> Imputing missing values</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-basics.html"><a href="data-basics.html#splitting-in-training--and-test-data"><i class="fa fa-check"></i><b>2.3</b> Splitting in training- and test-data</a></li>
<li class="chapter" data-level="2.4" data-path="data-basics.html"><a href="data-basics.html#feature-selection"><i class="fa fa-check"></i><b>2.4</b> Feature selection</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-basics.html"><a href="data-basics.html#a-priori-feature-selection"><i class="fa fa-check"></i><b>2.4.1</b> A priori feature selection</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-basics.html"><a href="data-basics.html#wrapper-methods"><i class="fa fa-check"></i><b>2.4.2</b> wrapper methods</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-basics.html"><a href="data-basics.html#advice-pitfalls"><i class="fa fa-check"></i><b>2.4.3</b> Advice &amp; Pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-basics.html"><a href="data-basics.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>2.5</b> Hyper-parameter tuning</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-basics.html"><a href="data-basics.html#grid-search"><i class="fa fa-check"></i><b>2.5.1</b> Grid search</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-basics.html"><a href="data-basics.html#randomized-search"><i class="fa fa-check"></i><b>2.5.2</b> randomized search</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="data-basics.html"><a href="data-basics.html#model-selection"><i class="fa fa-check"></i><b>2.6</b> Model selection</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="data-basics.html"><a href="data-basics.html#crossval"><i class="fa fa-check"></i><b>2.6.1</b> Cross Validation</a></li>
<li class="chapter" data-level="2.6.2" data-path="data-basics.html"><a href="data-basics.html#bootstrapping"><i class="fa fa-check"></i><b>2.6.2</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="data-basics.html"><a href="data-basics.html#errors-in-machine-learning"><i class="fa fa-check"></i><b>2.7</b> Errors in machine learning</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="data-basics.html"><a href="data-basics.html#bias-and-variance"><i class="fa fa-check"></i><b>2.7.1</b> Bias and Variance</a></li>
<li class="chapter" data-level="2.7.2" data-path="data-basics.html"><a href="data-basics.html#regularization"><i class="fa fa-check"></i><b>2.7.2</b> Regularization</a></li>
<li class="chapter" data-level="2.7.3" data-path="data-basics.html"><a href="data-basics.html#bagging"><i class="fa fa-check"></i><b>2.7.3</b> Bagging</a></li>
<li class="chapter" data-level="2.7.4" data-path="data-basics.html"><a href="data-basics.html#boosting"><i class="fa fa-check"></i><b>2.7.4</b> Boosting</a></li>
<li class="chapter" data-level="2.7.5" data-path="data-basics.html"><a href="data-basics.html#stacking"><i class="fa fa-check"></i><b>2.7.5</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="data-basics.html"><a href="data-basics.html#tips-for-machine-learning-projects"><i class="fa fa-check"></i><b>2.8</b> Tips for machine learning projects</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="data-basics.html"><a href="data-basics.html#general-advice"><i class="fa fa-check"></i><b>2.8.1</b> General advice</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="data-basics.html"><a href="data-basics.html#common-mistakes"><i class="fa fa-check"></i><b>2.9</b> Common mistakes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-methods.html"><a href="classification-methods.html"><i class="fa fa-check"></i><b>3</b> Classification Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-methods.html"><a href="classification-methods.html#evaluation-of-classifiers"><i class="fa fa-check"></i><b>3.1</b> Evaluation of Classifiers</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-methods.html"><a href="classification-methods.html#confusion-matrix"><i class="fa fa-check"></i><b>3.1.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-methods.html"><a href="classification-methods.html#basic-quality-measures"><i class="fa fa-check"></i><b>3.1.2</b> Basic Quality Measures</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-methods.html"><a href="classification-methods.html#area-under-the-curve"><i class="fa fa-check"></i><b>3.1.3</b> Area under the Curve</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-methods.html"><a href="classification-methods.html#handling-unbalanced-data"><i class="fa fa-check"></i><b>3.1.4</b> Handling Unbalanced Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-methods.html"><a href="classification-methods.html#classification-algorithms"><i class="fa fa-check"></i><b>3.2</b> Classification Algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-methods.html"><a href="classification-methods.html#nearest-neighbors-classifier"><i class="fa fa-check"></i><b>3.2.1</b> Nearest Neighbors Classifier</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-methods.html"><a href="classification-methods.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>3.2.2</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-methods.html"><a href="classification-methods.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>3.2.3</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-methods.html"><a href="classification-methods.html#support-vector-classifier-svc"><i class="fa fa-check"></i><b>3.2.4</b> Support Vector Classifier (SVC)</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-methods.html"><a href="classification-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.2.5</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#custering-methods"><i class="fa fa-check"></i><b>4.1</b> Custering Methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#metrics-for-clustering-algorithms"><i class="fa fa-check"></i><b>4.1.1</b> metrics for Clustering algorithms</a></li>
<li class="chapter" data-level="4.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#graph-based-clustering"><i class="fa fa-check"></i><b>4.1.3</b> Graph-Based Clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#spectral-clustering"><i class="fa fa-check"></i><b>4.1.4</b> Spectral Clustering</a></li>
<li class="chapter" data-level="4.1.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#SSP"><i class="fa fa-check"></i><b>4.1.5</b> Sparse Subspace Clustering (SSP)</a></li>
<li class="chapter" data-level="4.1.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#soft-assignment-clustering"><i class="fa fa-check"></i><b>4.1.6</b> Soft-assignment Clustering</a></li>
<li class="chapter" data-level="4.1.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#artificial-neural-networks-for-clustering"><i class="fa fa-check"></i><b>4.1.7</b> Artificial Neural Networks for Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#mapping-to-lower-dimensions"><i class="fa fa-check"></i><b>4.2</b> Mapping to lower dimensions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#manifold-learning"><i class="fa fa-check"></i><b>4.2.1</b> Manifold learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#decomposition-techniques"><i class="fa fa-check"></i><b>4.2.2</b> Decomposition techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#outlier-detection"><i class="fa fa-check"></i><b>4.3</b> Outlier detection</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#local-outlier-factor"><i class="fa fa-check"></i><b>4.3.1</b> Local outlier factor</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#isolation-forest"><i class="fa fa-check"></i><b>4.3.2</b> Isolation forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generative-models.html"><a href="generative-models.html"><i class="fa fa-check"></i><b>5</b> Generative models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="generative-models.html"><a href="generative-models.html#generative-models-for-discrete-data"><i class="fa fa-check"></i><b>5.1</b> Generative Models for Discrete Data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="generative-models.html"><a href="generative-models.html#bayesian-concept-learning"><i class="fa fa-check"></i><b>5.1.1</b> Bayesian Concept Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="generative-models.html"><a href="generative-models.html#beta-binomial-model"><i class="fa fa-check"></i><b>5.1.2</b> Beta-binomial model</a></li>
<li class="chapter" data-level="5.1.3" data-path="generative-models.html"><a href="generative-models.html#dirichlet-multinomial-model"><i class="fa fa-check"></i><b>5.1.3</b> Dirichlet-multinomial model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#evaluation-of-regression-models"><i class="fa fa-check"></i><b>6.1</b> Evaluation of regression models</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#r2-score"><i class="fa fa-check"></i><b>6.1.1</b> R^2 score</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#mean-squared-error"><i class="fa fa-check"></i><b>6.1.2</b> Mean squared error</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#visual-tools"><i class="fa fa-check"></i><b>6.1.3</b> Visual tools</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#linear-models"><i class="fa fa-check"></i><b>6.2</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>6.2.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#lasso-regression"><i class="fa fa-check"></i><b>6.2.2</b> Lasso regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="regression.html"><a href="regression.html#ridge-regression"><i class="fa fa-check"></i><b>6.2.3</b> Ridge regression</a></li>
<li class="chapter" data-level="6.2.4" data-path="regression.html"><a href="regression.html#bayesian-regression"><i class="fa fa-check"></i><b>6.2.4</b> Bayesian regression</a></li>
<li class="chapter" data-level="6.2.5" data-path="regression.html"><a href="regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>6.2.5</b> Generalized linear models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.3</b> Gaussian process regression</a></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#gradient-boosted-tree-regression"><i class="fa fa-check"></i><b>6.4</b> Gradient boosted tree regression</a></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#time-series-forecasting"><i class="fa fa-check"></i><b>6.5</b> Time Series Forecasting</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#arimax-model"><i class="fa fa-check"></i><b>6.5.1</b> ARIMA(X) Model</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#varmmax-model"><i class="fa fa-check"></i><b>6.5.2</b> VARMMA(X) Model</a></li>
<li class="chapter" data-level="6.5.3" data-path="regression.html"><a href="regression.html#prophet-model"><i class="fa fa-check"></i><b>6.5.3</b> Prophet-Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Networks {#Neural Networks}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#non-linearities"><i class="fa fa-check"></i><b>7.1.1</b> Non-Linearities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#feedforward-neural-network-multi-layer-perceptron"><i class="fa fa-check"></i><b>7.2</b> Feedforward Neural Network / Multi-Layer Perceptron</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>7.2.1</b> Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>7.3</b> Convolutional Neural Networks</a></li>
<li class="chapter" data-level="7.4" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>7.4</b> Autoencoders</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders-for-clustering"><i class="fa fa-check"></i><b>7.4.1</b> Autoencoders for clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>7.5</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="7.6" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>7.6</b> Recurrent neural networks</a></li>
<li class="chapter" data-level="7.7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#long-short-term-memory-networks"><i class="fa fa-check"></i><b>7.7</b> Long short-term memory networks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html"><i class="fa fa-check"></i><b>8</b> Explanation and inspection methods</a>
<ul>
<li class="chapter" data-level="8.1" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html#global-explainability-methods"><i class="fa fa-check"></i><b>8.1</b> Global explainability methods</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html#inherently-explainable-algorithms"><i class="fa fa-check"></i><b>8.1.1</b> Inherently explainable algorithms</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html#local-explainability-methods"><i class="fa fa-check"></i><b>8.2</b> Local explainability methods</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html#shapely-method"><i class="fa fa-check"></i><b>8.2.1</b> Shapely method</a></li>
<li class="chapter" data-level="8.2.2" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html#lime"><i class="fa fa-check"></i><b>8.2.2</b> LIME</a></li>
<li class="chapter" data-level="8.2.3" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html#anchor-methods"><i class="fa fa-check"></i><b>8.2.3</b> Anchor methods</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Moritz’ Machine Learning Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-basics" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Data Basics<a href="data-basics.html#data-basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="similarity-and-distance-measures" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Similarity and Distance Measures<a href="data-basics.html#similarity-and-distance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Choosing the right distance measures is important for achieving good
results in statistics, predictions and clusterings.</p>
<div id="metrics" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Metrics<a href="data-basics.html#metrics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a distance measure to be called a metric <span class="math inline">\(d\)</span>, the following criteria
need to be fulfilled:</p>
<ul>
<li><p>Positivity: <span class="math inline">\(d(x_1,x_2)≥0\)</span></p></li>
<li><p><span class="math inline">\(d(x_1,x_2)=0 \text{ if and only if } x_1 = x_2\)</span></p></li>
<li><p>Symmetry: <span class="math inline">\(d(x_1, x_2) = d(x_2, x_1)\)</span></p></li>
<li><p>Triangle inequality: <span class="math inline">\(d(x_1, x_3) ≤ d(x_1, x_2) + d(x_2, x_3)\)</span></p></li>
</ul>
<p>There may be distance measures that do not fulfill these criteria, but
those are not metrics.</p>
</div>
<div id="similarity-measures-on-vectors" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Similarity measures on vectors<a href="data-basics.html#similarity-measures-on-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>These measures are used in many objective functions to compare data
points.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="data-basics.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> pairwise_distances</span>
<span id="cb1-2"><a href="data-basics.html#cb1-2" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> np.array([[<span class="dv">2</span>,<span class="dv">3</span>]])</span>
<span id="cb1-3"><a href="data-basics.html#cb1-3" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> np.array([[<span class="dv">2</span>,<span class="dv">4</span>]])</span>
<span id="cb1-4"><a href="data-basics.html#cb1-4" aria-hidden="true" tabindex="-1"></a>pairwise_distances(X1,X2, metric<span class="op">=</span><span class="st">&quot;manhattan&quot;</span>)</span></code></pre></div>
<p>The available metrics in sklearn are: ‘cityblock’, ‘cosine’,
‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’, and from scipy: ‘braycurtis’,
‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’,
‘kulsinski’, ‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’<br />
More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html">scikit-learn.org</a></p>
<div id="manhattan-distance" class="section level4 hasAnchor" number="2.1.2.1">
<h4><span class="header-section-number">2.1.2.1</span> Manhattan distance<a href="data-basics.html#manhattan-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The distance is the sum of the absolute differences of the components
(single coordinates) of the two points:
<span class="math display">\[d(A, B) = \sum_{i=1}^d | A_i - B_i |\]</span></p>
<p>More info at
<a href="https://en.wikipedia.org/wiki/Taxicab_geometry">wikipedia.org</a>.</p>
</div>
<div id="hamming-distance" class="section level4 hasAnchor" number="2.1.2.2">
<h4><span class="header-section-number">2.1.2.2</span> Hamming distance<a href="data-basics.html#hamming-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This metric is used for pairs of strings and works equivalently to the
Manhattan distance. It is the number of positions that are different
between the strings.<br />
More info at
<a href="https://en.wikipedia.org/wiki/Hamming_distance">wikipedia.org</a>.</p>
</div>
<div id="euclidian-distance" class="section level4 hasAnchor" number="2.1.2.3">
<h4><span class="header-section-number">2.1.2.3</span> Euclidian distance<a href="data-basics.html#euclidian-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[d(A, B) = | A - B | = \sqrt{\sum_{i=1}^d (A_i-B_i)^2} \]</span></p>
<p>More info on the euclidian distance on
<a href="https://en.wikipedia.org/wiki/Euclidean_distance">wikipedia.org</a>.<br />
The usefulness of this metric can deteriorate in high dimensional
spaces. See <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_function">curse of
dimensionality</a></p>
</div>
<div id="chebyshev-distance" class="section level4 hasAnchor" number="2.1.2.4">
<h4><span class="header-section-number">2.1.2.4</span> Chebyshev distance<a href="data-basics.html#chebyshev-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Chebyshev distance is the largest difference along any of the
components of the two vectors.</p>
<p><span class="math display">\[d(A, B) = \max_i(|A_i-B_i|) \]</span></p>
<p>More info at
<a href="https://en.wikipedia.org/wiki/Chebyshev_distance">wikipedia.org</a>.</p>
</div>
<div id="minkowski-distance" class="section level4 hasAnchor" number="2.1.2.5">
<h4><span class="header-section-number">2.1.2.5</span> Minkowski Distance<a href="data-basics.html#minkowski-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[d(A, B) = (\sum_{i=1}^d |A_i-B_i|^p)^\frac{1}{p} \]</span></p>
<p>For <span class="math inline">\(p=2\)</span> the Minkowski distance is equal to the Euclidian distance, for
<span class="math inline">\(p=1\)</span> it corresponds to the Manhattan distance and it converges to the
Chebyshev distance for <span class="math inline">\(p \to \infty\)</span>.   More info at
<a href="https://en.wikipedia.org/wiki/Minkowski_distance">wikipedia.org</a>.</p>
<!--# ### Similarity measures on sets of objects

#### Jaccard coefficient

#### Jaccard distance

#### Overlap coefficient

#### Sorensen-Dice coefficient

### Similarity measures on sets of vectors

#### Single link distance function

#### Complete link distance function

#### Average link distance function

### Similarity measures on sets of strings

#### k-mer based similarity measures

### Similartiy measures for nodes in a Graph

#### Shortest path distance

### Similarity measures on Graphs

#### Wiener index

#### Weisfeiler Lehmann Kernel

### Similarty measures for time series

#### Dynamic Time Warping (DTW) distance -->
</div>
</div>
</div>
<div id="preprocessing-data" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Preprocessing data<a href="data-basics.html#preprocessing-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="standardization" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Standardization<a href="data-basics.html#standardization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Many machine learning models assume that the features are centered
around 0 and that all have a similar variance. Therefore the data has to
be centered and scaled to unit variance before training and prediction.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="data-basics.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb2-2"><a href="data-basics.html#cb2-2" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb2-3"><a href="data-basics.html#cb2-3" aria-hidden="true" tabindex="-1"></a>scaler.fit(input_df)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">scikit-learn.org</a></p>
<p>Another option for scaling is normalization. This is used, when the
values have to fall strictly between a max and min value.<br />
More info:
<a href="https://scikit-learn.org/stable/modules/preprocessing.html#normalization">scikit-learn.org</a></p>
</div>
<div id="encoding-categorical-features" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Encoding categorical features<a href="data-basics.html#encoding-categorical-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The string values (e.g. “male”, “female”) of features have to be
converted into integers. This can be done by two methods:</p>
<div id="ordinal-encoding" class="section level4 hasAnchor" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Ordinal Encoding<a href="data-basics.html#ordinal-encoding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An integer is assigned to each category (e.g. “male”=0, “female”=1)</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="data-basics.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OrdinalEncoder</span>
<span id="cb3-2"><a href="data-basics.html#cb3-2" aria-hidden="true" tabindex="-1"></a>ord_enc <span class="op">=</span> preprocessing.OrdinalEncoder()</span>
<span id="cb3-3"><a href="data-basics.html#cb3-3" aria-hidden="true" tabindex="-1"></a>ord_enc.fit(X)</span>
<span id="cb3-4"><a href="data-basics.html#cb3-4" aria-hidden="true" tabindex="-1"></a>ord_enc.transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder">scikit-learn.org</a><br />
This method is useful when the categories have an ordered relationship
(e.g. “bad”, “medium”, “good”). If this is not the case (e.g. “dog”,
“cat”, “bunny”) this is to be avoided since the algorithm might deduct
an ordered relationship where there is none. For these cases
one-hot-encoding is to be used.</p>
</div>
<div id="one-hot-encoding" class="section level4 hasAnchor" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> One-Hot Encoding<a href="data-basics.html#one-hot-encoding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One-hot encoding assigns a separate feature-column for each category and
encodes it binarily (e.g. if the sample is a dog, it has 1 in the
dog-column and 0 in the cat and bunny column).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="data-basics.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb4-2"><a href="data-basics.html#cb4-2" aria-hidden="true" tabindex="-1"></a>onehot_enc <span class="op">=</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&#39;ignore&#39;</span>)</span>
<span id="cb4-3"><a href="data-basics.html#cb4-3" aria-hidden="true" tabindex="-1"></a>onehot_enc.fit(X)</span>
<span id="cb4-4"><a href="data-basics.html#cb4-4" aria-hidden="true" tabindex="-1"></a>onehot_enc.transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">scikit-learn.org</a><br />
</p>
</div>
</div>
<div id="imputing-missing-values" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Imputing missing values<a href="data-basics.html#imputing-missing-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some algorithms assume that all features of all samples have numerical
values. In these cases missing values have to be imputed (i.e. inferred)
or (if affordable) the samples with missing feature values can be
deleted from the data set.</p>
<div id="iterative-imputor-by-sklearn" class="section level4 hasAnchor" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> Iterative imputor by sklearn<a href="data-basics.html#iterative-imputor-by-sklearn" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For features with missing values, this imputor imputes the missing
values by modelling each feature using the existing values from the
other features. It uses several iterations until the results converge.<br />
<strong>!</strong> This method scales with <span class="math inline">\(O(nd^3)\)</span>, where <span class="math inline">\(n\)</span> is the number of
samples and <span class="math inline">\(d\)</span> is the number of features.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="data-basics.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.experimental <span class="im">import</span> enable_iterative_imputer <span class="co"># necessary since the imputor is still experimental</span></span>
<span id="cb5-2"><a href="data-basics.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> IterativeImputer</span>
<span id="cb5-3"><a href="data-basics.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor </span>
<span id="cb5-4"><a href="data-basics.html#cb5-4" aria-hidden="true" tabindex="-1"></a>rf_estimator <span class="op">=</span> RamdomForestRegressor(n_estimators <span class="op">=</span> <span class="dv">8</span>, max_depth <span class="op">=</span> <span class="dv">6</span>, bootstrap <span class="op">=</span> true)</span>
<span id="cb5-5"><a href="data-basics.html#cb5-5" aria-hidden="true" tabindex="-1"></a>imputor <span class="op">=</span> IterativeImputer(random_state<span class="op">=</span><span class="dv">0</span>, estimator <span class="op">=</span> rf_estimator, max_iter <span class="op">=</span> <span class="dv">25</span>)</span>
<span id="cb5-6"><a href="data-basics.html#cb5-6" aria-hidden="true" tabindex="-1"></a>imputor.fit_transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html">scikit-learn.org</a><br />
</p>
</div>
</div>
</div>
<div id="splitting-in-training--and-test-data" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Splitting in training- and test-data<a href="data-basics.html#splitting-in-training--and-test-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You need to split your training set into test- and training-samples. The
algorithm uses the training samples with the known label/target value
for fitting the parameters. The test-set is used to determine if the
trained algorithm performs well on new samples as well. You need to give
special considerations to the following points:</p>
<ul>
<li><p>Avoiding data or other information to leak from the training set to
the test-set</p></li>
<li><p>Validating if the predictive performance deteriorates over time
(i.e. the algorithm will perform worse on new samples). This is
especially important for models that make predictions for future
events.</p></li>
<li><p>Conversely, sampling the test- and training-sets randomly to avoid
introducing bias in the two sets.</p></li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="data-basics.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># assuming you already imported the data and separated the label column:</span></span>
<span id="cb6-2"><a href="data-basics.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-3"><a href="data-basics.html#cb6-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.33</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">scikit-learn.org</a></p>
</div>
<div id="feature-selection" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Feature selection<a href="data-basics.html#feature-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Usually the label does not depend on all available features. To detect
causal features, remove noisy ones and reduce the running and training
costs of the algorithm, we reduce the amount of features to the relevant
ones. This can be done a priori (before training) or using wrapper
methods (integrated with the prediction algorithm to be used).<br />
<strong>!</strong> There are methods that have feature selection already built-in,
such as decision trees.</p>
<div id="a-priori-feature-selection" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> A priori feature selection<a href="data-basics.html#a-priori-feature-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="low-variance-threshold" class="section level4 hasAnchor" number="2.4.1.1">
<h4><span class="header-section-number">2.4.1.1</span> Low variance threshold<a href="data-basics.html#low-variance-threshold" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A cheap method is to remove all features with variance below a
threshold.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="data-basics.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb7-2"><a href="data-basics.html#cb7-2" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> VarianceThreshold(threshold<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb7-3"><a href="data-basics.html#cb7-3" aria-hidden="true" tabindex="-1"></a>selector.fit_transform(X)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold">scikit-learn.org</a></p>
</div>
<div id="mutual_info" class="section level4 hasAnchor" number="2.4.1.2">
<h4><span class="header-section-number">2.4.1.2</span> Mutual information<a href="data-basics.html#mutual_info" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This method works by choosing the features that have the highest
dependency between the features and the label.</p>
<p><span class="math display">\[ I(X, Y) =D_{KL} \left( P(X=x, Y=y), P(X=x) \otimes P(Y=y) \right) =\sum_{y \in Y} \sum_{x \in X}
    { P(X=x, Y=y) \log\left(\frac{P(X=x, Y=y)}{P(X=x)P(Y=y)}\right) }\]</span></p>
<p>where, <span class="math inline">\(D_{KL}\)</span> is the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler
divergence</a>
(A measure of similarity between distributions). The <span class="math inline">\(\log\)</span>-Term is for
quantifying how different the joint distribution is from the product of
the marginal distributions.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="data-basics.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SelectKBest</span>
<span id="cb8-2"><a href="data-basics.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> mutual_info_classif <span class="co"># for regression use mutual_info_regression</span></span>
<span id="cb8-3"><a href="data-basics.html#cb8-3" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> SelectKBest(mutual_info_classif, k<span class="op">=</span><span class="dv">8</span>).fit_transform(X, y)</span></code></pre></div>
<p>More [<a href="info:\\" class="uri">info:\\</a>](<a href="info:\" class="uri">info:\</a>){.uri}
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">scikit-learn.org</a><br />
<a href="https://en.wikipedia.org/wiki/Mutual_information">wikipedia.org/wiki/Mutual_information</a></p>
</div>
</div>
<div id="wrapper-methods" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> wrapper methods<a href="data-basics.html#wrapper-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="greedy-feature-selection" class="section level4 hasAnchor" number="2.4.2.1">
<h4><span class="header-section-number">2.4.2.1</span> Greedy feature selection<a href="data-basics.html#greedy-feature-selection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Using greedy feature selection as a wrapper method, one commonly starts
with 0 features and adds the feature that returns the highest score with
the used classifier.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="data-basics.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SequentialFeatureSelector</span>
<span id="cb9-2"><a href="data-basics.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb9-3"><a href="data-basics.html#cb9-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb9-4"><a href="data-basics.html#cb9-4" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> SequentialFeatureSelector(classifier, n_features_to_select<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb9-5"><a href="data-basics.html#cb9-5" aria-hidden="true" tabindex="-1"></a>selector.fit_transform(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html">scikit-learn.org</a></p>
</div>
</div>
<div id="advice-pitfalls" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Advice &amp; Pitfalls<a href="data-basics.html#advice-pitfalls" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Selected advice from paper from <a href="https://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf">Guyon and
Elisseeff</a>:</p>
<ul>
<li><p>If you have domain knowledge: Use it.</p></li>
<li><p>Are your features commensurate (same proportion): Normalize them.</p></li>
<li><p>Do you suspect interdependent features: Construct conjunctive
features or products of features.</p></li>
</ul>
<p>Other advice:</p>
<ul>
<li><p>Features that are useless on their own, can be useful in combination
with other features.</p></li>
<li><p>Using multiple redundant variables can be useful to reduce noise.</p></li>
<li><p>There are also models (e.g. lasso regression, decision trees) that
have feature selection built into the model (i.e. by only allowing
for a certain number of features to be used or penalizing the use of
additional features).</p></li>
</ul>
</div>
</div>
<div id="hyper-parameter-tuning" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Hyper-parameter tuning<a href="data-basics.html#hyper-parameter-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The hyper-parameters (e.g. kernel, gamma, number of nodes in tree) are
not trained by algorithm itself. An outer loop of hyper-parameter tuning
is needed to find the optimal hyper parameters.<br />
<strong>!</strong> It is strongly recommended to separate another validation set from
the training set for hyper-parameter tuning (you’ll end up with
training-, validation- and test-set). See <a href="data-basics.html#crossval">Cross Validation</a>
for best practice.</p>
<div id="grid-search" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Grid search<a href="data-basics.html#grid-search" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The classic approach is exhaustive grid search: You create a grid of
hyperparameters and iterate over all combinations. The combination with
the best score is used in the end. This approach causes big
computational costs due to the combinatorial explosion.</p>
</div>
<div id="randomized-search" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> randomized search<a href="data-basics.html#randomized-search" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This approach is used, if there are too many combinations of
hyper-parameters for tuning. You allocate a budget of iterations and the
combinations of parameters are sampled randomly according to the
distributions you provide.</p>
<p>If you want to evaluate on a large set of hyperparameters, you can use a
halving strategy: You tune a large combination of parameters on few
resources (e.g. samples, trees). The best performing half of candidates
is re-evaluated on twice as many resources. This continues until the
best-performing candidate is evaluated on the full amount of resources.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="data-basics.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb10-2"><a href="data-basics.html#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.experimental <span class="im">import</span> enable_halving_search_cv  <span class="co"># since this method is still experimental</span></span>
<span id="cb10-3"><a href="data-basics.html#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> HalvingRandomSearchCV</span>
<span id="cb10-4"><a href="data-basics.html#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils.fixes <span class="im">import</span> loguniform</span>
<span id="cb10-5"><a href="data-basics.html#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="data-basics.html#cb10-6" aria-hidden="true" tabindex="-1"></a>rf_clf <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb10-7"><a href="data-basics.html#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="data-basics.html#cb10-8" aria-hidden="true" tabindex="-1"></a>param_distributions <span class="op">=</span> {<span class="st">&quot;max_depth&quot;</span>: [<span class="dv">3</span>, <span class="va">None</span>],</span>
<span id="cb10-9"><a href="data-basics.html#cb10-9" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;min_samples_split&quot;</span>: loguniform(<span class="dv">1</span>, <span class="dv">10</span>)}</span>
<span id="cb10-10"><a href="data-basics.html#cb10-10" aria-hidden="true" tabindex="-1"></a>hypa_search <span class="op">=</span> HalvingRandomSearchCV(rf_clf, param_distributions,</span>
<span id="cb10-11"><a href="data-basics.html#cb10-11" aria-hidden="true" tabindex="-1"></a>                               resource<span class="op">=</span><span class="st">&#39;n_estimators&#39;</span>,</span>
<span id="cb10-12"><a href="data-basics.html#cb10-12" aria-hidden="true" tabindex="-1"></a>                               max_resources<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb10-13"><a href="data-basics.html#cb10-13" aria-hidden="true" tabindex="-1"></a>                               n_jobs<span class="op">=-</span><span class="dv">1</span>, <span class="co"># important since hyper-parameter tuning is very costly</span></span>
<span id="cb10-14"><a href="data-basics.html#cb10-14" aria-hidden="true" tabindex="-1"></a>                               scoring <span class="op">=</span> <span class="st">&#39;balanced_accuracy&#39;</span>,</span>
<span id="cb10-15"><a href="data-basics.html#cb10-15" aria-hidden="true" tabindex="-1"></a>                               random_state<span class="op">=</span><span class="dv">0</span>).fit(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/grid_search.html#searching-for-optimal-parameters-with-successive-halving">scikit-learn.org</a><br />
</p>
</div>
</div>
<div id="model-selection" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Model selection<a href="data-basics.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The candidates for hyper-parameters must not be evaluated on the same
data that you trained it on (over-fitting risk). Thus, we separate
another data-set from the training data: The validation set. This is
reduces the amount of training data drastically. Therefore we use the
approaches of Cross Validation and Bootstrapping.</p>
<div id="crossval" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Cross Validation<a href="data-basics.html#crossval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In k-fold Cross Validation, we split the training set into k sub-sets.
We train on the samples in k-1 sub-sets and validate using the data in
the remaining sub-set. We iterate until we have validated on each
sub-set once. We then average out the k scores we obtain.</p>
<div class="figure">
<img src="figures/cross_validation.png" style="width:60.0%" alt="" />
<p class="caption">Schema of the process for 5-fold Cross Validation. The data is first
split into training- and test-data. The training data is split into 5
sub-sets. The algorithm is trained on 4 sub-sets and evaluated on the
remaining sub-set. Each sub-set is used for validation once. <em>Source:
<a href="https://scikit-learn.org/stable/modules/cross_validation.html">scikit-learn.org</a>.</em></p>
</div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="data-basics.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb11-2"><a href="data-basics.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb11-3"><a href="data-basics.html#cb11-3" aria-hidden="true" tabindex="-1"></a>SVM_clf <span class="op">=</span> svm.SVC (kernel<span class="op">=</span><span class="st">&#39;polynomial&#39;</span>)</span>
<span id="cb11-4"><a href="data-basics.html#cb11-4" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> cross_val_score(SVM_clf, X, y, cv <span class="op">=</span> <span class="dv">7</span>)</span>
<span id="cb11-5"><a href="data-basics.html#cb11-5" aria-hidden="true" tabindex="-1"></a>cv_score <span class="op">=</span> cv_scores.mean()</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics">scikit-learn.org</a></p>
<p><strong>!</strong> If you have time-series data (and other clearly not i.i.d.) data,
you have to use <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">special cross-validation
strategies</a>.
  There are <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">further
strategies</a>
worth considering.</p>
</div>
<div id="bootstrapping" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Bootstrapping<a href="data-basics.html#bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Instead of splitting the data into k subsets, you can also just sample
data into training and validation sets.<br />
More info:
<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">wikipedia.org</a>.</p>
</div>
</div>
<div id="errors-in-machine-learning" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Errors in machine learning<a href="data-basics.html#errors-in-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are irreducible errors and reducible errors. Irreducible errors
stem from unknown variables or variables we have no data on. Reducible
errors are deviations from our model to its desired behavior and can be
reduced. Bias and variance are reducible errors.</p>
<p><span class="math display">\[\text{Error} = \text{Bias} + \text{Var} + \text{irr. Error}\]</span></p>
<div id="bias-and-variance" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Bias and Variance<a href="data-basics.html#bias-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="bias-of-an-estimator" class="section level4 hasAnchor" number="2.7.1.1">
<h4><span class="header-section-number">2.7.1.1</span> Bias of an estimator<a href="data-basics.html#bias-of-an-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Bias tells you if your model oversimplifies the true relationship in
your data (underfitting).<br />
You have a model with a parameter <span class="math inline">\(\hat{\theta}\)</span> that is an estimator
for the true <span class="math inline">\(\theta\)</span>. You want to know whether your model over- or
underestimates the true <span class="math inline">\(\theta\)</span> systematically.</p>
<p><span class="math display">\[\text{Bias}[\hat{\theta}]=\text{E}_{X|\mathcal{D}}[\hat{\theta}]- \theta\]</span></p>
<p>E.g. if the parameter captures how polynomial the model / relationship
of your data is, a too high value means that your model is
underfitting.<br />
</p>
<p>More info:
<a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">wikipedia.org</a></p>
</div>
<div id="variance-of-an-estimator" class="section level4 hasAnchor" number="2.7.1.2">
<h4><span class="header-section-number">2.7.1.2</span> Variance of an estimator<a href="data-basics.html#variance-of-an-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Variance tells you if your model learns from noise instead of the true
relationship in your data (overfitting).</p>
<p><span class="math display">\[\text{Var}[\hat{\theta}]=\text{E}_{X|\mathcal{D}}[(\text{E}_{X|\mathcal{D}}[\hat{\theta}]- \hat{\theta})^2]\]</span>
i.e. If you would bootstrap your data, it would show you how much your
parameter would jump around its mean, when it learns from the different
sampled sets.<br />
</p>
<p>Your goal is now to find the sweet spot between a too biased (too simple
model) and a model with too high variance (too complex model).<br />
</p>
<div class="figure">
<img src="figures/Bias_and_variance_contributing_to_total_error.png" style="width:60.0%" alt="" />
<p class="caption">Relationship between bias, variance and the total error. The minimum
of the total error lies at the best compromise between bias and
variance. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Bias_and_variance_contributing_to_total_error.svg">User Bigbossfarin on
wikimedia.org.</a>.</em></p>
</div>
<p>More info:
<a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">wikipedia.org</a></p>
</div>
</div>
<div id="regularization" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Regularization<a href="data-basics.html#regularization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To combat overfitting, we can introduce a term into our loss-function
that penalizes complex models. For linear regression, our regularized
loss function is will be:</p>
<p><span class="math display">\[\min L(\hat{y},y)= \min_{W,b} f(WX+b,y)+\lambda R(W)\]</span> where <span class="math inline">\(f\)</span> is
the unregularized loss function, <span class="math inline">\(W\)</span> is the weight matrix, <span class="math inline">\(X\)</span> is the
sample matrix and <span class="math inline">\(b\)</span> is the bias or offset term of the model (bias term
<span class="math inline">\(\neq\)</span> bias of estimator!). <span class="math inline">\(R\)</span> is the regularization function and
<span class="math inline">\(\lambda\)</span> is a parameter controlling its strength.<br />
i.e. The regularized loss function punishes large weights <span class="math inline">\(W\)</span> and leads
to flatter/smoother functions.<br />
</p>
<p>More info:
<a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">wikipedia.org</a></p>
</div>
<div id="bagging" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Bagging<a href="data-basics.html#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Train several instances of a complex estimator (aka. strong learner,
like large decision trees or KNN with small radius) on a subset of the
data. Then use a majority vote or average the scores for classifying to
get the final prediction. By training on different subsets and averaging
the results, the chances of overfitting are greatly reduced.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="data-basics.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb12-2"><a href="data-basics.html#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb12-3"><a href="data-basics.html#cb12-3" aria-hidden="true" tabindex="-1"></a>bagging <span class="op">=</span> BaggingClassifier(KNeighborsClassifier(), max_features<span class="op">=</span><span class="fl">0.5</span>, n_estimators<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html">scikit-learn.org</a><br />
</p>
<p>A classic example for a bagging classifier is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Random Forest
Classifier</a>
or its variant <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Extremely Randomized
Trees</a>
which further reduces variance and increases bias.</p>
</div>
<div id="boosting" class="section level3 hasAnchor" number="2.7.4">
<h3><span class="header-section-number">2.7.4</span> Boosting<a href="data-basics.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compared to bagging, we use weak learners that are not trained
independently of each other. We start with a single weak learner (e.g. a
small decision tree) and repeat the following steps:</p>
<ol style="list-style-type: decimal">
<li>Add an additional model and train it.</li>
<li>Increase weights of training samples that are falsely classified,
decrease weights of correctly classified samples. (to be used by
next added model.)</li>
<li>Reweight results from the models in the combined model to reduce the
training error.</li>
</ol>
<p>The final model is an weighted ensemble of weak classifiers.<br />
The most popular ones are <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier">gradient boosted decision
tree</a>
algorithms.</p>
</div>
<div id="stacking" class="section level3 hasAnchor" number="2.7.5">
<h3><span class="header-section-number">2.7.5</span> Stacking<a href="data-basics.html#stacking" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Stacking closely resembles bagging: An ensemble of separately trained
base models is used to create an ensemble model. However, the continuous
(instead of discrete) outputs of commonly fewer heterogeneous models
(instead of same type of models) are used. The continuous outputs are
then fed into a final estimator (commonly logistic regression
classifier).</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="data-basics.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb13-2"><a href="data-basics.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb13-3"><a href="data-basics.html#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb13-4"><a href="data-basics.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb13-5"><a href="data-basics.html#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb13-6"><a href="data-basics.html#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> StackingClassifier</span>
<span id="cb13-7"><a href="data-basics.html#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="data-basics.html#cb13-8" aria-hidden="true" tabindex="-1"></a>classifiers <span class="op">=</span> [</span>
<span id="cb13-9"><a href="data-basics.html#cb13-9" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;svc&#39;</span>, SVC()),</span>
<span id="cb13-10"><a href="data-basics.html#cb13-10" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;knn&#39;</span>, KNeighborsClassifier()),</span>
<span id="cb13-11"><a href="data-basics.html#cb13-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;dtc&#39;</span>, DecisionTreeClassifier())</span>
<span id="cb13-12"><a href="data-basics.html#cb13-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb13-13"><a href="data-basics.html#cb13-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-14"><a href="data-basics.html#cb13-14" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> StackingClassifier(</span>
<span id="cb13-15"><a href="data-basics.html#cb13-15" aria-hidden="true" tabindex="-1"></a>    classifiers<span class="op">=</span>estimators, final_estimator<span class="op">=</span>LogisticRegression()</span>
<span id="cb13-16"><a href="data-basics.html#cb13-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-17"><a href="data-basics.html#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="data-basics.html#cb13-18" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span></code></pre></div>
<p>More info:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html">scikit-learn.org</a></p>
</div>
</div>
<div id="tips-for-machine-learning-projects" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Tips for machine learning projects<a href="data-basics.html#tips-for-machine-learning-projects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="general-advice" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> General advice<a href="data-basics.html#general-advice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>General advice for machine learning from <a href="https://courses.cs.duke.edu/spring20/compsci527/papers/Domingos.pdf">Pedro
Domingos</a>:</p>
<ul>
<li><p>Let your knowledge about the problem help you choose the candidate
algorithms. E.g. You know the rules on which comparing samples makes
most sense <span class="math inline">\(\rightarrow\)</span> Choose instance based learners. If you know
that statistical dependencies are relevant <span class="math inline">\(\rightarrow\)</span> choose
Graph based models.</p></li>
<li><p>Don’t underestimate the impact of feature engineering: Many domain
specific features can boost the accuracy.</p></li>
<li><p>Get more samples and candidate features (instead of focussing on the
algorithm)</p></li>
<li><p>Don’t confuse correlation with causation. Just because your model
can predict something, it does not mean that the features cause the
target and you thus cannot easily deduct a clear action from it.</p></li>
</ul>
</div>
</div>
<div id="common-mistakes" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Common mistakes<a href="data-basics.html#common-mistakes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Be aware: This list will never capture everything that can go wrong. ;-)</p>
<ul>
<li><p><a href="https://scikit-learn.org/stable/common_pitfalls.html#data-leakage"><strong>Data
Leakage</strong></a><strong>:</strong>
Information from Samples in your test data have leaked into your
training data.</p>
<ul>
<li>You have not deleted duplicates beforehand</li>
<li>You falsely assumed that your samples where drawn independently
and have sampled the training set randomly. (E.g. multiple
samples from the same patient, time series data)</li>
<li>You have the class label encoded in the training features in a
way that you will not find in “Nature”.</li>
<li>You just used the wrong training / test set while programming.</li>
<li>You did feature engineering like finding n-grams or Max, Min of
data using your test-set data.</li>
<li><strong>Remedy:</strong> Careful preliminary data analysis, deduplication,</li>
</ul></li>
<li><p><strong>Using bad quality measures on unbalanced data:</strong> E.g. Accuracy on
unbalanced data is not a reasonable quality measure.</p></li>
<li><p><a href="https://scikit-learn.org/stable/common_pitfalls.html#inconsistent-preprocessing"><strong>Inconsistent
preprocessing</strong></a><strong>:</strong>
If you preprocess your training data in a certain way, you have to
do the same with the test- and prediction-data.</p>
<ul>
<li><strong>Remedy</strong>: Use one preprocessing pipeline that you can use for
training, testing and prediction.</li>
</ul></li>
<li><p><strong>Curse of dimensionality:</strong></p>
<ul>
<li>You use too many features for the amount of samples that you
have</li>
<li>Your distance measure is not suitable for high-dimensional space
(e.g. Hamming distance, Euclidean distance)</li>
<li><strong>Remedy:</strong> Use lower-dimensional mapping.</li>
</ul></li>
<li><p><strong>Overfitting:</strong></p>
<ul>
<li>You use a too complex algorithm (too many degrees of freedom)
for the amount of data you have</li>
<li>You have too many features</li>
<li><strong>Remedy:</strong> Get more samples, reduce the dimensionanlity,
feature selection, regularization, bagging, boosting, stacking.</li>
</ul></li>
<li><p><strong>Bad Data:</strong></p>
<ul>
<li>Your data is not representative of what you would find in the
“real world”. (skewed population, too old data, only of specific
sensors, locations…)</li>
<li>Your have many missing values among your features.</li>
<li>The data that you have is only remotely linked to the target
that you want to predict.</li>
<li>There are erroneous entries in your data.</li>
<li><strong>Remedy:</strong> Clean data at source, impute data, clean data during
preprocessing, get more representative data, limit scope of
application.</li>
</ul></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability-theory-linear-algebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-Data-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
