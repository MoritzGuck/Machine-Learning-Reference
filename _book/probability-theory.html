<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Probability Theory | Moritz’ Machine Learning Summary</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Probability Theory | Moritz’ Machine Learning Summary" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Probability Theory | Moritz’ Machine Learning Summary" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
set in the _output.yml file.
The HTML output format for this example is bookdown::gitbook,</p>" />
  

<meta name="author" content="Moritz Gück" />


<meta name="date" content="2023-01-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="data-basics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-theory.html"><a href="probability-theory.html#probability-basics"><i class="fa fa-check"></i><b>1.1</b> Probability Basics</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-theory.html"><a href="probability-theory.html#probability-interpretations"><i class="fa fa-check"></i><b>1.1.1</b> Probability interpretations</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-theory.html"><a href="probability-theory.html#probability-space"><i class="fa fa-check"></i><b>1.1.2</b> Probability Space</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-theory.html"><a href="probability-theory.html#axioms-of-probability"><i class="fa fa-check"></i><b>1.1.3</b> Axioms of Probability</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-theory.html"><a href="probability-theory.html#random-variable-rv"><i class="fa fa-check"></i><b>1.1.4</b> Random Variable (RV)</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-theory.html"><a href="probability-theory.html#proposition"><i class="fa fa-check"></i><b>1.1.5</b> Proposition</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions"><i class="fa fa-check"></i><b>1.2</b> Probability distributions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-theory.html"><a href="probability-theory.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>1.2.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="1.2.2" data-path="probability-theory.html"><a href="probability-theory.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>1.2.2</b> Probability Density Function (PDF)</a></li>
<li class="chapter" data-level="1.2.3" data-path="probability-theory.html"><a href="probability-theory.html#dist_prop"><i class="fa fa-check"></i><b>1.2.3</b> Properties of Distributions</a></li>
<li class="chapter" data-level="1.2.4" data-path="probability-theory.html"><a href="probability-theory.html#diracdelta"><i class="fa fa-check"></i><b>1.2.4</b> Dirac delta function</a></li>
<li class="chapter" data-level="1.2.5" data-path="probability-theory.html"><a href="probability-theory.html#uniform-distribution"><i class="fa fa-check"></i><b>1.2.5</b> Uniform distribution</a></li>
<li class="chapter" data-level="1.2.6" data-path="probability-theory.html"><a href="probability-theory.html#discrete-distributions"><i class="fa fa-check"></i><b>1.2.6</b> Discrete distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-theory.html"><a href="probability-theory.html#continuous-distributions"><i class="fa fa-check"></i><b>1.3</b> Continuous distributions</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="probability-theory.html"><a href="probability-theory.html#normalgaussian-distribution"><i class="fa fa-check"></i><b>1.3.1</b> Normal/Gaussian distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="probability-theory.html"><a href="probability-theory.html#multivariate-normalgaussian-distribution"><i class="fa fa-check"></i><b>1.3.2</b> Multivariate normal/Gaussian distribution</a></li>
<li class="chapter" data-level="1.3.3" data-path="probability-theory.html"><a href="probability-theory.html#beta-distribution"><i class="fa fa-check"></i><b>1.3.3</b> Beta distribution</a></li>
<li class="chapter" data-level="1.3.4" data-path="probability-theory.html"><a href="probability-theory.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.3.4</b> Dirichlet distribution</a></li>
<li class="chapter" data-level="1.3.5" data-path="probability-theory.html"><a href="probability-theory.html#marginal-distributions"><i class="fa fa-check"></i><b>1.3.5</b> Marginal distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability-theory.html"><a href="probability-theory.html#CLT"><i class="fa fa-check"></i><b>1.4</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.5" data-path="probability-theory.html"><a href="probability-theory.html#bayesian-probability"><i class="fa fa-check"></i><b>1.5</b> Bayesian probability</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="probability-theory.html"><a href="probability-theory.html#conditionalposterior-probability"><i class="fa fa-check"></i><b>1.5.1</b> Conditional/Posterior Probability</a></li>
<li class="chapter" data-level="1.5.2" data-path="probability-theory.html"><a href="probability-theory.html#independence"><i class="fa fa-check"></i><b>1.5.2</b> Independence</a></li>
<li class="chapter" data-level="1.5.3" data-path="probability-theory.html"><a href="probability-theory.html#conditional-independence"><i class="fa fa-check"></i><b>1.5.3</b> Conditional independence</a></li>
<li class="chapter" data-level="1.5.4" data-path="probability-theory.html"><a href="probability-theory.html#bayes-rule"><i class="fa fa-check"></i><b>1.5.4</b> Bayes Rule</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="probability-theory.html"><a href="probability-theory.html#further-concepts"><i class="fa fa-check"></i><b>1.6</b> Further Concepts</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="probability-theory.html"><a href="probability-theory.html#convergence-in-probability-of-random-variables"><i class="fa fa-check"></i><b>1.6.1</b> Convergence in Probability of Random Variables</a></li>
<li class="chapter" data-level="1.6.2" data-path="probability-theory.html"><a href="probability-theory.html#bernoullis-theorem-weak-law-of-large-numbers"><i class="fa fa-check"></i><b>1.6.2</b> Bernoulli’s Theorem / Weak Law of Large Numbers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-basics.html"><a href="data-basics.html"><i class="fa fa-check"></i><b>2</b> Data Basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-basics.html"><a href="data-basics.html#similarity-and-distance-measures"><i class="fa fa-check"></i><b>2.1</b> Similarity and Distance Measures</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-basics.html"><a href="data-basics.html#metrics"><i class="fa fa-check"></i><b>2.1.1</b> Metrics</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-basics.html"><a href="data-basics.html#similarity-measures-on-vectors"><i class="fa fa-check"></i><b>2.1.2</b> Similarity measures on vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-basics.html"><a href="data-basics.html#preprocessing-data"><i class="fa fa-check"></i><b>2.2</b> Preprocessing data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-basics.html"><a href="data-basics.html#standardization"><i class="fa fa-check"></i><b>2.2.1</b> Standardization</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-basics.html"><a href="data-basics.html#encoding-categorical-features"><i class="fa fa-check"></i><b>2.2.2</b> Encoding categorical features</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-basics.html"><a href="data-basics.html#imputing-missing-values"><i class="fa fa-check"></i><b>2.2.3</b> Imputing missing values</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-basics.html"><a href="data-basics.html#splitting-in-training--and-test-data"><i class="fa fa-check"></i><b>2.3</b> Splitting in training- and test-data</a></li>
<li class="chapter" data-level="2.4" data-path="data-basics.html"><a href="data-basics.html#feature-selection"><i class="fa fa-check"></i><b>2.4</b> Feature selection</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-basics.html"><a href="data-basics.html#a-priori-feature-selection"><i class="fa fa-check"></i><b>2.4.1</b> A priori feature selection</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-basics.html"><a href="data-basics.html#wrapper-methods"><i class="fa fa-check"></i><b>2.4.2</b> wrapper methods</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-basics.html"><a href="data-basics.html#advice-pitfalls"><i class="fa fa-check"></i><b>2.4.3</b> Advice &amp; Pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-basics.html"><a href="data-basics.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>2.5</b> Hyper-parameter tuning</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-basics.html"><a href="data-basics.html#grid-search"><i class="fa fa-check"></i><b>2.5.1</b> Grid search</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-basics.html"><a href="data-basics.html#randomized-search"><i class="fa fa-check"></i><b>2.5.2</b> randomized search</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="data-basics.html"><a href="data-basics.html#model-selection"><i class="fa fa-check"></i><b>2.6</b> Model selection</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="data-basics.html"><a href="data-basics.html#crossval"><i class="fa fa-check"></i><b>2.6.1</b> Cross Validation</a></li>
<li class="chapter" data-level="2.6.2" data-path="data-basics.html"><a href="data-basics.html#bootstrapping"><i class="fa fa-check"></i><b>2.6.2</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="data-basics.html"><a href="data-basics.html#errors-in-machine-learning"><i class="fa fa-check"></i><b>2.7</b> Errors in machine learning</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="data-basics.html"><a href="data-basics.html#bias-and-variance"><i class="fa fa-check"></i><b>2.7.1</b> Bias and Variance</a></li>
<li class="chapter" data-level="2.7.2" data-path="data-basics.html"><a href="data-basics.html#regularization"><i class="fa fa-check"></i><b>2.7.2</b> Regularization</a></li>
<li class="chapter" data-level="2.7.3" data-path="data-basics.html"><a href="data-basics.html#bagging"><i class="fa fa-check"></i><b>2.7.3</b> Bagging</a></li>
<li class="chapter" data-level="2.7.4" data-path="data-basics.html"><a href="data-basics.html#boosting"><i class="fa fa-check"></i><b>2.7.4</b> Boosting</a></li>
<li class="chapter" data-level="2.7.5" data-path="data-basics.html"><a href="data-basics.html#stacking"><i class="fa fa-check"></i><b>2.7.5</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="data-basics.html"><a href="data-basics.html#tips-for-machine-learning-projects"><i class="fa fa-check"></i><b>2.8</b> Tips for machine learning projects</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="data-basics.html"><a href="data-basics.html#general-advice"><i class="fa fa-check"></i><b>2.8.1</b> General advice</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="data-basics.html"><a href="data-basics.html#common-mistakes"><i class="fa fa-check"></i><b>2.9</b> Common mistakes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-methods.html"><a href="classification-methods.html"><i class="fa fa-check"></i><b>3</b> Classification Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-methods.html"><a href="classification-methods.html#evaluation-of-classifiers"><i class="fa fa-check"></i><b>3.1</b> Evaluation of Classifiers</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-methods.html"><a href="classification-methods.html#confusion-matrix"><i class="fa fa-check"></i><b>3.1.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-methods.html"><a href="classification-methods.html#basic-quality-measures"><i class="fa fa-check"></i><b>3.1.2</b> Basic Quality Measures</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-methods.html"><a href="classification-methods.html#area-under-the-curve"><i class="fa fa-check"></i><b>3.1.3</b> Area under the Curve</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-methods.html"><a href="classification-methods.html#handling-unbalanced-data"><i class="fa fa-check"></i><b>3.1.4</b> Handling Unbalanced Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-methods.html"><a href="classification-methods.html#classification-algorithms"><i class="fa fa-check"></i><b>3.2</b> Classification Algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-methods.html"><a href="classification-methods.html#nearest-neighbors-classifier"><i class="fa fa-check"></i><b>3.2.1</b> Nearest Neighbors Classifier</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-methods.html"><a href="classification-methods.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>3.2.2</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-methods.html"><a href="classification-methods.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>3.2.3</b> Linear discriminant analysis (LDA)</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-methods.html"><a href="classification-methods.html#support-vector-classifier-svc"><i class="fa fa-check"></i><b>3.2.4</b> Support Vector Classifier (SVC)</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-methods.html"><a href="classification-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.2.5</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#custering-methods"><i class="fa fa-check"></i><b>4.1</b> Custering Methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#metrics-for-clustering-algorithms"><i class="fa fa-check"></i><b>4.1.1</b> metrics for Clustering algorithms</a></li>
<li class="chapter" data-level="4.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#graph-based-clustering"><i class="fa fa-check"></i><b>4.1.3</b> Graph-Based Clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#spectral-clustering"><i class="fa fa-check"></i><b>4.1.4</b> Spectral Clustering</a></li>
<li class="chapter" data-level="4.1.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#SSP"><i class="fa fa-check"></i><b>4.1.5</b> Sparse Subspace Clustering (SSP)</a></li>
<li class="chapter" data-level="4.1.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#soft-assignment-clustering"><i class="fa fa-check"></i><b>4.1.6</b> Soft-assignment Clustering</a></li>
<li class="chapter" data-level="4.1.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#expectation-maximization-em-clustering"><i class="fa fa-check"></i><b>4.1.7</b> Expectation Maximization (EM) Clustering</a></li>
<li class="chapter" data-level="4.1.8" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#gaussian-mixture-models"><i class="fa fa-check"></i><b>4.1.8</b> Gaussian Mixture Models</a></li>
<li class="chapter" data-level="4.1.9" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#artificial-neural-networks-for-clustering"><i class="fa fa-check"></i><b>4.1.9</b> Artificial Neural Networks for Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#mapping-to-lower-dimensions"><i class="fa fa-check"></i><b>4.2</b> Mapping to lower dimensions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#manifold-learning"><i class="fa fa-check"></i><b>4.2.1</b> Manifold learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#decomposition-techniques"><i class="fa fa-check"></i><b>4.2.2</b> Decomposition techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#outlier-detection"><i class="fa fa-check"></i><b>4.3</b> Outlier detection</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#local-outlier-factor"><i class="fa fa-check"></i><b>4.3.1</b> Local outlier factor</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#isolation-forest"><i class="fa fa-check"></i><b>4.3.2</b> Isolation forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generative-models.html"><a href="generative-models.html"><i class="fa fa-check"></i><b>5</b> Generative models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="generative-models.html"><a href="generative-models.html#generative-models-for-discrete-data"><i class="fa fa-check"></i><b>5.1</b> Generative Models for Discrete Data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="generative-models.html"><a href="generative-models.html#bayesian-concept-learning"><i class="fa fa-check"></i><b>5.1.1</b> Bayesian Concept Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="generative-models.html"><a href="generative-models.html#beta-binomial-model"><i class="fa fa-check"></i><b>5.1.2</b> Beta-binomial model</a></li>
<li class="chapter" data-level="5.1.3" data-path="generative-models.html"><a href="generative-models.html#dirichlet-multinomial-model"><i class="fa fa-check"></i><b>5.1.3</b> Dirichlet-multinomial model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#evaluation-of-regression-models"><i class="fa fa-check"></i><b>6.1</b> Evaluation of regression models</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#r2-score"><i class="fa fa-check"></i><b>6.1.1</b> R^2 score</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#mean-squared-error"><i class="fa fa-check"></i><b>6.1.2</b> Mean squared error</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#visual-tools"><i class="fa fa-check"></i><b>6.1.3</b> Visual tools</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#regression-algorithms"><i class="fa fa-check"></i><b>6.2</b> Regression algorithms</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#linear-models"><i class="fa fa-check"></i><b>6.2.1</b> Linear Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Networks {#Neural Networks}</a>
<ul>
<li class="chapter" data-level="7.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#non-linearities"><i class="fa fa-check"></i><b>7.1.1</b> Non-Linearities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#feedforward-neural-network-multi-layer-perceptron"><i class="fa fa-check"></i><b>7.2</b> Feedforward Neural Network / Multi-Layer Perceptron</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#bekpropageshn"><i class="fa fa-check"></i><b>7.2.1</b> Bekpropageshn</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>7.3</b> Convolutional Neural Networks</a></li>
<li class="chapter" data-level="7.4" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>7.4</b> Autoencoders</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#autoencoders-for-clustering"><i class="fa fa-check"></i><b>7.4.1</b> Autoencoders for clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>7.5</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="7.6" data-path="neural-networks-neural-networks.html"><a href="neural-networks-neural-networks.html#recurrent-neural-networks"><i class="fa fa-check"></i><b>7.6</b> Recurrent Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="explanation-and-inspection-methods.html"><a href="explanation-and-inspection-methods.html"><i class="fa fa-check"></i><b>8</b> Explanation and inspection methods</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Moritz’ Machine Learning Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-theory" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Probability Theory<a href="probability-theory.html#probability-theory" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A probability is a measure of how frequent or likely an event will take
place.</p>
<div id="probability-basics" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Probability Basics<a href="probability-theory.html#probability-basics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="probability-interpretations" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Probability interpretations<a href="probability-theory.html#probability-interpretations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><strong>Frequentist:</strong> Fraction of positive samples, if we measured
infinitely many samples.</p></li>
<li><p><strong>Objectivist:</strong> Probabilities are due to inherent uncertainty
properties. Probabilities are calculated by putting outcomes of
interest into relation with all possible outcomes.</p></li>
<li><p><strong>Subjectivist:</strong> An agent’s <em>rational</em> degree of belief (not
external). The belief needs to be coherent (i.e. if you make bets
using your probabilities you should not be guaranteed lose money)
and therefore need to follow the rules of probability.</p></li>
<li><p><strong>Bayesian:</strong> (Building on subjectivism) A reasonable expectation /
degree of belief based on the information available to the
statistician / system. It allows to give certainties to events,
where we don’t have samples on (e.g. disappearance of the south pole
until 2030).</p></li>
</ul>
<p>Also the frequentist view is not free of subjectivity since you need to
compare events on otherwise similar objects. Usually there are no
completely similar objects, so you need to define them.</p>
</div>
<div id="probability-space" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Probability Space<a href="probability-theory.html#probability-space" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The probability space is a triplet space containing a sample/outcome
space <span class="math inline">\(\Omega\)</span> (containing all possible atomic events), a collection of
events <span class="math inline">\(S\)</span> (containing a subset of <span class="math inline">\(\Omega\)</span> to which we want to assign
probabilities) and the mapping <span class="math inline">\(P\)</span> between <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(S\)</span>.</p>
</div>
<div id="axioms-of-probability" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Axioms of Probability<a href="probability-theory.html#axioms-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The mapping <span class="math inline">\(P\)</span> must fulfill the axioms of probability:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P(a) \geq 0\)</span></p></li>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span></p></li>
<li><p><span class="math inline">\(a,b \in S\)</span> and <span class="math inline">\(a \cap b = \{\}\)</span>
<span class="math inline">\(\Rightarrow P(a \cup b) = P(a) + P(b)\)</span></p></li>
</ol>
<p><span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> are events.</p>
</div>
<div id="random-variable-rv" class="section level3 hasAnchor" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Random Variable (RV)<a href="probability-theory.html#random-variable-rv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A RV is a <strong>function</strong> that maps points from the sample space <span class="math inline">\(\Omega\)</span>
to some range (e.g. Real numbers or booleans). They are characterized by
their distribution function. E.g. for a coin toss:
<span class="math display">\[X(\omega) = \begin{cases}
                0, \text{ if } \omega = heads\\
                1, \text{ if } \omega = tails.
            \end{cases}\]</span></p>
</div>
<div id="proposition" class="section level3 hasAnchor" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Proposition<a href="probability-theory.html#proposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A Proposition is a conclusion of a statistical inference/prediction that
can be true or false (e.g. a classification of a datapoint). More
formally: A disjunction of events where the logic model holds. An event
can be written as a <strong>propositional logic model</strong>:<br />
<span class="math inline">\(A = true, B = false \Rightarrow a \land \neg b\)</span>. Propositions can be
continuous, discrete or boolean.</p>
</div>
</div>
<div id="probability-distributions" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Probability distributions<a href="probability-theory.html#probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Probability distributions assign probabilities to to all possible points
in <span class="math inline">\(\Omega\)</span> (e.g. <span class="math inline">\(P(Weather) = \langle 0.3, 0.4, 0.2, 0.1 \rangle\)</span>,
representing Rain, sunshine, clouds and snow). Joint probability
distributions give you a probability for each atomic event of the RVs
(e.g. <span class="math inline">\(P(weather, accident)\)</span> gives you a <span class="math inline">\(2\times 4\)</span> matrix.)</p>
<div id="cumulative-distribution-function-cdf" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Cumulative Distribution Function (CDF)<a href="probability-theory.html#cumulative-distribution-function-cdf" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The CDF is defined as <span class="math inline">\(F_X(x) = P(X \leq x)\)</span> (See figure
<a href="probability-theory.html#CDF" reference-type="ref" reference="CDF"><span class="math inline">\(CDF\)</span></a>).</p>
<div class="figure">
<img src="figures/Normal_Distribution_CDF.png" id="CDF" style="width:60.0%" alt="" />
<p class="caption">Cumulative distribution function of a normal distribution for
different mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma\)</span>). <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Normal_Distribution_CDF.svg">user
Inductiveload on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="probability-density-function-pdf" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Probability Density Function (PDF)<a href="probability-theory.html#probability-density-function-pdf" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For continuous functions the PDF is defined by
<span class="math display">\[p(x) =  {d \over dx} p(X \leq x).\]</span> The probability of x being in a
finite interval is <span class="math display">\[P(a &lt; X \leq b) = \int_a^b p(x) dx\]</span> A PDF is shown
in the following figure.</p>
<div class="figure">
<img src="figures/Boxplot_vs_PDF.png" id="Boxplot" style="width:60.0%" alt="" />
<p class="caption">Probability density function of a normal distribution with variance
(<span class="math inline">\(\sigma\)</span>). In red a range from a Box-plot is shown with
<a href="probability-theory.html#dist_prop">quartiles</a> (Q1, Q3) and interquartile range (IQR). For the
cutoffs (borders to darker blue regions) the IQR (on top) and <span class="math inline">\(\sigma\)</span>
are chosen. Another common cutoff is the confidence interval with light
blue regions having a probability mass of <span class="math inline">\(2 * \alpha / 2\)</span>. <em>Source:
<a href="https://commons.wikimedia.org/wiki/File:Boxplot_vs_PDF.svg">user Jhguch on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="dist_prop" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Properties of Distributions<a href="probability-theory.html#dist_prop" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The <strong>expected value</strong> (<span class="math inline">\(E\)</span>) or <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) is given by
<span class="math inline">\(E[X] = \sum_{x \in X} x*p(x)\)</span> for discrete RVs and
<span class="math inline">\(E[X] = \int_X x*p(x) dx\)</span> for continuous RVs.</p></li>
<li><p>The <strong>variance</strong> measures the spread of a distribution:
<span class="math inline">\(var[X] = \sigma^2 = E[(X-\mu)^2] = E[X]^2 - \mu^2\)</span>.</p></li>
<li><p>The <strong>standard deviation</strong> is given by: <span class="math inline">\(\sqrt{var[X]} = \sigma\)</span>.</p></li>
<li><p>The <strong>mode</strong> is the value with the highest probability (or the point
in the PDF with the highest value):</p></li>
<li><p>The <strong>median</strong> is the point at which all point less than the median
and all points greater than the median have the same probability
(<span class="math inline">\(0.5\)</span>).</p></li>
<li><p>The <strong>quantiles</strong> (<span class="math inline">\(Q\)</span>) divide the datapoints into sets of equal
number. The <span class="math inline">\(Q_1\)</span> qua<strong>r</strong>tile has 25% of the values below it. The
<strong>interquartile range</strong> (IQR) is a measure to show the variability
in the data (how distant the points from the first and last quartile
are)</p></li>
</ul>
</div>
<div id="diracdelta" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Dirac delta function<a href="probability-theory.html#diracdelta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>dirac delta</strong> is simply a function that is infinite at one point
and 0 everywhere else:
<span class="math display">\[\delta(x)=\begin{cases} \infty , \; \text{ if } x = 0 \\0, \quad \text{if } x \neq 0 \end{cases} \qquad \text{and } \int_{-\infty}^{\infty} \delta(x) dx = 1\]</span>
(Needed for distributions further on)</p>
</div>
<div id="uniform-distribution" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Uniform distribution<a href="probability-theory.html#uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The uniform distribution has the same probability throughout a specific
interval:
<span class="math display">\[\text{Unif}(a,b) = \frac{1}{b-a} 1 \mkern-6mu 1 (a &lt; x \leq b) = \begin{cases}
                \frac{1}{b-a}, \quad \text{if } x \in [ a,b ] \\
                0, \qquad \text{else}
            \end{cases}\]</span> <span class="math inline">\(1 \mkern-6mu 1\)</span> is a vector of ones.</p>
<div class="figure">
<img src="figures/Uniform_Distribution.png" style="width:40.0%" alt="" />
<p class="caption">Uniform distribution. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Uniform_Distribution_PDF_SVG.svg">user IkamusumeFan on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="discrete-distributions" class="section level3 hasAnchor" number="1.2.6">
<h3><span class="header-section-number">1.2.6</span> Discrete distributions<a href="probability-theory.html#discrete-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Used for random variables that have discrete states.</p>
<div id="binomial-distribution" class="section level4 hasAnchor" number="1.2.6.1">
<h4><span class="header-section-number">1.2.6.1</span> Binomial distribution<a href="probability-theory.html#binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Used for series of experiments with two outcomes (success or miss. e.g.
a series of coin flips).
<span class="math display">\[X \sim \text{Bin}(n, \theta ), \quad \text{Bin}(k|n,\theta)={n \choose k} \theta^k (1-\theta)^{n-k} , \quad {n \choose k} = \frac{n!}{k!(n-k)!},\]</span>
where <span class="math inline">\(n\)</span> is the number of total experiments, <span class="math inline">\(k\)</span> is the number of
successful experiments and <span class="math inline">\(\theta\)</span> is the probability of success of an
experiment.</p>
<div class="figure">
<img src="figures/Pascals_triangle_binomial_distribution.png" alt="" />
<p class="caption">Binomial distribution of balls in <a href="https://en.wikipedia.org/wiki/Pascal%27s_triangle">Pascals
triangles</a> with
different numbers of layers (The top one has 0 layers). Example: For a
triangle with <span class="math inline">\(n=6\)</span> layers, the probability that a ball lands in the
middle box <span class="math inline">\(k=3\)</span> is <span class="math inline">\(20/64\)</span>. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Pascal%27s_triangle;_binomial_distribution.svg">user Watchduck on
wikimedia.org</a></em></p>
</div>
</div>
<div id="bernoulli-distribution" class="section level4 hasAnchor" number="1.2.6.2">
<h4><span class="header-section-number">1.2.6.2</span> Bernoulli distribution<a href="probability-theory.html#bernoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Is a special case of the binomial distribution with <span class="math inline">\(n=1\)</span> (e.g. one coin
toss).
<span class="math display">\[X \sim \text{Ber}(\theta ), \quad \text{Ber}(x | \theta)=\theta^{1 \mkern-6mu 1 (x=1)} (1-\theta)^{1 \mkern-6mu 1(x=0)}= \begin{cases}
                    \theta, \qquad \text{if } x=1 \\
                    1 - \theta, \; \text{if } x=0
                \end{cases}\]</span></p>
</div>
<div id="multinomial-distribution" class="section level4 hasAnchor" number="1.2.6.3">
<h4><span class="header-section-number">1.2.6.3</span> Multinomial distribution<a href="probability-theory.html#multinomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Used for experiments with k different outcomes (e.g. dice rolls:
Probability of different counts of the different sides).
<span class="math display">\[\text{Mu}(x|n,\theta) =  {n \choose x_1, ..., x_K}\prod_{i=1}^K\theta_j^{x_j} = \frac{n!}{x_1!, ..., x_k!}\prod_{i=1}^K\theta_j^{x_j},\]</span>
where <span class="math inline">\(k\)</span> is the number of outcomes, <span class="math inline">\(x_j\)</span> is the number times that
outcome <span class="math inline">\(j\)</span> happens. <span class="math inline">\(X = (X_1, ..., X_K)\)</span> is the <em>random vector</em>.</p>
</div>
<div id="multinoulli-distribution" class="section level4 hasAnchor" number="1.2.6.4">
<h4><span class="header-section-number">1.2.6.4</span> Multinoulli distribution<a href="probability-theory.html#multinoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Is a special case of the multinomial distribution with <span class="math inline">\(n=1\)</span>. The random
vector is then represented in <em>dummy-</em> or <em>one-hot-encoding</em> (e.g.
<span class="math inline">\((0,0,1,0,0,0)\)</span> if outcome 3 takes place).
<span class="math display">\[\text{Mu}(x|1,\theta) = \prod_{j=0}^K \theta_j^{1 \mkern-6mu 1(x_j=1)}\]</span></p>
</div>
<div id="empirical-distribution" class="section level4 hasAnchor" number="1.2.6.5">
<h4><span class="header-section-number">1.2.6.5</span> Empirical distribution<a href="probability-theory.html#empirical-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The empirical distribution follows the empirical measurements strictly.
The CDF jumps by 1/n every time a sample is “encountered” (see figure).</p>
<p><span class="math display">\[\text{p}_{\text{emp}}(A) = \frac{1}{N} \sum_{i=1}^N \delta_{x_i}(A), \quad \delta_{x_i}=\begin{cases}1, \quad \text{if } x \in A \\0, \quad \text{if } x \notin A \end{cases},\]</span>
w where <span class="math inline">\(x_1, ..., x_N\)</span> is a data set with N points. The points can also
be weighted: <span class="math display">\[p(x) =  \sum_{i=1}^N w_i \delta_{x_i}(x)\]</span></p>
<div class="figure">
<img src="figures/Empirical_distribution_function.png" style="width:50.0%" alt="" />
<p class="caption">Cumulative empirical distribution function (blue line) for samples
drawn from a standard normal distribution (green line). The values of
the drawn samples is shown as grey lines at the bottom. Source: <a href="https://commons.wikimedia.org/wiki/File:Empirical_distribution_function.png">user
nagualdesign on
wikimedia.org.</a></p>
</div>
</div>
</div>
</div>
<div id="continuous-distributions" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Continuous distributions<a href="probability-theory.html#continuous-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Used for random variables that have continuous states.</p>
<div id="normalgaussian-distribution" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Normal/Gaussian distribution<a href="probability-theory.html#normalgaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Often chosen for random noise because it is simple and needs few
assumptions (see sect. <a href="probability-theory.html#CLT">CLT</a>). The PDF is given by:</p>
<p><span class="math display">\[p(x|\mu\sigma^2)= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right],\]</span>
where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance. The CDF is given
by:
<span class="math display">\[\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{\infty}^xe^{\frac{-t^2}{2}dt}\]</span></p>
</div>
<div id="multivariate-normalgaussian-distribution" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Multivariate normal/Gaussian distribution<a href="probability-theory.html#multivariate-normalgaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For T datapoints with k dimensions (features). The pdf is:
<span class="math display">\[p(x|\mu,\Sigma) = \dfrac{1}{\sqrt{(2\pi)^k|\Sigma|}}\exp\left[-\dfrac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu)\right],\]</span>
where x now has multiple dimension (<span class="math inline">\(x_1, x_2, ..., x_k\)</span>) and <span class="math inline">\(\Sigma\)</span>
is the <span class="math inline">\(k \times k\)</span> covariance matrix:
<span class="math inline">\(\Sigma = \text{E}[(X-\mu)(X-\mu)]\)</span>. The covariance between features is:
<span class="math inline">\(\text{Cov}[X_i, X_j] = \text{E}[(X_i-\mu_i)(X_j-\mu_j)]\)</span></p>
</div>
<div id="beta-distribution" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Beta distribution<a href="probability-theory.html#beta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>defined for <span class="math inline">\(0 \leq x \leq 1\)</span> (see figure <a href="#Beta_distr">Beta
distribution</a>). The pdf is:
<span class="math display">\[f(x|\alpha, \beta) = \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\]</span>
The <a href="https://en.wikipedia.org/wiki/Beta_function">beta function</a> <span class="math inline">\(B\)</span> is
there to normalize and ensure that the total probability is 1 .</p>
<div class="figure">
<img src="figures/Beta_distribution_pdf.png" style="width:60.0%" label="Beta_distr" alt="" />
<p class="caption">Probability density function of a beta-distribution with different
parameter values. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Beta_distribution_pdf.png">user MarkSweep on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="dirichlet-distribution" class="section level3 hasAnchor" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> Dirichlet distribution<a href="probability-theory.html#dirichlet-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The multivariate version of the Beta distribution. The PDF is:
<span class="math display">\[\text{Dir}({x}|{\alpha}) \triangleq \dfrac{1}{B({\alpha})}\prod\limits_{i=1}^K x_i^{\alpha_i-1},\quad \sum_{i=1}^K x_i =1, \quad x_i \geq 0 \text{ }\forall i\]</span></p>
<div class="figure">
<img src="figures/Dirichlet_distributions.png" style="width:60.0%" label="#Dirichlet_distr" alt="" />
<p class="caption">Probability density function of a Dirichlet-distribution on a
2-simplex (triangle) with different parameter values. Clockwise from top
left: <span class="math inline">\(\alpha\)</span> = (6,2,2), (3,7,5), (6,2,6), (2,3,4). <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Dirichlet_distributions.png">user ThG
on
wikimedia.org</a>.</em></p>
</div>
</div>
<div id="marginal-distributions" class="section level3 hasAnchor" number="1.3.5">
<h3><span class="header-section-number">1.3.5</span> Marginal distributions<a href="probability-theory.html#marginal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Are the probability distributions of subsets of the original
distribution. Marginal distributions of normal distributions are also
normal distributions.</p>
<div class="figure">
<img src="figures/Multivariate_gaussian.png" style="width:60.0%" alt="" />
<p class="caption">Data following a 2D-Gaussian distribution. Marginal distributions are
shown on the sides in blue and orange. <em>Source: <a href="https://commons.wikimedia.org/wiki/File:Multivariate_Gaussian_inequality_demonstration.svg">user Auguel on
wikimedia.org</a>.</em></p>
</div>
</div>
</div>
<div id="CLT" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Central limit theorem<a href="probability-theory.html#CLT" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many cases the sum of random variables will follow a normal
distribution as n goes to infinity.</p>
</div>
<div id="bayesian-probability" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Bayesian probability<a href="probability-theory.html#bayesian-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Baeysian probability represents the plausibility of a proposition based
on the available information (i.e. the degree at which the information
supports the proposition). The use of this form of statistics is
especially useful if random variables cannot be assumed to be i.i.d.
(i.e. When an event is not independent of the event before it (e.g.
drawing balls without laying them back into the urn)).</p>
<div id="conditionalposterior-probability" class="section level3 hasAnchor" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Conditional/Posterior Probability<a href="probability-theory.html#conditionalposterior-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Expresses the probability of one event (<span class="math inline">\(Y\)</span>) under the condition that
another event (<span class="math inline">\(E\)</span>) has occurred. (e.g. <span class="math inline">\(C\)</span> = “gets cancer”, <span class="math inline">\(S\)</span> = “is a
smoker” <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(p(C|S)=0.2\)</span>, meaning: “given the <em>sole
information</em> that someone is a smoker, their probability of getting
cancer is 20%.”)<br />
<br />
The conditional probability can be calculated like follows. By defining
the joined probability like so: <span class="math display">\[P(A \cap B) = P(A \mid B) P(B)\]</span> you
solve for <span class="math inline">\(P(A \mid B)\)</span>:
<span class="math display">\[P(A \mid B) = \frac{P(A \cap B)}{P(B)}=\frac{P(A, B)}{P(B)}=\alpha{P(A, B)},\]</span>
where <span class="math inline">\(\alpha\)</span> is used as a normalization constant. If you have hidden
variables (confounding factors) you need to sum them out like so:
<span class="math display">\[P(Y|E=e)=\alpha P(Y,E=e)=\alpha\sum_h P(Y,E=e,H=h)\]</span> where <span class="math inline">\(X\)</span>
contains all variables, <span class="math inline">\(Y\)</span> is called <em>query variable</em>, <span class="math inline">\(E\)</span> is called
<em>evidence variable</em>, <span class="math inline">\(H=X-Y-E\)</span> is called <em>hidden variable</em> or
<em>confounding factor</em>. You get the joint probabilities by summing out the
hidden variable. <!--# check if the formula is correct --></p>
<p><strong>!</strong> Usually <span class="math inline">\(p(A|B) \neq p(B|A)\)</span><br />
<strong>!</strong> Priors are often forgotten: E.g. <span class="math inline">\(P(\text{&quot;COVID-19&quot;})\)</span> is
confused with <span class="math inline">\(P(\text{&quot;COVID-19&quot;}|\text{&quot;Person is getting tested&quot;})\)</span>
(because only people with symptoms go to the testing station).<br />
<strong>!</strong> Base rate neglect: Under-representing the prior probability. E.g.
You have a test with a 5% false positive rate and a incidence of disease
of 2% in the population. If you are tested positive in a population
screening your probability of having the disease is only 29%.<br />
Conditional distributions of Gaussian distributions are Gaussian
distributions themselves.</p>
</div>
<div id="independence" class="section level3 hasAnchor" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> Independence<a href="probability-theory.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For independent variables it holds: <span class="math inline">\(P(A|B)=P(A)\)</span> or <span class="math inline">\(P(B|A)=P(B)\)</span></p>
</div>
<div id="conditional-independence" class="section level3 hasAnchor" number="1.5.3">
<h3><span class="header-section-number">1.5.3</span> Conditional independence<a href="probability-theory.html#conditional-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, given <span class="math inline">\(C\)</span>: <span class="math inline">\(P(A|B,C)=P(A|C)\)</span>.
<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> must not have any information on each other, given the
information on <span class="math inline">\(C\)</span>. E.g. for school children:
<span class="math inline">\(P(\text{&quot;vocabulary&quot;}|\text{&quot;height&quot;}, \text{&quot;age&quot;})= P(\text{&quot;vocabulary&quot;}|\text{&quot;age&quot;})\)</span>.</p>
</div>
<div id="bayes-rule" class="section level3 hasAnchor" number="1.5.4">
<h3><span class="header-section-number">1.5.4</span> Bayes Rule<a href="probability-theory.html#bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bayes rule is a structured approach to update prior beliefs /
probabilities with new information (data). With the conditional
probability from before (<span class="math inline">\(P(A,B)=P(A|B)P(B)=P(B|A)P(A)\)</span>) we get <strong>Bayes
rule</strong> by transforming the right-side equation
to:<span class="math display">\[P(\text{hypothesis}|\text{evidence}) =\dfrac{P(\text{evidence}|\text{hypothesis})P(\text{hypothesis})}{P(\text{evidence})}\]</span>
often used as:
<span class="math display">\[P(\text{model}|\text{data}) =\dfrac{P(\text{data}|\text{model})P(\text{model})}{P(\text{data})}\]</span></p>
<div id="terminology" class="section level4 hasAnchor" number="1.5.4.1">
<h4><span class="header-section-number">1.5.4.1</span> Terminology:<a href="probability-theory.html#terminology" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><span class="math inline">\(P(\text{hypothesis}|\text{evidence})\)</span> = Posterior (How probable
hypothesis is after incorporating new evidence)</p></li>
<li><p><span class="math inline">\(P(\text{evidence}|\text{hypothesis})\)</span> = Likelihood (How probable
the evidence is, if the hypothesis is true)</p></li>
<li><p><span class="math inline">\(P(\text{hypothesis})\)</span> = Prior (How probable hypothesis was before
seeing evidence)</p></li>
<li><p><span class="math inline">\(P(\text{evidence})\)</span> = Marginal (How probable evidence is under all
possible hypotheses)</p></li>
<li><p><span class="math inline">\(\dfrac{P(\text{evidence}|\text{hypothesis})}{P(\text{evidence})}\)</span> =
Support <span class="math inline">\(B\)</span> provides for <span class="math inline">\(A\)</span></p></li>
<li><p><span class="math inline">\(P(\text{data}|\text{model})P(\text{model})\)</span> = joint probability
(<span class="math inline">\(P(A,B)\)</span>)</p></li>
</ul>
</div>
<div id="example-for-bayes-rule-using-covid-19-diagnostics" class="section level4 hasAnchor" number="1.5.4.2">
<h4><span class="header-section-number">1.5.4.2</span> Example for Bayes Rule using COVID-19 Diagnostics<a href="probability-theory.html#example-for-bayes-rule-using-covid-19-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[P(\text{COVID-19}|\text{cough}) =\dfrac{P(\text{cough}|\text{COVID-19})P(\text{COVID-19})}{P(\text{cough})} = \frac{0.7*0.01}{0.1}=0.07\]</span>
Estimating <span class="math inline">\(P(\text{COVID-19}|\text{cough})\)</span> is difficult, because there
can be an outbreak and the number changes. However,
<span class="math inline">\(P(\text{cough}|\text{COVID-19})\)</span> stays stable, <span class="math inline">\(P(\text{COVID-19})\)</span> and
<span class="math inline">\(P(\text{cough})\)</span> can be easily determined.</p>
</div>
</div>
</div>
<div id="further-concepts" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Further Concepts<a href="probability-theory.html#further-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="convergence-in-probability-of-random-variables" class="section level3 hasAnchor" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> Convergence in Probability of Random Variables<a href="probability-theory.html#convergence-in-probability-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You expect your random variables (<span class="math inline">\(X_i\)</span>) to converge to an expected
random variable <span class="math inline">\(X\)</span>. I.e. after looking at infinite samples, the
probability that your random variable <span class="math inline">\(X_n\)</span> differs more than a
threshold <span class="math inline">\(\epsilon\)</span> from your target <span class="math inline">\(X\)</span> should be zero.
<span class="math display">\[\lim_{n \rightarrow \infty} P(|X_n - X| &gt; \epsilon) = 0\]</span></p>
</div>
<div id="bernoullis-theorem-weak-law-of-large-numbers" class="section level3 hasAnchor" number="1.6.2">
<h3><span class="header-section-number">1.6.2</span> Bernoulli’s Theorem / Weak Law of Large Numbers<a href="probability-theory.html#bernoullis-theorem-weak-law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\lim_{n \rightarrow \infty} P(|\frac{\sum_{i=1}^n X_i}{n} - \mu| &gt; \epsilon) = 0,\]</span>
where <span class="math inline">\(X_1,...,X_n\)</span> are independent &amp; identically distributed (i.i.d.)
RVs. <span class="math inline">\(\Rightarrow\)</span> With enough samples, the sample mean will approach
the true mean. The <strong>strong law of large numbers</strong> states that
<span class="math inline">\(|\frac{\sum_{i=1}^n X_i}{n} - \mu| &lt; \epsilon\)</span> for any <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>

<hr />
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-basics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-Probability-Theory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
